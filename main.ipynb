{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c742b58a-36cc-4401-b6cc-a524bb12d3da",
   "metadata": {},
   "source": [
    "# The main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea73c43-6c92-400d-8b92-5334ed89a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'important', 'for', 'ai', 'research']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Tokenize text\n",
    "text = input(\"Enter your text\")\n",
    "cleaned_text = clean_text(text)\n",
    "tokens = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d871d137-1f7e-4cce-b516-300a66e886a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.text for p in paragraphs[:5]])  # Get first 5 paragraphs\n",
    "        return text\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find relevant study material.\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e17586-009c-48a1-ab3d-d2dca5adf3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.38.1, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "modelUI = gr.Interface(\n",
    "    fn = scrape_wikipedia,\n",
    "    inputs = 'text',\n",
    "    outputs = gr.Textbox(label = 'The answer to the question requested is')\n",
    ")\n",
    "modelUI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d09a72cc-3475-4092-9a9f-049d52368d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n",
      " ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n",
      " Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n",
      " From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n",
      " The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = input(\"Enter the topic\") \n",
    "study_material = scrape_wikipedia(topic)\n",
    "print(study_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a386ae-36b1-406f-89e5-fd75370d7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(study_material)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf8efbc-4387-45c8-8990-4f68e336806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://cedar.buffalo.edu/~srihari/CSE574/', 'https://online.stanford.edu/courses/xcs229-machine-learning', 'https://ocw.mit.edu/courses/6-036-introduction-to-machine-learning-fall-2020/']\n"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def google_search(query):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query + \" site:edu\",\n",
    "        \"api_key\": os.getenv(\"SERPAPI_KEY\")\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    links = [result[\"link\"] for result in results[\"organic_results\"][:3]]\n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "query = \"Machine Learning course material\"\n",
    "search_results = google_search(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08a92d8-7c32-44ae-a73e-a4f11427a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reference textbooks for different parts of the course are \"Pattern Recognition and Machine Learning\" by Chris Bishop (Springer 2006) and  \"Probabilistic Graphical Models\" by Daphne Koller and Nir Friedman (MIT Press 2009) and \"Deep Learning\" by Goodfellow, Bengio and Courville (MIT Press 2016).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "Course topics are listed below with links to lecture slides and lecture videos. \n",
      "\n",
      "\n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      " \n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      "\n",
      "\n",
      "This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement â¦ This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement learning, with applications to images and to temporal sequences. This course is part of the Open Learning Library, which is free to use.Â You have the option to sign up and enroll in the course if you want to track your progress, or you can view and use all the materials without enrolling.\n"
     ]
    }
   ],
   "source": [
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([p.text for p in paragraphs[:5]])\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for link in search_results:\n",
    "    study_material = scrape_website(link)\n",
    "    print(study_material)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c53347b4-387d-4a62-bb5d-b880f2c66c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Stanford Q&A dataset\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "\n",
    "print(squad['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "308a6692-b612-4b68-987e-97b587f04690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 of 3 \n",
      " \n",
      " \n",
      " \n",
      "SOEN 305 \n",
      "KISII \n",
      "UNIVERSITY \n",
      "UNIVERSITY EXAMINATIONS \n",
      "THIRD YEAR EXAMINATION FOR THE AWARD OF THE  \n",
      "DEGREE OF BACHELOR OF SCIENCE IN SOFTWARE ENGINEERING \n",
      "FIRST SEMESTER, 2023/2024 \n",
      "(AUGUST-DECEMBER, 2023)  \n",
      "SOEN 305: OBJECT ORIENTED PROGRAMMING II [JAVA] \n",
      "STREAM:  Y3 S1  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "TIME:   2 HOURS  \n",
      "DAY: TUESDAY, 03.00–02:00 PM \n",
      " \n",
      " \n",
      "         DATE: 21/11/2023 \n",
      " \n",
      "INSTRUCTIONS  \n",
      "1. Do not write anything on this question paper. \n",
      "2. Answer Question ONE [Compulsory] and any other TWO Questions. \n",
      " \n",
      "QUESTION ONE (30 MARKS) \n",
      "(a) In your words, describe object oriented programming \n",
      " \n",
      " \n",
      " \n",
      "(2 marks) \n",
      "(b) What \n",
      "are \n",
      "abstract \n",
      "methods? \n",
      "Describe \n",
      "the \n",
      "circumstances \n",
      "in \n",
      "which \n",
      "an \n",
      "abstract \n",
      "method would be appropriate  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(2 marks) \n",
      "(c)  \n",
      "i. Create a class called invoice that a hardware store might use to represent an \n",
      "invoice for an item sold at the store. An invoice should include 4 pieces of \n",
      "information as instance variables; a part number (type string), a part description \n",
      "(type string), a quantity of the item being purchased (type int) and a price per item(double). \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(5 marks) \n",
      "ii. . Your class should have a constructor that initializes the 4 instance variables. \n",
      "Provide a set and a get method for each of the 4 instance variables. In addition \n",
      "provide a method named getInvoiceAmount that calculates the invoice amount i.e. multiplies \n",
      "the quantity by the price per item, then returns the amount as a double value. If the quantity is \n",
      "not positive, it should be set to 0.0. (5 marks) \n",
      "iii. Write a test application named invoiceTest that demonstrates class invoice’s \n",
      "capabilities. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(5 marks) \n",
      " \n",
      "Page 2 of 3 \n",
      " \n",
      " \n",
      "(d) Explain how: \n",
      "i. The super reference is important to a child class.  \n",
      " \n",
      " \n",
      "(2 marks) \n",
      "ii. Inheritance support polymorphism \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(2 marks) \n",
      "(e) Describe the principles of the object oriented paradigm \n",
      " \n",
      " \n",
      " \n",
      "(7 marks) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "QUESTION TWO (20 marks) \n",
      "(a) As a software developer, discuss briefly the various error and exception handling options available \n",
      "in Java. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(6 marks) \n",
      "(b) Write a complete java/C++ program that prompt the \n",
      "user to enter two non-negative integer number. Your \n",
      "program should handle bad input data through the use \n",
      "of a try/catch block to handle the \n",
      "inputMismathException \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(8 marks) \n",
      "(c) Explain the difference between implementing an \n",
      "interface and a derived class. Give code illustrations.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(6 marks) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "QUESTION THREE (20 marks) \n",
      "(a) Consider a SurveyTaker class, which can be used to \n",
      "collect data from a yes/no survey.  \n",
      "i.  In the class definition, clearly identify one of each \n",
      "of the following components:  field, constructor, method, parameter (4 marks) \n",
      "ii. What would happen if the user called the enterResponse method with an input value other \n",
      "than \"yes\" or \"no\"? For example, suppose the user entered \"maybe\" in the input box when \n",
      "prompted. Would an error occur? If not, what would the method do? Explain your answer. \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "(5 marks) \n",
      "iii. Discuss the context of use of this keyword in this code. \n",
      " \n",
      " \n",
      "(3 marks) \n",
      "iv. Write an application to test the capability of the SurveyTaker class.  \n",
      " (8 marks) \n",
      " \n",
      "(b) Discuss the concept of method overloading and method overriding as used in object oriented \n",
      "programming. Write simple java codes that illustrates the concepts.  \n",
      " \n",
      "(8 marks) \n",
      " \n",
      "QUESTION FOUR (20 marks) \n",
      "public class SurveyTaker {   \n",
      "private int numOfYes;  \n",
      "private int numOfNo;  \n",
      "public SurveyTaker() {  \n",
      "this.numOfYes = 0;  \n",
      "this.numOfNo = 0;  \n",
      "}  \n",
      "public void enterResponse(String response) {  \n",
      "if (response.equals(\"yes\")) { \n",
      "this.numOfYes++;  \n",
      "} else if (response.equals(\"no\")) { \n",
      "this.numOfNo++;  \n",
      "} }  \n",
      "public int getYesses() {  \n",
      "return this.numOfYes;  \n",
      "}  \n",
      "} \n",
      "Page 3 of 3 \n",
      " \n",
      "(a) Write a program to generate the Graphical User Interface(GUI)with two buttons labelled IN \n",
      "and OUT. If the user clicks the IN button, the message “DOCTOR IS IN” flashes and if the \n",
      "user click OUT button the message “DOCTOR IS OUT” flashes. \n",
      " (8 marks) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(b) Bu use of examples, discuss the concept of method overloading and method overriding as used \n",
      "in object oriented programming. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(8 marks) \n",
      "(c) Explain the advantage for programming your GUI in an applet environment vs SWING \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "               (4 marks)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "QUESTION FIVE (20 MARKS) \n",
      "(a) Write a java program to create a class ‘STUDENT’ with data members Rgno, Name, Course, \n",
      "Branch, and Semester. Store them in an array of objects. \n",
      " \n",
      " \n",
      " \n",
      "(5 marks) \n",
      "(b) Write an applet that will display the resulting GUI below.  \n",
      " \n",
      " \n",
      "(15 marks) \n",
      "  \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"C:/Users/Home/Desktop/LearningManagementAI/LearningManagementAI/Docs/past_paper.pdf\")\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7189568-c48f-48ae-8e72-463a42548b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(pdf_text)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "792874cf-ca19-4bf6-8b54-b72e93af7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(data, filename=\"study_data.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "study_data = {\"topic\": \"Machine Learning\", \"content\": pdf_text}\n",
    "save_to_json(study_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d015ee9-5f2c-4be9-b718-567653ed70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning field ai visit https example com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\W+\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Machine learning is a field of AI. Visit https://example.com for more!\"\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c48ab1-07fd-49b1-a5c0-78812a1e4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KISII UNIVERSITY\n",
      "Faculty of Information Science and Technology (SIST)\n",
      "Department of Computing Sciences\n",
      "YEAR 3 SEM 1 BSC COMPUTER SCIENCE AND APPLIED COMPUTER\n",
      "SCIENCE\n",
      "SEPT-DEC, 2024\n",
      "COURSE OUTLINE\n",
      "Course Details\n",
      "Course Code COMP 303\n",
      "Course Name AUTOMATA THEORY\n",
      "Credit Hours 3.5\n",
      "Day/Time/Location THURSDAYS / 9am – 11 am / TC G2\n",
      "Lecturer Silas Momanyi Nyabuga\n",
      "Email Address smnyabuga@gmail.com\n",
      "Cell 0722-891-892\n",
      "Contact Hours: Lectures 30 and Practicals/Tutorials 30\n",
      "Purpose of the course:\n",
      "The course will provide students with knowledge and skills regarding\n",
      "fundamental concepts of Finite Automata, Regular Languages, and Pushdown\n",
      "Automata before moving onto Turing machines and Decidability.\n",
      "Expected Learning outcomes of the Course:\n",
      "Upon successful completion of this course, the student will be able to:\n",
      "i. Prove properties of languages, grammars and automata with rigorously\n",
      "formal mathematical methods;\n",
      "ii. Design automata, regular expressions and context-free grammars\n",
      "accepting or generating a certain language;\n",
      "iii. Simplify automata and context-free grammars;\n",
      "iv. Determine if a certain word belongs to a language;\n",
      "v. Define Turing machines performing simple tasks.\n",
      "vi. Explain and manipulate the different concepts in automata theory and\n",
      "formal languages such as formal proofs, (non-)deterministic automata,\n",
      "regular expressions, regular languages, context-free grammars, context-\n",
      "free languages, Turing machines;\n",
      "Course content:\n",
      "WK TOPIC PARTS\n",
      "1 Introduction to  Definition of automata theory\n",
      "automata theory\n",
      " Types of automata\n",
      " Deterministic (DFA) and Non-Deterministic\n",
      "Finite Automaton (NDFA)\n",
      " Related terminologies\n",
      "2 Classification of  Grammer\n",
      "Grammers\n",
      " Derivation from a Grammer\n",
      " Language generated by a Grammer\n",
      " Construction of a Grammer Generating\n",
      "Language\n",
      " Chomsky Classification of Grammars\n",
      "3. Regular Grammars  Regular Expressions\n",
      " Regular Sets..\n",
      " Identities Related to Regular Expressions\n",
      "4. Regular Grammars  Construction of an FA from an RE\n",
      " Pumping Lemma for Regular Languages\n",
      " Applications of Pumping Lemma\n",
      "5. Context-Free Grammars  Context-Free Grammar\n",
      " Closure Property of CFL\n",
      " Chomsky Normal Form\n",
      " Left and Right Recursive Grammars\n",
      "6. Pushdown Automata  Basic Structure of PDA\n",
      " Terminologies Related to PDA\n",
      " Parsing and PDA\n",
      " Pushdown Automata and Parsing\n",
      "7 CAT ONE\n",
      "8 Turing Machine  Definition\n",
      " Accepted Language and Decided Language\n",
      " Multi-track Turing Machine\n",
      " Non-Deterministic Turing machine\n",
      "9 Turing Machine  Turing Machine with Semi-infinite Tape\n",
      " Time and Space Complexity of a Turing\n",
      "Machine cable testers\n",
      " Linear Bounded Automata\n",
      "10 Decidability  Decidability and Decidable Languages\n",
      " Undecidable Languages\n",
      " TM Halting Problem\n",
      "11 Computability and  Turing Machine Halting Problem\n",
      "Undecidability\n",
      "CAT TWO\n",
      "12 Arden's Theorem  Assumptions for Applying Arden’s Theorem\n",
      " Linear bounded Automata\n",
      "13 Rice theorem  Rice theorem of Computation\n",
      "14 REVISION AND END OF SEMESTER EXAMINATIONS\n",
      "Mode of delivery:\n",
      "The course will be taught by using lectures, tutorials and assignments.\n",
      "Instructional Materials and/or Equipment:\n",
      "Textbooks, hand-outs, chalk/white boards.\n",
      "Course assessment\n",
      "Course work (Assessments and tests) 30%\n",
      "Final Examination 70%\n",
      "Total 100%\n",
      "Core Reading Materials for the course\n",
      "Recommended Reference Materials\n",
      "i. introduction to automata theory , Langauges, and Computation by John E\n",
      "Hopcroft, Rajeev Motwani and Jeffrey D.Ullman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "course_outline_text = extract_text_from_pdf(\"C:/Users/Home/Desktop/Comp3.1/Automata Theory303/COMP 303  AUTOMATA THEORY  COURSE OUTLINE.pdf\")\n",
    "print(course_outline_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5febd32b-7526-4817-ab34-0c552a3d62aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at 'C:/Users/Home/Desktop/Course Outline.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     doc \u001b[38;5;241m=\u001b[39m Document(docx_path)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([para\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs])\n\u001b[1;32m----> 7\u001b[0m course_outline_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_docx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Home/Desktop/Course Outline.docx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(course_outline_text)\n",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m, in \u001b[0;36mextract_text_from_docx\u001b[1;34m(docx_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_docx\u001b[39m(docx_path):\n\u001b[1;32m----> 4\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([para\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mparagraphs])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\docx\\api.py:27\u001b[0m, in \u001b[0;36mDocument\u001b[1;34m(docx)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a |Document| object loaded from `docx`, where `docx` can be either a path\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03mto a ``.docx`` file (a string) or a file-like object.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mIf `docx` is missing or ``None``, the built-in default document \"template\" is\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mloaded.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m docx \u001b[38;5;241m=\u001b[39m _default_docx_path() \u001b[38;5;28;01mif\u001b[39;00m docx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m docx\n\u001b[1;32m---> 27\u001b[0m document_part \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentPart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mPackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain_document_part)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mcontent_type \u001b[38;5;241m!=\u001b[39m CT\u001b[38;5;241m.\u001b[39mWML_DOCUMENT_MAIN:\n\u001b[0;32m     29\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a Word file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\docx\\opc\\package.py:127\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OpcPackage:\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     pkg_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPackageReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m    129\u001b[0m     Unmarshaller\u001b[38;5;241m.\u001b[39munmarshal(pkg_reader, package, PartFactory)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\docx\\opc\\pkgreader.py:22\u001b[0m, in \u001b[0;36mPackageReader.from_file\u001b[1;34m(pkg_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(pkg_file):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     phys_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPhysPkgReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     content_types \u001b[38;5;241m=\u001b[39m _ContentTypeMap\u001b[38;5;241m.\u001b[39mfrom_xml(phys_reader\u001b[38;5;241m.\u001b[39mcontent_types_xml)\n\u001b[0;32m     24\u001b[0m     pkg_srels \u001b[38;5;241m=\u001b[39m PackageReader\u001b[38;5;241m.\u001b[39m_srels_for(phys_reader, PACKAGE_URI)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\docx\\opc\\phys_pkg.py:21\u001b[0m, in \u001b[0;36mPhysPkgReader.__new__\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m         reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage not found at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m pkg_file)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume it's a stream and pass it to Zip reader to sort out\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: Package not found at 'C:/Users/Home/Desktop/Course Outline.docx'"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "course_outline_text = extract_text_from_docx(\"C:/Users/Home/Desktop/Course Outline.docx\")\n",
    "print(course_outline_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a78cab-1886-4b28-a4c2-78dc089df521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import pytesseract\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    return text\n",
    "\n",
    "def extract_course_info(text):\n",
    "    info = {\n",
    "        \"Course Title\": None,\n",
    "        \"Course Duration\": None,\n",
    "        \"Weekly Breakdown\": {},\n",
    "        \"Objectives\": [],\n",
    "        \"Recommended Materials\": []\n",
    "    }\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    course_title_pattern = re.compile(r\"(?i)Course\\s*Title:\\s*(.+)\")\n",
    "    duration_pattern = re.compile(r\"(?i)Duration:\\s*(.+)\")\n",
    "    week_pattern = re.compile(r\"(?i)Week\\s*(\\d+):?\\s*(.*)\")\n",
    "    objectives_pattern = re.compile(r\"(?i)Objectives?\")\n",
    "    materials_pattern = re.compile(r\"(?i)Recommended\\s*Materials?\")\n",
    "\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        title_match = course_title_pattern.search(line)\n",
    "        if title_match:\n",
    "            info[\"Course Title\"] = title_match.group(1)\n",
    "            continue\n",
    "\n",
    "        duration_match = duration_pattern.search(line)\n",
    "        if duration_match:\n",
    "            info[\"Course Duration\"] = duration_match.group(1)\n",
    "            continue\n",
    "\n",
    "        week_match = week_pattern.search(line)\n",
    "        if week_match:\n",
    "            week_number = int(week_match.group(1))\n",
    "            week_content = week_match.group(2).strip()\n",
    "            info[\"Weekly Breakdown\"][week_number] = week_content\n",
    "            continue\n",
    "\n",
    "        if objectives_pattern.search(line):\n",
    "            current_section = \"Objectives\"\n",
    "            continue\n",
    "        elif materials_pattern.search(line):\n",
    "            current_section = \"Recommended Materials\"\n",
    "            continue\n",
    "\n",
    "        if current_section == \"Objectives\":\n",
    "            info[\"Objectives\"].append(line)\n",
    "        elif current_section == \"Recommended Materials\":\n",
    "            info[\"Recommended Materials\"].append(line)\n",
    "\n",
    "    return info\n",
    "\n",
    "def process_course_outline(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")\n",
    "\n",
    "    return extract_course_info(text)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"C:/Users/Home/Desktop/Comp 3.2/Comp 302/SOEN 302 - COMP 302 course outline.docx\" \n",
    "course_data = process_course_outline(file_path)\n",
    "\n",
    "print(course_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7b4bb-00a6-4e55-9f0b-36e23594ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import pytesseract\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    return text\n",
    "\n",
    "def extract_course_info(text):\n",
    "    info = {\n",
    "        \"Course Title\": None,\n",
    "        \"Course Code\": None,\n",
    "        \"Course Duration\": None,\n",
    "        \"Weekly Breakdown\": {},\n",
    "        \"Objectives\": [],\n",
    "        \"Recommended Materials\": []\n",
    "    }\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    course_title_pattern = re.compile(r\"(?i)([A-Z]{3,4}\\s*\\d{3})\\s*[-:]?\\s*(.+)\")\n",
    "    duration_pattern = re.compile(r\"(?i)Duration:\\s*(.+)\")\n",
    "\n",
    "    week_pattern = re.compile(r\"(?i)(?:Week\\s*|^)(\\d+)[\\.:]?\\s*(.*)\")\n",
    "    bullet_pattern = re.compile(r\"^[•*-]\\s*(.+)\")\n",
    "\n",
    "    objectives_pattern = re.compile(r\"(?i)Objectives?\")\n",
    "    materials_pattern = re.compile(r\"(?i)Recommended\\s*Materials?\")\n",
    "\n",
    "    current_section = None\n",
    "    week_counter = 0\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        title_match = course_title_pattern.search(line)\n",
    "        if title_match:\n",
    "            info[\"Course Code\"] = title_match.group(1)\n",
    "            info[\"Course Title\"] = title_match.group(2)\n",
    "            continue\n",
    "\n",
    "        duration_match = duration_pattern.search(line)\n",
    "        if duration_match:\n",
    "            info[\"Course Duration\"] = duration_match.group(1)\n",
    "            continue\n",
    "\n",
    "        week_match = week_pattern.search(line)\n",
    "        if week_match:\n",
    "            week_number = int(week_match.group(1))\n",
    "            week_content = week_match.group(2).strip()\n",
    "            info[\"Weekly Breakdown\"][week_number] = week_content\n",
    "            continue\n",
    "\n",
    "        bullet_match = bullet_pattern.search(line)\n",
    "        if bullet_match:\n",
    "            week_counter += 1\n",
    "            info[\"Weekly Breakdown\"][week_counter] = bullet_match.group(1)\n",
    "            continue\n",
    "\n",
    "        if objectives_pattern.search(line):\n",
    "            current_section = \"Objectives\"\n",
    "            continue\n",
    "        elif materials_pattern.search(line):\n",
    "            current_section = \"Recommended Materials\"\n",
    "            continue\n",
    "\n",
    "        if current_section == \"Objectives\":\n",
    "            info[\"Objectives\"].append(line)\n",
    "        elif current_section == \"Recommended Materials\":\n",
    "            info[\"Recommended Materials\"].append(line)\n",
    "\n",
    "    return info\n",
    "\n",
    "def process_course_outline(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")\n",
    "\n",
    "    return extract_course_info(text)\n",
    "\n",
    "file_path = \"C:/Users/Home/Desktop/Comp 3.2/Comp 306/COMP 306 CO.pdf\" \n",
    "course_data = process_course_outline(file_path)\n",
    "\n",
    "print(course_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d34a17-e7cd-4872-be4a-0c3327bd14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_weeks_and_topics(text):\n",
    "    weeks = re.findall(r\"Week \\d+: (.+)\", text)\n",
    "    return weeks\n",
    "\n",
    "weekly_topics = extract_weeks_and_topics(course_outline_text)\n",
    "print(weekly_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68ecff-9e61-4b84-9f77-011e24d0ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "def scrape_study_materials(topic):\n",
    "    api_key = os.getenv(\"SERPAPI_KEY\")\n",
    "    search = GoogleSearch({\n",
    "        \"q\": f\"{topic} study materials\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": api_key\n",
    "    })\n",
    "    \n",
    "    results = search.get_dict()\n",
    "    links = []\n",
    "\n",
    "    for result in results.get(\"organic_results\", []):\n",
    "        if \"link\" in result:\n",
    "            links.append(result[\"link\"])\n",
    "    \n",
    "    return links[:5]\n",
    "\n",
    "def summarize_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        return article.summary\n",
    "    except Exception as e:\n",
    "        return f\"Error processing article: {e}\"\n",
    "\n",
    "study_materials = scrape_study_materials(\"Machine Learning\")\n",
    "\n",
    "if study_materials:\n",
    "    summary = summarize_article(study_materials[0])\n",
    "    print(\"Summary of the first article:\\n\", summary)\n",
    "else:\n",
    "    print(\"No study materials found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf5053-65af-4c05-a820-00539224f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff117cc5-6092-4197-81c5-01f4470d668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your text So explain the meaning of machine availability in context to machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['availability' 'context' 'explain' 'learning' 'machine' 'meaning']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords(text):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10)\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    keywords = vectorizer.get_feature_names_out()\n",
    "    return keywords\n",
    "\n",
    "text = input(\"Enter your text\")\n",
    "print(extract_keywords(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a88cfba0-a799-4464-886f-f5452ad80771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/lijqhs/deeplearning-notes', 'https://medium.com/@tiraldj/study-guide-deep-learning-by-yoshua-bengio-yann-lecun-geoffrey-hinton-nature-2015-7cbdd5077947', 'https://udlbook.github.io/udlbook/', 'https://www.learnpytorch.io/', 'https://cs230.stanford.edu/']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def scrape_study_materials(query):\n",
    "    api_key = os.getenv(\"SERPAPI_KEY\")\n",
    "    url = f\"https://serpapi.com/search?q={query}+study+materials&api_key={api_key}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    links = [result[\"link\"] for result in data[\"organic_results\"][:5]]\n",
    "    return links\n",
    "\n",
    "query = \"deep learning\"\n",
    "study_materials = scrape_study_materials(query)\n",
    "print(study_materials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee1b74d4-81b6-4095-ad87-4e752a1097da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Advanced Neural Networks', 'Introduction to Deep Learning', 'Machine Learning Basics', 'Data Science Foundations']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_materials(course_desc, study_materials):\n",
    "    texts = [course_desc] + study_materials  # Combine all texts\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    vectors = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    similarities = cosine_similarity(vectors[0], vectors[1:])  # Compare course with materials\n",
    "    ranked_indices = similarities.argsort()[0][::-1]  # Rank by similarity score\n",
    "    \n",
    "    return [study_materials[i] for i in ranked_indices]\n",
    "\n",
    "course_description = \"This course covers deep learning and neural networks.\"\n",
    "study_materials = [\n",
    "    \"Introduction to Deep Learning\",\n",
    "    \"Advanced Neural Networks\",\n",
    "    \"Machine Learning Basics\",\n",
    "    \"Data Science Foundations\"\n",
    "]\n",
    "\n",
    "recommended = recommend_materials(course_description, study_materials)\n",
    "print(recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3950d4cf-e579-41af-a14e-a0d681ddc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Course(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "\n",
    "@app.post(\"/recommend\")\n",
    "def recommend(course: Course):\n",
    "    study_materials = scrape_study_materials(course.title)\n",
    "    recommended = recommend_materials(course.description, study_materials)\n",
    "    return {\"recommended_materials\": recommended}\n",
    "\n",
    "# Run using: uvicorn filename:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02470149-443c-4ef1-8c31-4d96789d8c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
