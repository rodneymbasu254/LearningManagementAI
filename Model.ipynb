{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c742b58a-36cc-4401-b6cc-a524bb12d3da",
   "metadata": {},
   "source": [
    "# The main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea73c43-6c92-400d-8b92-5334ed89a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'important', 'for', 'ai', 'research']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Machine Learning is important for AI research!\"\n",
    "cleaned_text = clean_text(text)\n",
    "tokens = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d871d137-1f7e-4cce-b516-300a66e886a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.text for p in paragraphs[:5]])  # Get first 5 paragraphs\n",
    "        return text\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find relevant study material.\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e17586-009c-48a1-ab3d-d2dca5adf3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.38.1, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "modelUI = gr.Interface(\n",
    "    fn = scrape_wikipedia,\n",
    "    inputs = 'text',\n",
    "    outputs = gr.Textbox(label = 'The answer to the question requested is')\n",
    ")\n",
    "modelUI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09a72cc-3475-4092-9a9f-049d52368d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic Machine Learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n",
      " ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n",
      " Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n",
      " From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n",
      " The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = input(\"Enter the topic\")\n",
    "study_material = scrape_wikipedia(topic)\n",
    "print(study_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a386ae-36b1-406f-89e5-fd75370d7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(study_material)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf8efbc-4387-45c8-8990-4f68e336806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://cedar.buffalo.edu/~srihari/CSE574/', 'https://online.stanford.edu/courses/xcs229-machine-learning', 'https://ocw.mit.edu/courses/6-036-introduction-to-machine-learning-fall-2020/']\n"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def google_search(query):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query + \" site:edu\",\n",
    "        \"api_key\": \"4c3a39f406bb56f40ea28e7b2b59186206d9eb054c352ef8e19552d9900611f2\",\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    links = [result[\"link\"] for result in results[\"organic_results\"][:3]]\n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "query = \"Machine Learning course material\"\n",
    "search_results = google_search(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08a92d8-7c32-44ae-a73e-a4f11427a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reference textbooks for different parts of the course are \"Pattern Recognition and Machine Learning\" by Chris Bishop (Springer 2006) and  \"Probabilistic Graphical Models\" by Daphne Koller and Nir Friedman (MIT Press 2009) and \"Deep Learning\" by Goodfellow, Bengio and Courville (MIT Press 2016).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "Course topics are listed below with links to lecture slides and lecture videos. \n",
      "\n",
      "\n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      " \n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      "\n",
      "\n",
      "This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement â¦ This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement learning, with applications to images and to temporal sequences. This course is part of the Open Learning Library, which is free to use.Â You have the option to sign up and enroll in the course if you want to track your progress, or you can view and use all the materials without enrolling.\n"
     ]
    }
   ],
   "source": [
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([p.text for p in paragraphs[:5]])\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for link in search_results:\n",
    "    study_material = scrape_website(link)\n",
    "    print(study_material)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c53347b4-387d-4a62-bb5d-b880f2c66c00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load Stanford Q&A dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m squad \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Stanford Q&A dataset\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "\n",
    "print(squad['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a6692-b612-4b68-987e-97b587f04690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"C:/Users/Home/Desktop/LearningManagementAI/LearningManagementAI/Docs/past_paper.pdf\")\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7189568-c48f-48ae-8e72-463a42548b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(pdf_text)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792874cf-ca19-4bf6-8b54-b72e93af7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(data, filename=\"study_data.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "study_data = {\"topic\": \"Machine Learning\", \"content\": pdf_text}\n",
    "save_to_json(study_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d015ee9-5f2c-4be9-b718-567653ed70ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
