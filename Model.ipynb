{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c742b58a-36cc-4401-b6cc-a524bb12d3da",
   "metadata": {},
   "source": [
    "# The main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea73c43-6c92-400d-8b92-5334ed89a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'important', 'for', 'ai', 'research']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Machine Learning is important for AI research!\"\n",
    "cleaned_text = clean_text(text)\n",
    "tokens = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d871d137-1f7e-4cce-b516-300a66e886a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.text for p in paragraphs[:5]])  # Get first 5 paragraphs\n",
    "        return text\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find relevant study material.\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e17586-009c-48a1-ab3d-d2dca5adf3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7 (_do_normal_analytics_request):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 216, in handle_request\n",
      "    raise exc from None\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 196, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 99, in handle_request\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 76, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 154, in _connect\n",
      "    stream = stream.start_tls(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 152, in start_tls\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectTimeout: _ssl.c:983: The handshake operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\analytics.py\", line 70, in _do_normal_analytics_request\n",
      "    data[\"ip_address\"] = get_local_ip_address()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\analytics.py\", line 131, in get_local_ip_address\n",
      "    ip_address = httpx.get(\n",
      "                 ^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_api.py\", line 198, in get\n",
      "    return request(\n",
      "           ^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_api.py\", line 106, in request\n",
      "    return client.request(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py\", line 827, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 232, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectTimeout: _ssl.c:983: The handshake operation timed out\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "modelUI = gr.Interface(\n",
    "    fn = scrape_wikipedia,\n",
    "    inputs = 'text',\n",
    "    outputs = gr.Textbox(label = 'The answer to the question requested is')\n",
    ")\n",
    "modelUI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09a72cc-3475-4092-9a9f-049d52368d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic Machine Learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n",
      " ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n",
      " Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n",
      " From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n",
      " The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = input(\"Enter the topic\")\n",
    "study_material = scrape_wikipedia(topic)\n",
    "print(study_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a386ae-36b1-406f-89e5-fd75370d7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(study_material)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8efbc-4387-45c8-8990-4f68e336806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def google_search(query):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query + \" site:edu\",\n",
    "        \"api_key\": \"4c3a39f406bb56f40ea28e7b2b59186206d9eb054c352ef8e19552d9900611f2\",\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    links = [result[\"link\"] for result in results[\"organic_results\"][:3]]\n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "query = \"Machine Learning course material\"\n",
    "search_results = google_search(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a92d8-7c32-44ae-a73e-a4f11427a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([p.text for p in paragraphs[:5]])\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for link in search_results:\n",
    "    study_material = scrape_website(link)\n",
    "    print(study_material)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53347b4-387d-4a62-bb5d-b880f2c66c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Stanford Q&A dataset\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# Print first sample\n",
    "print(squad['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a6692-b612-4b68-987e-97b587f04690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_text = extract_text_from_pdf(\"past_paper.pdf\")\n",
    "print(pdf_text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792874cf-ca19-4bf6-8b54-b72e93af7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(data, filename=\"study_data.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "study_data = {\"topic\": \"Machine Learning\", \"content\": pdf_text}\n",
    "save_to_json(study_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
