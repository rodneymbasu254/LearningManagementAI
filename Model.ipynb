{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c742b58a-36cc-4401-b6cc-a524bb12d3da",
   "metadata": {},
   "source": [
    "# The main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea73c43-6c92-400d-8b92-5334ed89a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'important', 'for', 'ai', 'research']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Machine Learning is important for AI research!\"\n",
    "cleaned_text = clean_text(text)\n",
    "tokens = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d871d137-1f7e-4cce-b516-300a66e886a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.text for p in paragraphs[:5]])  # Get first 5 paragraphs\n",
    "        return text\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find relevant study material.\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e17586-009c-48a1-ab3d-d2dca5adf3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.38.1, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "modelUI = gr.Interface(\n",
    "    fn = scrape_wikipedia,\n",
    "    inputs = 'text',\n",
    "    outputs = gr.Textbox(label = 'The answer to the question requested is')\n",
    ")\n",
    "modelUI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09a72cc-3475-4092-9a9f-049d52368d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic Machine Learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n",
      " ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n",
      " Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n",
      " From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n",
      " The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = input(\"Enter the topic\")\n",
    "study_material = scrape_wikipedia(topic)\n",
    "print(study_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a386ae-36b1-406f-89e5-fd75370d7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(study_material)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf8efbc-4387-45c8-8990-4f68e336806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://cedar.buffalo.edu/~srihari/CSE574/', 'https://online.stanford.edu/courses/xcs229-machine-learning', 'https://ocw.mit.edu/courses/6-036-introduction-to-machine-learning-fall-2020/']\n"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def google_search(query):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query + \" site:edu\",\n",
    "        \"api_key\": \"4c3a39f406bb56f40ea28e7b2b59186206d9eb054c352ef8e19552d9900611f2\",\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    links = [result[\"link\"] for result in results[\"organic_results\"][:3]]\n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "query = \"Machine Learning course material\"\n",
    "search_results = google_search(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08a92d8-7c32-44ae-a73e-a4f11427a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reference textbooks for different parts of the course are \"Pattern Recognition and Machine Learning\" by Chris Bishop (Springer 2006) and  \"Probabilistic Graphical Models\" by Daphne Koller and Nir Friedman (MIT Press 2009) and \"Deep Learning\" by Goodfellow, Bengio and Courville (MIT Press 2016).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "Course topics are listed below with links to lecture slides and lecture videos. \n",
      "\n",
      "\n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      " \n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      "\n",
      "\n",
      "This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement â¦ This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement learning, with applications to images and to temporal sequences. This course is part of the Open Learning Library, which is free to use.Â You have the option to sign up and enroll in the course if you want to track your progress, or you can view and use all the materials without enrolling.\n"
     ]
    }
   ],
   "source": [
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([p.text for p in paragraphs[:5]])\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for link in search_results:\n",
    "    study_material = scrape_website(link)\n",
    "    print(study_material)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53347b4-387d-4a62-bb5d-b880f2c66c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Stanford Q&A dataset\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "\n",
    "print(squad['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a6692-b612-4b68-987e-97b587f04690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"C:/Users/Home/Desktop/LearningManagementAI/LearningManagementAI/Docs/past_paper.pdf\")\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7189568-c48f-48ae-8e72-463a42548b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(pdf_text)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792874cf-ca19-4bf6-8b54-b72e93af7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(data, filename=\"study_data.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "study_data = {\"topic\": \"Machine Learning\", \"content\": pdf_text}\n",
    "save_to_json(study_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d015ee9-5f2c-4be9-b718-567653ed70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning field ai visit https example com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\W+\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Machine learning is a field of AI. Visit https://example.com for more!\"\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c48ab1-07fd-49b1-a5c0-78812a1e4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KISII UNIVERSITY\n",
      "Faculty of Information Science and Technology (SIST)\n",
      "Department of Computing Sciences\n",
      "YEAR 3 SEM 1 BSC COMPUTER SCIENCE AND APPLIED COMPUTER\n",
      "SCIENCE\n",
      "SEPT-DEC, 2024\n",
      "COURSE OUTLINE\n",
      "Course Details\n",
      "Course Code COMP 303\n",
      "Course Name AUTOMATA THEORY\n",
      "Credit Hours 3.5\n",
      "Day/Time/Location THURSDAYS / 9am – 11 am / TC G2\n",
      "Lecturer Silas Momanyi Nyabuga\n",
      "Email Address smnyabuga@gmail.com\n",
      "Cell 0722-891-892\n",
      "Contact Hours: Lectures 30 and Practicals/Tutorials 30\n",
      "Purpose of the course:\n",
      "The course will provide students with knowledge and skills regarding\n",
      "fundamental concepts of Finite Automata, Regular Languages, and Pushdown\n",
      "Automata before moving onto Turing machines and Decidability.\n",
      "Expected Learning outcomes of the Course:\n",
      "Upon successful completion of this course, the student will be able to:\n",
      "i. Prove properties of languages, grammars and automata with rigorously\n",
      "formal mathematical methods;\n",
      "ii. Design automata, regular expressions and context-free grammars\n",
      "accepting or generating a certain language;\n",
      "iii. Simplify automata and context-free grammars;\n",
      "iv. Determine if a certain word belongs to a language;\n",
      "v. Define Turing machines performing simple tasks.\n",
      "vi. Explain and manipulate the different concepts in automata theory and\n",
      "formal languages such as formal proofs, (non-)deterministic automata,\n",
      "regular expressions, regular languages, context-free grammars, context-\n",
      "free languages, Turing machines;\n",
      "Course content:\n",
      "WK TOPIC PARTS\n",
      "1 Introduction to  Definition of automata theory\n",
      "automata theory\n",
      " Types of automata\n",
      " Deterministic (DFA) and Non-Deterministic\n",
      "Finite Automaton (NDFA)\n",
      " Related terminologies\n",
      "2 Classification of  Grammer\n",
      "Grammers\n",
      " Derivation from a Grammer\n",
      " Language generated by a Grammer\n",
      " Construction of a Grammer Generating\n",
      "Language\n",
      " Chomsky Classification of Grammars\n",
      "3. Regular Grammars  Regular Expressions\n",
      " Regular Sets..\n",
      " Identities Related to Regular Expressions\n",
      "4. Regular Grammars  Construction of an FA from an RE\n",
      " Pumping Lemma for Regular Languages\n",
      " Applications of Pumping Lemma\n",
      "5. Context-Free Grammars  Context-Free Grammar\n",
      " Closure Property of CFL\n",
      " Chomsky Normal Form\n",
      " Left and Right Recursive Grammars\n",
      "6. Pushdown Automata  Basic Structure of PDA\n",
      " Terminologies Related to PDA\n",
      " Parsing and PDA\n",
      " Pushdown Automata and Parsing\n",
      "7 CAT ONE\n",
      "8 Turing Machine  Definition\n",
      " Accepted Language and Decided Language\n",
      " Multi-track Turing Machine\n",
      " Non-Deterministic Turing machine\n",
      "9 Turing Machine  Turing Machine with Semi-infinite Tape\n",
      " Time and Space Complexity of a Turing\n",
      "Machine cable testers\n",
      " Linear Bounded Automata\n",
      "10 Decidability  Decidability and Decidable Languages\n",
      " Undecidable Languages\n",
      " TM Halting Problem\n",
      "11 Computability and  Turing Machine Halting Problem\n",
      "Undecidability\n",
      "CAT TWO\n",
      "12 Arden's Theorem  Assumptions for Applying Arden’s Theorem\n",
      " Linear bounded Automata\n",
      "13 Rice theorem  Rice theorem of Computation\n",
      "14 REVISION AND END OF SEMESTER EXAMINATIONS\n",
      "Mode of delivery:\n",
      "The course will be taught by using lectures, tutorials and assignments.\n",
      "Instructional Materials and/or Equipment:\n",
      "Textbooks, hand-outs, chalk/white boards.\n",
      "Course assessment\n",
      "Course work (Assessments and tests) 30%\n",
      "Final Examination 70%\n",
      "Total 100%\n",
      "Core Reading Materials for the course\n",
      "Recommended Reference Materials\n",
      "i. introduction to automata theory , Langauges, and Computation by John E\n",
      "Hopcroft, Rajeev Motwani and Jeffrey D.Ullman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "course_outline_text = extract_text_from_pdf(\"C:/Users/Home/Desktop/Comp3.1/Automata Theory303/COMP 303  AUTOMATA THEORY  COURSE OUTLINE.pdf\")\n",
    "print(course_outline_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5febd32b-7526-4817-ab34-0c552a3d62aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEB PROGRAMMING II (30/30; CF 3.5) Y3S2\n",
      "Prerequisite: BIT 202\n",
      "Contact Hours: Lectures 30 and Practical/Tutorials 30\n",
      "\n",
      "\n",
      "Purpose of the course:\n",
      "To provide the student with the knowledge and skills to construct dynamic and interactive websites for various types of business applications.\n",
      "Expected learning outcome\n",
      "By the end of the course units the learner should:\n",
      "Have a practical experience in the use of programming and scripting languages for web. Development\n",
      "Develop back end applications using JavaScript libraries such as Nodes.js, \n",
      "Develop a Data-Driven Node.js Web App and connect with the web database\n",
      "Develop dynamic and interactive web applications using various types of business information\n",
      "Each student will come up with a small website with full functionalities of a website\n",
      "Course Content\n",
      "Server Side Web application development using scripting languages such as Node.js, PHP, etc Node.js frameworks, Installing Node.js, NodeJS Module System, Node.js as a File Server, Multiple exports, NodeJS Events and Streams, Node.js Frameworks, Node.js MySQL. Node.js in database applications such as Node.js MongoDB, Installing MongoDB Driver, Connecting MongoDB Database to the Web App, creating the database and displaying data in the front end, JavaScript Back-End Frameworks e.g. Angular-JS (Front-End), React.JS (Front-End), Vue.Js (Front-End), Meteor.Js (Back-End) and their features React framework and other server side frameworks. Images and sound. Dynamic client server applications using tools such as: PHP; PHP operators, loops, form processing and business logic, verifying username and password, connecting to a database, cookies. JSP; scripting components, standard actions, directives, custom tags libraries. Extensive library of ready to use plugins and code blocks. Introduction to Perl. Web hosting, shared hosting and performance monitoring. How to use web based APIs. Web optimization techniques.  \n",
      "\n",
      "\n",
      "\n",
      "Mode of delivery:\n",
      "The course will be taught by using lectures, practical demonstrations, tutorials and assignments.\n",
      "Instructional materials and/or equipment:\n",
      "Audio visual aids in lecture rooms, Resource persons, textbooks, hand-outs, LCD projectors, laptops/computers, discussion aids, and marker pen/white boards and e-journals.\n",
      "\n",
      "Course assessment:\n",
      "Course work (Assignment and CATs) \t\t 40%\n",
      "Final Examination \t\t\t\t\t 60%\n",
      "Total \t\t\t\t\t\t\t100%\n",
      "Core Reading Materials for the course:\n",
      "Internet & World Wide Web - How to Program (3rd Edition) by Harvey M. Deitel, Paul J. Deitel, Andrew B. Goldberg, Prentice Hall, 2004, ISBN: 0131450913\n",
      "\n",
      "Recommendation Reference Materials:\n",
      "\n",
      "Jennifer Niederst Robbins, 2012. Learning Web Design: A Beginner's Guide to HTML, CSS, JavaScript, and Web Graphics, 4th ed, O'Reilly Media.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "course_outline_text = extract_text_from_docx(\"C:/Users/Home/Desktop/Course Outline.docx\")\n",
    "print(course_outline_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a78cab-1886-4b28-a4c2-78dc089df521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Course Title': None, 'Course Duration': None, 'Weekly Breakdown': {}, 'Objectives': ['At the end of the course, students should be able to: Analyze and Design Algorithms, compare the performance of Algorithms.', 'Course content', 'Developing the skills of analysing the behaviour of algorithms. Detailed study of the basic notions of the design of algorithms and the underlying data structures. Major topics: the analysis with respect to average and worst case bahaviour and correctness of algorithms for internal sorting, pattern matching on strings, graph algorithms and methods such as recursive elimination, dynamic programming and program profiling. It will also cover Complexity problem, Structure, complexity and efficiency of algorithms. Examples are taken from numerical computations.', 'Mode of delivery', 'The course will be taught by using lectures, tutorials and assignments.', 'Instructional resources', 'Resource persons, textbooks, hand-outs, LCD projectors, laptops/computers, discussion aids, chalk/white boards, DVDs, and e-journals.', 'Course assessment', 'Course work (Assessments and tests) 30%', 'Final Examination 70%', 'Total 100%', 'Core Reading Materials for the course', 'i. McConnell J. (2007), Analysis of Algorithms (2nd Ed.). Jones & Bartlett Learning', 'Recommended Reference Materials', 'Robert S. &Wayne K. (2011), Algorithms (4th Ed.). Addison-Wesley Professional', ''], 'Recommended Materials': []}\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import pytesseract\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    return text\n",
    "\n",
    "def extract_course_info(text):\n",
    "    info = {\n",
    "        \"Course Title\": None,\n",
    "        \"Course Duration\": None,\n",
    "        \"Weekly Breakdown\": {},\n",
    "        \"Objectives\": [],\n",
    "        \"Recommended Materials\": []\n",
    "    }\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    course_title_pattern = re.compile(r\"(?i)Course\\s*Title:\\s*(.+)\")\n",
    "    duration_pattern = re.compile(r\"(?i)Duration:\\s*(.+)\")\n",
    "    week_pattern = re.compile(r\"(?i)Week\\s*(\\d+):?\\s*(.*)\")\n",
    "    objectives_pattern = re.compile(r\"(?i)Objectives?\")\n",
    "    materials_pattern = re.compile(r\"(?i)Recommended\\s*Materials?\")\n",
    "\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        title_match = course_title_pattern.search(line)\n",
    "        if title_match:\n",
    "            info[\"Course Title\"] = title_match.group(1)\n",
    "            continue\n",
    "\n",
    "        duration_match = duration_pattern.search(line)\n",
    "        if duration_match:\n",
    "            info[\"Course Duration\"] = duration_match.group(1)\n",
    "            continue\n",
    "\n",
    "        week_match = week_pattern.search(line)\n",
    "        if week_match:\n",
    "            week_number = int(week_match.group(1))\n",
    "            week_content = week_match.group(2).strip()\n",
    "            info[\"Weekly Breakdown\"][week_number] = week_content\n",
    "            continue\n",
    "\n",
    "        if objectives_pattern.search(line):\n",
    "            current_section = \"Objectives\"\n",
    "            continue\n",
    "        elif materials_pattern.search(line):\n",
    "            current_section = \"Recommended Materials\"\n",
    "            continue\n",
    "\n",
    "        if current_section == \"Objectives\":\n",
    "            info[\"Objectives\"].append(line)\n",
    "        elif current_section == \"Recommended Materials\":\n",
    "            info[\"Recommended Materials\"].append(line)\n",
    "\n",
    "    return info\n",
    "\n",
    "def process_course_outline(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")\n",
    "\n",
    "    return extract_course_info(text)\n",
    "\n",
    "# Example Usage\n",
    "file_path = \"C:/Users/Home/Desktop/Comp 3.2/Comp 302/SOEN 302 - COMP 302 course outline.docx\" \n",
    "course_data = process_course_outline(file_path)\n",
    "\n",
    "print(course_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c7b4bb-00a6-4e55-9f0b-36e23594ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Course Title': 'ADVANCED DATABASE SYSTEMS', 'Course Code': 'COMP 306', 'Course Duration': None, 'Weekly Breakdown': {1: 'MYSQL for relational databases', 2: 'MongoDB for NoSQL databases', 3: 'Date, C.(2006). An Introduction to Database Systems, 8 edition. Pearson publisher.', 4: 'Bradshaw, S., Brazil, E., Chodorow, K.(2019) MongoDB: The Definitive Guide:', 5: 'Distributed database management system', 6: 'NOSQL Databases', 7: 'Cloud Databases and Database as a Service (DBaaS)', 8: 'Emerging Trends and Research Topics', 978: '-0321826626'}, 'Objectives': [], 'Recommended Materials': []}\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import pytesseract\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    return text\n",
    "\n",
    "def extract_course_info(text):\n",
    "    info = {\n",
    "        \"Course Title\": None,\n",
    "        \"Course Code\": None,\n",
    "        \"Course Duration\": None,\n",
    "        \"Weekly Breakdown\": {},\n",
    "        \"Objectives\": [],\n",
    "        \"Recommended Materials\": []\n",
    "    }\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    course_title_pattern = re.compile(r\"(?i)([A-Z]{3,4}\\s*\\d{3})\\s*[-:]?\\s*(.+)\")\n",
    "    duration_pattern = re.compile(r\"(?i)Duration:\\s*(.+)\")\n",
    "\n",
    "    week_pattern = re.compile(r\"(?i)(?:Week\\s*|^)(\\d+)[\\.:]?\\s*(.*)\")\n",
    "    bullet_pattern = re.compile(r\"^[•*-]\\s*(.+)\")\n",
    "\n",
    "    objectives_pattern = re.compile(r\"(?i)Objectives?\")\n",
    "    materials_pattern = re.compile(r\"(?i)Recommended\\s*Materials?\")\n",
    "\n",
    "    current_section = None\n",
    "    week_counter = 0\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        title_match = course_title_pattern.search(line)\n",
    "        if title_match:\n",
    "            info[\"Course Code\"] = title_match.group(1)\n",
    "            info[\"Course Title\"] = title_match.group(2)\n",
    "            continue\n",
    "\n",
    "        duration_match = duration_pattern.search(line)\n",
    "        if duration_match:\n",
    "            info[\"Course Duration\"] = duration_match.group(1)\n",
    "            continue\n",
    "\n",
    "        week_match = week_pattern.search(line)\n",
    "        if week_match:\n",
    "            week_number = int(week_match.group(1))\n",
    "            week_content = week_match.group(2).strip()\n",
    "            info[\"Weekly Breakdown\"][week_number] = week_content\n",
    "            continue\n",
    "\n",
    "        bullet_match = bullet_pattern.search(line)\n",
    "        if bullet_match:\n",
    "            week_counter += 1\n",
    "            info[\"Weekly Breakdown\"][week_counter] = bullet_match.group(1)\n",
    "            continue\n",
    "\n",
    "        if objectives_pattern.search(line):\n",
    "            current_section = \"Objectives\"\n",
    "            continue\n",
    "        elif materials_pattern.search(line):\n",
    "            current_section = \"Recommended Materials\"\n",
    "            continue\n",
    "\n",
    "        if current_section == \"Objectives\":\n",
    "            info[\"Objectives\"].append(line)\n",
    "        elif current_section == \"Recommended Materials\":\n",
    "            info[\"Recommended Materials\"].append(line)\n",
    "\n",
    "    return info\n",
    "\n",
    "def process_course_outline(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")\n",
    "\n",
    "    return extract_course_info(text)\n",
    "\n",
    "file_path = \"C:/Users/Home/Desktop/Comp 3.2/Comp 306/COMP 306 CO.pdf\" \n",
    "course_data = process_course_outline(file_path)\n",
    "\n",
    "print(course_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d34a17-e7cd-4872-be4a-0c3327bd14b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_weeks_and_topics(text):\n",
    "    weeks = re.findall(r\"Week \\d+: (.+)\", text)\n",
    "    return weeks\n",
    "\n",
    "weekly_topics = extract_weeks_and_topics(course_outline_text)\n",
    "print(weekly_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d68ecff-9e61-4b84-9f77-011e24d0ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the first article:\n",
      " Welcome to r/learnmachinelearning - a community of learners and educators passionate about machine learning!\n",
      "This is your space to ask questions, share resources, and grow together in understanding ML concepts - from basic principles to advanced techniques.\n",
      "Whether you're writing your first neural network or diving into transformers, you'll find supportive peers here.\n",
      "For ML research, /r/machinelearning For resume review, /r/engineeringresumes For ML engineers, /r/mlengineeringMembers Online\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "def scrape_study_materials(topic):\n",
    "    api_key = \"4c3a39f406bb56f40ea28e7b2b59186206d9eb054c352ef8e19552d9900611f2\"\n",
    "    search = GoogleSearch({\n",
    "        \"q\": f\"{topic} study materials\",\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"api_key\": api_key\n",
    "    })\n",
    "    \n",
    "    results = search.get_dict()\n",
    "    links = []\n",
    "\n",
    "    for result in results.get(\"organic_results\", []):\n",
    "        if \"link\" in result:\n",
    "            links.append(result[\"link\"])\n",
    "    \n",
    "    return links[:5]\n",
    "\n",
    "def summarize_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        return article.summary\n",
    "    except Exception as e:\n",
    "        return f\"Error processing article: {e}\"\n",
    "\n",
    "study_materials = scrape_study_materials(\"Machine Learning\")\n",
    "\n",
    "if study_materials:\n",
    "    summary = summarize_article(study_materials[0])\n",
    "    print(\"Summary of the first article:\\n\", summary)\n",
    "else:\n",
    "    print(\"No study materials found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf5053-65af-4c05-a820-00539224f137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
