{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c742b58a-36cc-4401-b6cc-a524bb12d3da",
   "metadata": {},
   "source": [
    "# The main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea73c43-6c92-400d-8b92-5334ed89a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'important', 'for', 'ai', 'research']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Machine Learning is important for AI research!\"\n",
    "cleaned_text = clean_text(text)\n",
    "tokens = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d871d137-1f7e-4cce-b516-300a66e886a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.text for p in paragraphs[:5]])  # Get first 5 paragraphs\n",
    "        return text\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find relevant study material.\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e17586-009c-48a1-ab3d-d2dca5adf3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "modelUI = gr.Interface(\n",
    "    fn = scrape_wikipedia,\n",
    "    inputs = 'text',\n",
    "    outputs = gr.Textbox(label = 'The answer to the question requested is')\n",
    ")\n",
    "modelUI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09a72cc-3475-4092-9a9f-049d52368d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic Machine Learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n",
      " ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n",
      " Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n",
      " From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n",
      " The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = input(\"Enter the topic\")\n",
    "study_material = scrape_wikipedia(topic)\n",
    "print(study_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a386ae-36b1-406f-89e5-fd75370d7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(study_material)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8efbc-4387-45c8-8990-4f68e336806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def google_search(query):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query + \" site:edu\",\n",
    "        \"api_key\": \"\",\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    links = [result[\"link\"] for result in results[\"organic_results\"][:3]]\n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "query = \"Machine Learning course material\"\n",
    "search_results = google_search(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a92d8-7c32-44ae-a73e-a4f11427a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([p.text for p in paragraphs[:5]])\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for link in search_results:\n",
    "    study_material = scrape_website(link)\n",
    "    print(study_material)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53347b4-387d-4a62-bb5d-b880f2c66c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Stanford Q&A dataset\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# Print first sample\n",
    "print(squad['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a6692-b612-4b68-987e-97b587f04690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_text = extract_text_from_pdf(\"past_paper.pdf\")\n",
    "print(pdf_text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792874cf-ca19-4bf6-8b54-b72e93af7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(data, filename=\"study_data.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "study_data = {\"topic\": \"Machine Learning\", \"content\": pdf_text}\n",
    "save_to_json(study_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
