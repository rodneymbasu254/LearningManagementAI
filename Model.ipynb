{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c742b58a-36cc-4401-b6cc-a524bb12d3da",
   "metadata": {},
   "source": [
    "# The main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea73c43-6c92-400d-8b92-5334ed89a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'is', 'important', 'for', 'ai', 'research']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Machine Learning is important for AI research!\"\n",
    "cleaned_text = clean_text(text)\n",
    "tokens = tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d871d137-1f7e-4cce-b516-300a66e886a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia(topic):\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([p.text for p in paragraphs[:5]])  # Get first 5 paragraphs\n",
    "        return text\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find relevant study material.\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e17586-009c-48a1-ab3d-d2dca5adf3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.38.1, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "modelUI = gr.Interface(\n",
    "    fn = scrape_wikipedia,\n",
    "    inputs = 'text',\n",
    "    outputs = gr.Textbox(label = 'The answer to the question requested is')\n",
    ")\n",
    "modelUI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09a72cc-3475-4092-9a9f-049d52368d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic Machine Learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2]\n",
      " ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n",
      " Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n",
      " From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n",
      " The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = input(\"Enter the topic\")\n",
    "study_material = scrape_wikipedia(topic)\n",
    "print(study_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a386ae-36b1-406f-89e5-fd75370d7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(study_material)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf8efbc-4387-45c8-8990-4f68e336806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://cedar.buffalo.edu/~srihari/CSE574/', 'https://online.stanford.edu/courses/xcs229-machine-learning', 'https://ocw.mit.edu/courses/6-036-introduction-to-machine-learning-fall-2020/']\n"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def google_search(query):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query + \" site:edu\",\n",
    "        \"api_key\": \"4c3a39f406bb56f40ea28e7b2b59186206d9eb054c352ef8e19552d9900611f2\",\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    links = [result[\"link\"] for result in results[\"organic_results\"][:3]]\n",
    "    return links\n",
    "\n",
    "# Example usage\n",
    "query = \"Machine Learning course material\"\n",
    "search_results = google_search(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08a92d8-7c32-44ae-a73e-a4f11427a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reference textbooks for different parts of the course are \"Pattern Recognition and Machine Learning\" by Chris Bishop (Springer 2006) and  \"Probabilistic Graphical Models\" by Daphne Koller and Nir Friedman (MIT Press 2009) and \"Deep Learning\" by Goodfellow, Bengio and Courville (MIT Press 2016).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "Course topics are listed below with links to lecture slides and lecture videos. \n",
      "\n",
      "\n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      " \n",
      "\n",
      "The course is followed by two other courses, one focusing on Probabilistic Graphical Models\n",
      "\n",
      "and another on Deep Learning. \n",
      "\n",
      "\n",
      "The slides and videos were last updated in Fall 2020. \n",
      "Chapters 1-17 (Topic titles in Red) are more recently taught versions. \n",
      "\n",
      "\n",
      "This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement â¦ This course introduces principles, algorithms, and applications of machine learning from the point of view of modeling and prediction. It includes formulation of learning problems and concepts of representation, over-fitting, and generalization. These concepts are exercised in supervised learning and reinforcement learning, with applications to images and to temporal sequences. This course is part of the Open Learning Library, which is free to use.Â You have the option to sign up and enroll in the course if you want to track your progress, or you can view and use all the materials without enrolling.\n"
     ]
    }
   ],
   "source": [
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join([p.text for p in paragraphs[:5]])\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for link in search_results:\n",
    "    study_material = scrape_website(link)\n",
    "    print(study_material)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c53347b4-387d-4a62-bb5d-b880f2c66c00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load Stanford Q&A dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m squad \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Stanford Q&A dataset\n",
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "\n",
    "print(squad['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a6692-b612-4b68-987e-97b587f04690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"C:/Users/Home/Desktop/LearningManagementAI/LearningManagementAI/Docs/past_paper.pdf\")\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7189568-c48f-48ae-8e72-463a42548b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(pdf_text)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792874cf-ca19-4bf6-8b54-b72e93af7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(data, filename=\"study_data.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "study_data = {\"topic\": \"Machine Learning\", \"content\": pdf_text}\n",
    "save_to_json(study_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d015ee9-5f2c-4be9-b718-567653ed70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning field ai visit https example com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\W+\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"Machine learning is a field of AI. Visit https://example.com for more!\"\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c48ab1-07fd-49b1-a5c0-78812a1e4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KISII UNIVERSITY\n",
      "Faculty of Information Science and Technology (SIST)\n",
      "Department of Computing Sciences\n",
      "YEAR 3 SEM 1 BSC COMPUTER SCIENCE AND APPLIED COMPUTER\n",
      "SCIENCE\n",
      "SEPT-DEC, 2024\n",
      "COURSE OUTLINE\n",
      "Course Details\n",
      "Course Code COMP 303\n",
      "Course Name AUTOMATA THEORY\n",
      "Credit Hours 3.5\n",
      "Day/Time/Location THURSDAYS / 9am – 11 am / TC G2\n",
      "Lecturer Silas Momanyi Nyabuga\n",
      "Email Address smnyabuga@gmail.com\n",
      "Cell 0722-891-892\n",
      "Contact Hours: Lectures 30 and Practicals/Tutorials 30\n",
      "Purpose of the course:\n",
      "The course will provide students with knowledge and skills regarding\n",
      "fundamental concepts of Finite Automata, Regular Languages, and Pushdown\n",
      "Automata before moving onto Turing machines and Decidability.\n",
      "Expected Learning outcomes of the Course:\n",
      "Upon successful completion of this course, the student will be able to:\n",
      "i. Prove properties of languages, grammars and automata with rigorously\n",
      "formal mathematical methods;\n",
      "ii. Design automata, regular expressions and context-free grammars\n",
      "accepting or generating a certain language;\n",
      "iii. Simplify automata and context-free grammars;\n",
      "iv. Determine if a certain word belongs to a language;\n",
      "v. Define Turing machines performing simple tasks.\n",
      "vi. Explain and manipulate the different concepts in automata theory and\n",
      "formal languages such as formal proofs, (non-)deterministic automata,\n",
      "regular expressions, regular languages, context-free grammars, context-\n",
      "free languages, Turing machines;\n",
      "Course content:\n",
      "WK TOPIC PARTS\n",
      "1 Introduction to  Definition of automata theory\n",
      "automata theory\n",
      " Types of automata\n",
      " Deterministic (DFA) and Non-Deterministic\n",
      "Finite Automaton (NDFA)\n",
      " Related terminologies\n",
      "2 Classification of  Grammer\n",
      "Grammers\n",
      " Derivation from a Grammer\n",
      " Language generated by a Grammer\n",
      " Construction of a Grammer Generating\n",
      "Language\n",
      " Chomsky Classification of Grammars\n",
      "3. Regular Grammars  Regular Expressions\n",
      " Regular Sets..\n",
      " Identities Related to Regular Expressions\n",
      "4. Regular Grammars  Construction of an FA from an RE\n",
      " Pumping Lemma for Regular Languages\n",
      " Applications of Pumping Lemma\n",
      "5. Context-Free Grammars  Context-Free Grammar\n",
      " Closure Property of CFL\n",
      " Chomsky Normal Form\n",
      " Left and Right Recursive Grammars\n",
      "6. Pushdown Automata  Basic Structure of PDA\n",
      " Terminologies Related to PDA\n",
      " Parsing and PDA\n",
      " Pushdown Automata and Parsing\n",
      "7 CAT ONE\n",
      "8 Turing Machine  Definition\n",
      " Accepted Language and Decided Language\n",
      " Multi-track Turing Machine\n",
      " Non-Deterministic Turing machine\n",
      "9 Turing Machine  Turing Machine with Semi-infinite Tape\n",
      " Time and Space Complexity of a Turing\n",
      "Machine cable testers\n",
      " Linear Bounded Automata\n",
      "10 Decidability  Decidability and Decidable Languages\n",
      " Undecidable Languages\n",
      " TM Halting Problem\n",
      "11 Computability and  Turing Machine Halting Problem\n",
      "Undecidability\n",
      "CAT TWO\n",
      "12 Arden's Theorem  Assumptions for Applying Arden’s Theorem\n",
      " Linear bounded Automata\n",
      "13 Rice theorem  Rice theorem of Computation\n",
      "14 REVISION AND END OF SEMESTER EXAMINATIONS\n",
      "Mode of delivery:\n",
      "The course will be taught by using lectures, tutorials and assignments.\n",
      "Instructional Materials and/or Equipment:\n",
      "Textbooks, hand-outs, chalk/white boards.\n",
      "Course assessment\n",
      "Course work (Assessments and tests) 30%\n",
      "Final Examination 70%\n",
      "Total 100%\n",
      "Core Reading Materials for the course\n",
      "Recommended Reference Materials\n",
      "i. introduction to automata theory , Langauges, and Computation by John E\n",
      "Hopcroft, Rajeev Motwani and Jeffrey D.Ullman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "course_outline_text = extract_text_from_pdf(\"C:/Users/Home/Desktop/Comp3.1/Automata Theory303/COMP 303  AUTOMATA THEORY  COURSE OUTLINE.pdf\")\n",
    "print(course_outline_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5febd32b-7526-4817-ab34-0c552a3d62aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEB PROGRAMMING II (30/30; CF 3.5) Y3S2\n",
      "Prerequisite: BIT 202\n",
      "Contact Hours: Lectures 30 and Practical/Tutorials 30\n",
      "\n",
      "\n",
      "Purpose of the course:\n",
      "To provide the student with the knowledge and skills to construct dynamic and interactive websites for various types of business applications.\n",
      "Expected learning outcome\n",
      "By the end of the course units the learner should:\n",
      "Have a practical experience in the use of programming and scripting languages for web. Development\n",
      "Develop back end applications using JavaScript libraries such as Nodes.js, \n",
      "Develop a Data-Driven Node.js Web App and connect with the web database\n",
      "Develop dynamic and interactive web applications using various types of business information\n",
      "Each student will come up with a small website with full functionalities of a website\n",
      "Course Content\n",
      "Server Side Web application development using scripting languages such as Node.js, PHP, etc Node.js frameworks, Installing Node.js, NodeJS Module System, Node.js as a File Server, Multiple exports, NodeJS Events and Streams, Node.js Frameworks, Node.js MySQL. Node.js in database applications such as Node.js MongoDB, Installing MongoDB Driver, Connecting MongoDB Database to the Web App, creating the database and displaying data in the front end, JavaScript Back-End Frameworks e.g. Angular-JS (Front-End), React.JS (Front-End), Vue.Js (Front-End), Meteor.Js (Back-End) and their features React framework and other server side frameworks. Images and sound. Dynamic client server applications using tools such as: PHP; PHP operators, loops, form processing and business logic, verifying username and password, connecting to a database, cookies. JSP; scripting components, standard actions, directives, custom tags libraries. Extensive library of ready to use plugins and code blocks. Introduction to Perl. Web hosting, shared hosting and performance monitoring. How to use web based APIs. Web optimization techniques.  \n",
      "\n",
      "\n",
      "\n",
      "Mode of delivery:\n",
      "The course will be taught by using lectures, practical demonstrations, tutorials and assignments.\n",
      "Instructional materials and/or equipment:\n",
      "Audio visual aids in lecture rooms, Resource persons, textbooks, hand-outs, LCD projectors, laptops/computers, discussion aids, and marker pen/white boards and e-journals.\n",
      "\n",
      "Course assessment:\n",
      "Course work (Assignment and CATs) \t\t 40%\n",
      "Final Examination \t\t\t\t\t 60%\n",
      "Total \t\t\t\t\t\t\t100%\n",
      "Core Reading Materials for the course:\n",
      "Internet & World Wide Web - How to Program (3rd Edition) by Harvey M. Deitel, Paul J. Deitel, Andrew B. Goldberg, Prentice Hall, 2004, ISBN: 0131450913\n",
      "\n",
      "Recommendation Reference Materials:\n",
      "\n",
      "Jennifer Niederst Robbins, 2012. Learning Web Design: A Beginner's Guide to HTML, CSS, JavaScript, and Web Graphics, 4th ed, O'Reilly Media.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "course_outline_text = extract_text_from_docx(\"C:/Users/Home/Desktop/Course Outline.docx\")\n",
    "print(course_outline_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d34a17-e7cd-4872-be4a-0c3327bd14b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_weeks_and_topics(text):\n",
    "    weeks = re.findall(r\"Week \\d+: (.+)\", text)\n",
    "    return weeks\n",
    "\n",
    "weekly_topics = extract_weeks_and_topics(course_outline_text)\n",
    "print(weekly_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6446f065-35c0-4dd9-a5a3-724e71669afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 263 for command:\n",
      "        open question.mp3\n",
      "    The specified device is not open or is not recognized by MCI.\n",
      "\n",
      "    Error 263 for command:\n",
      "        close question.mp3\n",
      "    The specified device is not open or is not recognized by MCI.\n",
      "Failed to close the file: question.mp3\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 263 for command:\n        open question.mp3\n    The specified device is not open or is not recognized by MCI.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     playsound\u001b[38;5;241m.\u001b[39mplaysound(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Newton\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms First Law of Motion?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36mspeak\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m tts \u001b[38;5;241m=\u001b[39m gTTS(text)\n\u001b[0;32m      6\u001b[0m tts\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mplaysound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaysound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\playsound.py:72\u001b[0m, in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mwinCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopen \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     winCommand(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplay \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sound, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m wait\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     74\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\playsound.py:64\u001b[0m, in \u001b[0;36m_playsoundWin.<locals>.winCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     60\u001b[0m     exceptionMessage \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Error \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(errorCode) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for command:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m command\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     62\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m errorBuffer\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     63\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(exceptionMessage)\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(exceptionMessage)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 263 for command:\n        open question.mp3\n    The specified device is not open or is not recognized by MCI."
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text)\n",
    "    tts.save(\"question.mp3\")\n",
    "    playsound.playsound(\"question.mp3\")\n",
    "\n",
    "question = \"What is Newton's First Law of Motion?\"\n",
    "speak(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d68ecff-9e61-4b84-9f77-011e24d0ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/httpservice/retry/enablejs?sei=NWasZ_XJJZ6awbkPxYuhmAE', 'https://support.google.com/websearch']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_study_materials(topic):\n",
    "    url = f\"https://www.google.com/search?q={topic}+study+materials\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if \"http\" in a[\"href\"]]\n",
    "    return links[:5]\n",
    "\n",
    "study_materials = scrape_study_materials(\"Machine Learning\")\n",
    "print(study_materials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbda0df6-7fb2-4d61-9ebb-a16d6560aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid URL: \n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.scheme) and bool(parsed.netloc)\n",
    "\n",
    "url = \"\"\n",
    "if is_valid_url(url):\n",
    "    print(\"Valid url\")\n",
    "else:\n",
    "    print(\"Invalid URL:\", url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37b958c8-536c-47d3-ad59-ac48f5f46778",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArticleException",
     "evalue": "You must `download()` an article first!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     article\u001b[38;5;241m.\u001b[39mnlp()\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m article\u001b[38;5;241m.\u001b[39msummary\n\u001b[1;32m---> 10\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_materials\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36msummarize_article\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      4\u001b[0m article \u001b[38;5;241m=\u001b[39m Article(url)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#article.download()\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m article\u001b[38;5;241m.\u001b[39mnlp()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m article\u001b[38;5;241m.\u001b[39msummary\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\newspaper\\article.py:191\u001b[0m, in \u001b[0;36mArticle.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_doc \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\newspaper\\article.py:529\u001b[0m, in \u001b[0;36mArticle.throw_if_not_downloaded_verbose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse ArticleDownloadState -> log readable status\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m-> maybe throw ArticleException\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mNOT_STARTED:\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must `download()` an article first!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mFAILED_RESPONSE:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    532\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_exception_msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl))\n",
      "\u001b[1;31mArticleException\u001b[0m: You must `download()` an article first!"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "def summarize_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    return article.summary\n",
    "\n",
    "summary = summarize_article(study_materials[0])\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b35af9-6d3a-4977-9bcf-44a94c0538f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure Selenium\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run without opening a browser\n",
    "service = Service(\"chromedriver_path\")  # Replace with actual path\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Open the webpage\n",
    "url = \"https://example.com/article\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source after JS execution\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "print(text)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858cc133-8a1e-4d12-84eb-ba9fede4e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base_url = \"https://example.com\"\n",
    "relative_url = \"/httpservice/retry/enablejs?...\"\n",
    "absolute_url = urljoin(base_url, relative_url)\n",
    "print(absolute_url)  # Outputs: https://example.com/httpservice/retry/enablejs?...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
