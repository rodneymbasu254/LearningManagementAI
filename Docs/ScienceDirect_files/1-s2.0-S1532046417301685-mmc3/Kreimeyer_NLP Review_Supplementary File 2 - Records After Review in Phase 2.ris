TY  - JOUR
AB  - Knowledge about maternal history is critical for guiding certain aspects of newborn clinical care as well as for research on neonatal issues. However, often the only maternal history available in the newborn record is in the clinical notes. We are using data from the MIMIC-II database for a clinical study on newborns admitted to the intensive care unit. Important maternal data were only available in the newborn notes, so we developed a simple algorithm to extract those data. We manually derived patterns for maternal age, gravida/para status, and laboratory results by reviewing a small set of notes. Using regular expressions and specific filters for notes and results, we extracted maternal data with recall of 0.91-0.99 and precision of 0.95-1.0 for the 289 infants in our study. Our methods could be used with other research datasets and with clinical documentation systems to extract maternal data into a more useful, structured format.
AU  - Abhyankar, S.
AU  - Demner-Fushman, D.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21448055
4528
Abhyankar, Swapna Demner-Fushman, Dina Intramural NIH HHS/United States Research Support, N.I.H., Intramural United States AMIA Annu Symp Proc. 2013 Nov 16;2013:2-9. eCollection 2013.
PY  - 2013
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 2-9
ST  - A simple method to extract key maternal data from neonatal clinical notes
T2  - AMIA Annu Symp Proc
TI  - A simple method to extract key maternal data from neonatal clinical notes
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900117/pdf/amia_2013_symposium_002.pdf
VL  - 2013
ID  - 140
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To develop a generalizable method for identifying patient cohorts from electronic health record (EHR) data-in this case, patients having dialysis-that uses simple information retrieval (IR) tools. METHODS: We used the coded data and clinical notes from the 24,506 adult patients in the Multiparameter Intelligent Monitoring in Intensive Care database to identify patients who had dialysis. We used SQL queries to search the procedure, diagnosis, and coded nursing observations tables based on ICD-9 and local codes. We used a domain-specific search engine to find clinical notes containing terms related to dialysis. We manually validated the available records for a 10% random sample of patients who potentially had dialysis and a random sample of 200 patients who were not identified as having dialysis based on any of the sources. RESULTS: We identified 1844 patients that potentially had dialysis: 1481 from the three coded sources and 1624 from the clinical notes. Precision for identifying dialysis patients based on available data was estimated to be 78.4% (95% CI 71.9% to 84.2%) and recall was 100% (95% CI 86% to 100%). CONCLUSIONS: Combining structured EHR data with information from clinical notes using simple queries increases the utility of both types of data for cohort identification. Patients identified by more than one source are more likely to meet the inclusion criteria; however, including patients found in any of the sources increases recall. This method is attractive because it is available to researchers with access to EHR data and off-the-shelf IR tools.
AU  - Abhyankar, S.
AU  - Demner-Fushman, D.
AU  - Callaghan, F. M.
AU  - McDonald, C. J.
DA  - 2014
DO  - 10.1136/amiajnl-2013-001915. Epub 2014 Jan 2.
IS  - 5
KW  - eppi-reviewer4
N1  - 21444153
4586
PY  - 2014
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 801-7
ST  - Combining structured and unstructured data to identify a cohort of ICU patients who received dialysis
T2  - J Am Med Inform Assoc
TI  - Combining structured and unstructured data to identify a cohort of ICU patients who received dialysis
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4147606/pdf/amiajnl-2013-001915.pdf
VL  - 21
ID  - 16
ER  - 


TY  - JOUR
AB  - The SLIF project combines text-mining and image processing to extract structured information from biomedical literature. SLIF extracts images and their captions from published papers. The captions are automatically parsed for relevant biological entities (protein and cell type names), while the images are classified according to their type (e.g., micrograph or gel). Fluorescence microscopy images are further processed and classified according to the depicted subcellular localization. The results of this process can be queried online using either a user-friendly web-interface or an XML-based web-service. As an alternative to the targeted query paradigm, SLIF also supports browsing the collection based on latent topic models which are derived from both the annotated text and the image data. The SLIF web application, as well as labeled datasets used for training system components, is publicly available at http://slif.cbi.cmu.edu.
AU  - Ahmed, A.
AU  - Arnold, A.
AU  - Coelho, L. P.
AU  - Kangas, J.
AU  - Sheikh, A. S.
AU  - Xing, E.
AU  - Cohen, W.
AU  - Murphy, R. F.
DA  - 2010
IS  - 2-3
KW  - eppi-reviewer4
N1  - 21448224
5742
Ahmed, Amr Arnold, Andrew Coelho, Luis Pedro Kangas, Joshua Sheikh, Abdul-Saboor Xing, Eric Cohen, William Murphy, Robert F R01 GM078622/GM/NIGMS NIH HHS/United States Web Semant. 2010 Jul 1;8(2-3):151-154.
PY  - 2010
SN  - 1570-8268 (Print)
SP  - 151-154
ST  - Structured Literature Image Finder: Parsing Text and Figures in Biomedical Literature
T2  - Web Semant
TI  - Structured Literature Image Finder: Parsing Text and Figures in Biomedical Literature
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4075770/pdf/nihms494721.pdf
VL  - 8
ID  - 5
ER  - 


TY  - JOUR
AB  - Effective management of technology plays an essential role in defining the power of an arena. In many developed countries, such as Japan, healthcare facilities employ advanced information systems to capture daily healthcare records. We have collected thousands of incidence reports from the Japan Council for Quality Health Care, which is managed by the Ministry of Health, Labour and Welfare. The incident reports were electronically stored in written conversation format. We successfully distinguished the incident reports using artificial intelligence technology. Using natural language processers, Japanese vocabularies were systematically structured, captured and classified. As a preliminary, we explored the similarities between reports and the co-occurrence events of related characters among medical incidences. In this study, we took advantage of advanced health informatics approaches and available encrypted datasets to extract hidden knowledge associated with medical error events. The occurrence of medical errors, such as inappropriate oral medicine, may be statistically associated with and can be explained by event scenes. This data-driven research involves the intimate collaboration and technology management of statisticians, computer scientists, and practitioners - a concept known as "Convergence" and attempts to statistically understand the dynamics of medical incidences to enhance clinical patient safety situation awareness.
AU  - Akiyama, M.
AU  - Yamamoto, S.
AU  - Fujita, K.
AU  - Sakata, I.
AU  - Kajikawa, Y.
DA  - 2012
KW  - eppi-reviewer4
N1  - 21444927
8870
Akiyama, Masanori Yamamoto, Satoru Fujita, Katsuhide Sakata, Ichiro Kajikawa, Yuya Kocaoglu, DF Anderson, TR Daim, TU Conference of PICMET - Technology Management for Emerging Technologies (PICMET) JUL 29-AUG 02, 2012 Vancouver, CANADA Portland State Univ, Maseeh Coll Engn & Comp Sci, Dept Engn & Technol Management, Portland State Univ, Tourism Vancouver, Portland Int Ctr Management Engn & Technol (PICMET) Kajikawa, Yuya/C-1996-2015 Kajikawa, Yuya/0000-0003-3577-5167 978-1-890843-26-7
PY  - 2012
SP  - 2337-2346
ST  - Effective Learning and Knowledge Discovery Using Processed Medical Incident Reports
T2  - Picmet '12: Proceedings - Technology Management for Emerging Technologies
TI  - Effective Learning and Knowledge Discovery Using Processed Medical Incident Reports
UR  - <Go to ISI>://WOS:000317186402010
ID  - 153
ER  - 


TY  - JOUR
AB  - We describe Pathology Extraction Pipeline (PEP)--a new Open Health Natural Language Processing pipeline that we have developed for information extraction from pathology reports, with the goal of populating the extracted data into a research data warehouse. Specifically, we have built upon Medical Knowledge Analysis Tool pipeline (MedKATp), which is an extraction framework focused on pathology reports. Our particular contributions include additional customization and development on MedKATp to extract data elements and relationships from cancer pathology reports in richer detail than at present, an abstraction layer that provides significantly easier configuration of MedKATp for extraction tasks, and a machine-learning-based approach that makes the extraction more resilient to deviations from the common reporting format in a pathology reports corpus. We present experimental results demonstrating the effectiveness of our pipeline for information extraction in a real-world task, demonstrating performance improvement due to our approach for increasing extractor resilience to format deviation, and finally demonstrating the scalability of the pipeline across pathology reports for different cancer types.
AU  - Ashish, N.
AU  - Dahm, L.
AU  - Boicey, C.
DA  - 2014
DO  - 10.1177/1460458213494032. Epub 2014 Aug 25.
IS  - 4
KW  - eppi-reviewer4
N1  - 21448596
4321
PY  - 2014
SN  - 1741-2811 (Electronic) 1460-4582 (Linking)
SP  - 288-305
ST  - University of California, Irvine-Pathology Extraction Pipeline: the pathology extraction pipeline for information extraction from pathology reports
T2  - Health Informatics J
TI  - University of California, Irvine-Pathology Extraction Pipeline: the pathology extraction pipeline for information extraction from pathology reports
VL  - 20
ID  - 17
ER  - 


TY  - JOUR
AB  - This work is focused on mapping biomedical datasets to a common representation, as an integral part of data harmonization for integrated biomedical data access and sharing. We present GEM, an intelligent software assistant for automated data mapping across different datasets or from a dataset to a common data model. The GEM system automates data mapping by providing precise suggestions for data element mappings. It leverages the detailed metadata about elements in associated dataset documentation such as data dictionaries that are typically available with biomedical datasets. It employs unsupervised text mining techniques to determine similarity between data elements and also employs machine-learning classifiers to identify element matches. It further provides an active-learning capability where the process of training the GEM system is optimized. Our experimental evaluations show that the GEM system provides highly accurate data mappings (over 90% accuracy) for real datasets of thousands of data elements each, in the Alzheimer's disease research domain. Further, the effort in training the system for new datasets is also optimized. We are currently employing the GEM system to map Alzheimer's disease datasets from around the globe into a common representation, as part of a global Alzheimer's disease integrated data sharing and analysis network called GAAIN1. GEM achieves significantly higher data mapping accuracy for biomedical datasets compared to other state-of-the-art tools for database schema matching that have similar functionality. With the use of active-learning capabilities, the user effort in training the system is minimal.
AU  - Ashish, N.
AU  - Dewan, P.
AU  - Toga, A. W.
DA  - 2016
IS  - JAN2016
KW  - eppi-reviewer4
N1  - 21445513
7656
PY  - 2016
SN  - 1662-5196
SP  - 1-10
ST  - The GAAIN entity mapper: An active-learning system for medical data mapping
T2  - Frontiers in Neuroinformatics
TI  - The GAAIN entity mapper: An active-learning system for medical data mapping
UR  - http://www.embase.com/search/results?subaction=viewrecord&from=export&id=L609301366 http://dx.doi.org/10.3389/fninf.2015.00030 http://sfxhosted.exlibrisgroup.com/fda?sid=EMBASE&issn=16625196&id=doi:10.3389%2Ffninf.2015.00030&atitle=The+GAAIN+entity+mapper%3A+An+active-learning+system+for+medical+data+mapping&stitle=Front.+Neuroinformatics&title=Frontiers+in+Neuroinformatics&volume=9&issue=JAN2016&spage=1&epage=10&aulast=Ashish&aufirst=Naveen&auinit=N.&aufull=Ashish+N.&coden=&isbn=&pages=1-10&d
VL  - 9
ID  - 1
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Data from electronic healthcare records (EHR) can be used to monitor drug safety, but in order to compare and pool data from different EHR databases, the extraction of potential adverse events must be harmonized. In this paper, we describe the procedure used for harmonizing the extraction from eight European EHR databases of five events of interest deemed to be important in pharmacovigilance: acute myocardial infarction (AMI); acute renal failure (ARF); anaphylactic shock (AS); bullous eruption (BE); and rhabdomyolysis (RHABD). DESIGN: The participating databases comprise general practitioners' medical records and claims for hospitalization and other healthcare services. Clinical information is collected using four different disease terminologies and free text in two different languages. The Unified Medical Language System was used to identify concepts and corresponding codes in each terminology. A common database model was used to share and pool data and verify the semantic basis of the event extraction queries. Feedback from the database holders was obtained at various stages to refine the extraction queries. MEASUREMENTS: Standardized and age specific incidence rates (IRs) were calculated to facilitate benchmarking and harmonization of event data extraction across the databases. This was an iterative process. RESULTS: The study population comprised overall 19 647 445 individuals with a follow-up of 59 929 690 person-years (PYs). Age adjusted IRs for the five events of interest across the databases were as follows: (1) AMI: 60-148/100 000 PYs; (2) ARF: 3-49/100 000 PYs; (3) AS: 2-12/100 000 PYs; (4) BE: 2-17/100 000 PYs; and (5) RHABD: 0.1-8/100 000 PYs. CONCLUSIONS: The iterative harmonization process enabled a more homogeneous identification of events across differently structured databases using different coding based algorithms. This workflow can facilitate transparent and reproducible event extractions and understanding of differences between databases.
AU  - Avillach, P.
AU  - Coloma, P. M.
AU  - Gini, R.
AU  - Schuemie, M.
AU  - Mougin, F.
AU  - Dufour, J. C.
AU  - Mazzaglia, G.
AU  - Giaquinto, C.
AU  - Fornari, C.
AU  - Herings, R.
AU  - Molokhia, M.
AU  - Pedersen, L.
AU  - Fourrier-Reglat, A.
AU  - Fieschi, M.
AU  - Sturkenboom, M.
AU  - van der, Lei
AU  - Pariente, A.
AU  - Trifiro, G.
DA  - 2013
DO  - 10.1136/amiajnl-2012-000933. Epub 2012 Sep 6.
IS  - 1
KW  - eppi-reviewer4
N1  - 21445709
5067
PY  - 2013
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 184-92
ST  - Harmonization process for the identification of medical events in eight European healthcare databases: the experience from the EU-ADR project
T2  - J Am Med Inform Assoc
TI  - Harmonization process for the identification of medical events in eight European healthcare databases: the experience from the EU-ADR project
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3555316/pdf/amiajnl-2012-000933.pdf
VL  - 20
ID  - 18
ER  - 


TY  - JOUR
AB  - Despite a trend to formalize and codify medical information, natural language communications still play a prominent role in health care workflows, in particular when it comes to hand-overs between providers. Natural language processing (NLP) attempts to bridge the gap between informal, natural language information and coded, machine-interpretable data. This paper reports on a study that applies an advanced NLP method for the extraction of sentinel events in palliative care consult letters. Sentinel events are of interest to predict survival and trajectory for patients with acute palliative conditions. Our NLP method combines several novel characteristics, e.g., the consideration of topological knowledge structures sourced from an ontological terminology system (SNOMED CT). The method has been applied to the extraction of different types of sentinel events, including simple facts, temporal conditions, quantities, and degrees. A random selection of 215 anonymized consult letters was used for the study. The results of the NLP extraction were evaluated by comparison with coded sentinel event data captured independently by clinicians. The average accuracy of the automated extraction was 73.6%.
AU  - Barrett, N.
AU  - Weber-Jahnke, J. H.
AU  - Thai, V.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21445053
4752
Barrett, Neil Weber-Jahnke, Jens H Thai, Vincent Research Support, Non-U.S. Gov't Netherlands Stud Health Technol Inform. 2013;192:594-8.
PY  - 2013
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 594-8
ST  - Engineering natural language processing solutions for structured information from clinical text: extracting sentinel events from palliative care consult letters
T2  - Stud Health Technol Inform
TI  - Engineering natural language processing solutions for structured information from clinical text: extracting sentinel events from palliative care consult letters
VL  - 192
ID  - 168
ER  - 


TY  - JOUR
AB  - The capability for secondary analysis of electronic medical records and transactional insurance claims databases for epidemiological studies has existed for many decades. As the number of such electronic data repositories has increased, we have seen greater and greater numbers of studies to ensure the safety of medicinal products through formal hypothesis testing. As such databases have very different and often unique strengths and weaknesses, the ready linking of such data has led to more interest in conducting analysis across networks of many distinct databases, and with this increased ease of access there is heightened interest in determining their use for near-real time surveillance. However, there remains a limited body of research on such linkage through standardized data structure influences analyses, and differences between different data standardization approaches. Also while these databases are rich there are on occasion missing information of crucial importance for Pharmacovigilance. Increasingly, therefore, there are efforts to better enrich the existing data by nesting primary data collection and also to mine the narratives and other information not captured in the structured data. Results illustrating developments against these challenges will be presented. Specifically we describe the use of a Natural Language Processing (NLP) approach to extract clinical concepts from narratives on the US EMR data set called Humedica focussing on acute liver injury identification in an Irritable BowelDisease (IBD) population.We also present results showing comparison of different widely used (Mini-Sentinel and OMOP) approaches to standardization of database structures, focussed on analysis of the Humana US insurance claims database. Developing and linking data is very much an international development, we describe efforts are underway to actively consider the value of Common Data Models in China in facilitating this process. In the future we anticipate even more widespread development, linking and analysis of electronic data sources, and ever more effective leverage of such data; often across very different sources. We expect more widespread appreciation of the value of secondary analysis of existing Electronic Medical Record data whether for studies or surveillance activities, and better acceptance of the circumstances under which a primary data collection study, whether interventional or not, is a more appropriate analysis approach. Lastly we also anticipate more frequent nesting of primary data collection for Pharmacovigilance in existing healthcare data sets to produce enriched data sets for analysis.
AU  - Bate, A.
DA  - 2014
IS  - 10
KW  - eppi-reviewer4
N1  - 21448281
7662
PY  - 2014
SN  - 0114-5916
SP  - 889
ST  - Surveillance in medical records and claims-state of the art and future developments
T2  - Drug Safety
TI  - Surveillance in medical records and claims-state of the art and future developments
UR  - http://www.embase.com/search/results?subaction=viewrecord&from=export&id=L71702322 http://dx.doi.org/10.1007/s40264-014-0208-1 http://sfxhosted.exlibrisgroup.com/fda?sid=EMBASE&issn=01145916&id=doi:10.1007%2Fs40264-014-0208-1&atitle=Surveillance+in+medical+records+and+claims-state+of+the+art+and+future+developments&stitle=Drug+Saf.&title=Drug+Safety&volume=37&issue=10&spage=889&epage=&aulast=Bate&aufirst=A.&auinit=A.&aufull=Bate+A.&coden=&isbn=&pages=889-&date=2014&auinit1=A&auinitm=
VL  - 37
ID  - 141
ER  - 


TY  - JOUR
AB  - Injury Surveillance Systems based on traditional hospital records or clinical data have the advantage of being a well established, highly reliable source of information for making an active surveillance on specific injuries, like choking in children. However, they suffer the drawback of delays in making data available to the analysis, due to inefficiencies in data collection procedures. In this sense, the integration of clinical based registries with unconventional data sources like newspaper articles has the advantage of making the system more useful for early alerting. Usage of such sources is difficult since information is only available in the form of free natural-language documents rather than structured databases as required by traditional data mining techniques. Information Extraction (IE) addresses the problem of transforming a corpus of textual documents into a more structured database. In this paper, on a corpora of Italian newspapers articles related to choking in children due to ingestion/inhalation of foreign body we compared the performance of three IE algorithms- (a) a classical rule based system which requires a manual annotation of the rules; (ii) a rule based system which allows for the automatic building of rules; (b) a machine learning method based on Support Vector Machine. Although some useful indications are extracted from the newspaper clippings, this approach is at the time far from being routinely implemented for injury surveillance purposes.
AU  - Berchialla, P.
AU  - Scarinzi, C.
AU  - Snidero, S.
AU  - Rahim, Y.
AU  - Gregori, D.
DA  - 2012
DO  - 10.1007/s10916-010-9492-1. Epub 2010 Apr 27.
IS  - 2
KW  - eppi-reviewer4
N1  - 21446063
5714
PY  - 2012
SN  - 0148-5598 (Print) 0148-5598 (Linking)
SP  - 475-81
ST  - Information extraction approaches to unconventional data sources for "Injury Surveillance System": the case of newspapers clippings
T2  - J Med Syst
TI  - Information extraction approaches to unconventional data sources for "Injury Surveillance System": the case of newspapers clippings
UR  - http://download.springer.com/static/pdf/7/art%253A10.1007%252Fs10916-010-9492-1.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10916-010-9492-1&token2=exp=1469731258~acl=%2Fstatic%2Fpdf%2F7%2Fart%25253A10.1007%25252Fs10916-010-9492-1.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs10916-010-9492-1*~hmac=0d7204816b5c29ae7d36058929ec3aa5f64d409351c24bf06e4a41b075b8b084
VL  - 36
ID  - 196
ER  - 


TY  - JOUR
AB  - BACKGROUND: The semantic integration of biomedical resources is still a challenging issue which is required for effective information processing and data analysis. The availability of comprehensive knowledge resources such as biomedical ontologies and integrated thesauri greatly facilitates this integration effort by means of semantic annotation, which allows disparate data formats and contents to be expressed under a common semantic space. In this paper, we propose a multidimensional representation for such a semantic space, where dimensions regard the different perspectives in biomedical research (e.g., population, disease, anatomy and protein/genes). RESULTS: This paper presents a novel method for building multidimensional semantic spaces from semantically annotated biomedical data collections. This method consists of two main processes: knowledge and data normalization. The former one arranges the concepts provided by a reference knowledge resource (e.g., biomedical ontologies and thesauri) into a set of hierarchical dimensions for analysis purposes. The latter one reduces the annotation set associated to each collection item into a set of points of the multidimensional space. Additionally, we have developed a visual tool, called 3D-Browser, which implements OLAP-like operators over the generated multidimensional space. The method and the tool have been tested and evaluated in the context of the Health-e-Child (HeC) project. Automatic semantic annotation was applied to tag three collections of abstracts taken from PubMed, one for each target disease of the project, the Uniprot database, and the HeC patient record database. We adopted the UMLS Meta-thesaurus 2010AA as the reference knowledge resource. CONCLUSIONS: Current knowledge resources and semantic-aware technology make possible the integration of biomedical resources. Such an integration is performed through semantic annotation of the intended biomedical data resources. This paper shows how these annotations can be exploited for integration, exploration, and analysis tasks. Results over a real scenario demonstrate the viability and usefulness of the approach, as well as the quality of the generated multidimensional semantic spaces.
AU  - Berlanga, R.
AU  - Jimenez-Ruiz, E.
AU  - Nebot, V.
DA  - 2012
DO  - 10.1186/1471-2105-13-S1-S6.
KW  - eppi-reviewer4
N1  - 21445258
5245
PY  - 2012
SN  - 1471-2105 (Electronic) 1471-2105 (Linking)
SP  - S6
ST  - Exploring and linking biomedical resources through multidimensional semantic spaces
T2  - BMC Bioinformatics
TI  - Exploring and linking biomedical resources through multidimensional semantic spaces
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3471347/pdf/1471-2105-13-S1-S6.pdf
VL  - 13 Suppl 1
ID  - 6
ER  - 


TY  - JOUR
AB  - Identifying populations of heart failure (HF) patients is paramount to research efforts aimed at developing strategies to effectively reduce the burden of this disease. The use of electronic medical record (EMR) data for this purpose is challenging given the syndromic nature of HF and the need to distinguish HF with preserved or reduced ejection fraction. Using a gold standard cohort of manually abstracted cases, an EMR-driven phenotype algorithm based on structured and unstructured data was developed to identify all the cases. The resulting algorithm was executed in two cohorts from the Electronic Medical Records and Genomics (eMERGE) Network with a positive predictive value of >95 %. The algorithm was expanded to include three hierarchical definitions of HF (i.e., definite, probable, possible) based on the degree of confidence of the classification to capture HF cases in a whole population whereby increasing the algorithm utility for use in e-Epidemiologic research.
AU  - Bielinski, S. J.
AU  - Pathak, J.
AU  - Carrell, D. S.
AU  - Takahashi, P. Y.
AU  - Olson, J. E.
AU  - Larson, N. B.
AU  - Liu, H.
AU  - Sohn, S.
AU  - Wells, Q. S.
AU  - Denny, J. C.
AU  - Rasmussen-Torvik, L. J.
AU  - Pacheco, J. A.
AU  - Jackson, K. L.
AU  - Lesnick, T. G.
AU  - Gullerud, R. E.
AU  - Decker, P. A.
AU  - Pereira, N. L.
AU  - Ryu, E.
AU  - Dart, R. A.
AU  - Peissig, P.
AU  - Linneman, J. G.
AU  - Jarvik, G. P.
AU  - Larson, E. B.
AU  - Bock, J. A.
AU  - Tromp, G. C.
AU  - de, Andrade
AU  - Roger, V. L.
DA  - 2015
DO  - 10.1007/s12265-015-9644-2. Epub 2015 Jul 21.
IS  - 8
KW  - eppi-reviewer4
N1  - 21447878
3957
PY  - 2015
SN  - 1937-5395 (Electronic) 1937-5387 (Linking)
SP  - 475-83
ST  - A Robust e-Epidemiology Tool in Phenotyping Heart Failure with Differentiation for Preserved and Reduced Ejection Fraction: the Electronic Medical Records and Genomics (eMERGE) Network
T2  - J Cardiovasc Transl Res
TI  - A Robust e-Epidemiology Tool in Phenotyping Heart Failure with Differentiation for Preserved and Reduced Ejection Fraction: the Electronic Medical Records and Genomics (eMERGE) Network
UR  - http://download.springer.com/static/pdf/263/art%253A10.1007%252Fs12265-015-9644-2.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs12265-015-9644-2&token2=exp=1469637905~acl=%2Fstatic%2Fpdf%2F263%2Fart%25253A10.1007%25252Fs12265-015-9644-2.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs12265-015-9644-2*~hmac=324c964b5b11d7cb12b304b4035f880a2bb5f2038de483690b4fc7423fcafd4c
VL  - 8
ID  - 187
ER  - 


TY  - JOUR
AB  - Despite increased functionality for obtaining family history in a structured format within electronic health record systems, clinical notes often still contain this information. We developed and evaluated an Unstructured Information Management Application (UIMA)-based natural language processing (NLP) module for automated extraction of family history information with functionality for identifying statements, observations (e.g., disease or procedure), relative or side of family with attributes (i.e., vital status, age of diagnosis, certainty, and negation), and predication ("indicator phrases"), the latter of which was used to establish relationships between observations and family member. The family history NLP system demonstrated F-scores of 66.9, 92.4, 82.9, 57.3, 97.7, and 61.9 for detection of family history statements, family member identification, observation identification, negation identification, vital status, and overall extraction of the predications between family members and observations, respectively. While the system performed well for detection of family history statements and predication constituents, further work is needed to improve extraction of certainty and temporal modifications.
AU  - Bill, R.
AU  - Pakhomov, S.
AU  - Chen, E. S.
AU  - Winden, T. J.
AU  - Carter, E. W.
AU  - Melton, G. B.
DA  - 2014
KW  - eppi-reviewer4
N1  - 21443736
4596
Bill, Robert Pakhomov, Serguei Chen, Elizabeth S Winden, Tamara J Carter, Elizabeth W Melton, Genevieve B 1 R01 GM102282-01A1/GM/NIGMS NIH HHS/United States 1 R01 LM011364-01/LM/NLM NIH HHS/United States 8UL1TR000114-02/TR/NCATS NIH HHS/United States R01 GM102282/GM/NIGMS NIH HHS/United States R01 LM011364/LM/NLM NIH HHS/United States U54 RR026066-01A2/RR/NCRR NIH HHS/United States UL1 TR000114/TR/NCATS NIH HHS/United States Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2014 Nov 14;2014:1709-17. eCollection 2014.
PY  - 2014
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1709-17
ST  - Automated extraction of family history information from clinical notes
T2  - AMIA Annu Symp Proc
TI  - Automated extraction of family history information from clinical notes
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4419952/pdf/1986799.pdf
VL  - 2014
ID  - 94
ER  - 


TY  - JOUR
AB  - Medical reports are, quite often, written and stored in computer systems in a non-structured free text form. As a consequence, the information contained in these reports is not easily available and it is not possible to take it into account by medical decision support systems. We propose a methodology to automatically process and analyze medical reports, identifying concepts and their instances, and populating a new ontology. This methodology is based in natural language processing techniques using linguistic and statistical information. The proposed system was applied successfully to a set of medical reports from the Veterinary Hospital of the University of Evora.
AU  - Borrego, L.
AU  - Quaresma, P.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21447543
4904
Borrego, Luis Quaresma, Paulo Netherlands Stud Health Technol Inform. 2013;183:201-5.
PY  - 2013
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 201-5
ST  - Processing medical reports to automatically populate ontologies
T2  - Stud Health Technol Inform
TI  - Processing medical reports to automatically populate ontologies
VL  - 183
ID  - 188
ER  - 


TY  - JOUR
AB  - Inter and intra-observer variability in mammographic interpretation is a challenging problem, and decision support systems (DSS) may be helpful to reduce variation in practice. Since radiology reports are created as unstructured text reports, Natural language processing (NLP) techniques are needed to extract structured information from reports in order to provide the inputs to DSS. Before creating NLP systems, producing high quality annotated data set is essential. The goal of this project is to develop an annotation schema to guide the information extraction tasks needed from free-text mammography reports.
AU  - Bozkurt, S.
AU  - Gulkesen, K. H.
AU  - Rubin, D.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21443587
4779
Bozkurt, Selen Gulkesen, Kemal Hakan Rubin, Daniel Netherlands Stud Health Technol Inform. 2013;190:183-5.
PY  - 2013
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 183-5
ST  - Annotation for information extraction from mammography reports
T2  - Stud Health Technol Inform
TI  - Annotation for information extraction from mammography reports
VL  - 190
ID  - 179
ER  - 


TY  - JOUR
AB  - OBJECTIVE: The opportunity to integrate clinical decision support systems into clinical practice is limited due to the lack of structured, machine readable data in the current format of the electronic health record. Natural language processing has been designed to convert free text into machine readable data. The aim of the current study was to ascertain the feasibility of using natural language processing to extract clinical information from >76,000 breast pathology reports. APPROACH AND PROCEDURE: Breast pathology reports from three institutions were analyzed using natural language processing software (Clearforest, Waltham, MA) to extract information on a variety of pathologic diagnoses of interest. Data tables were created from the extracted information according to date of surgery, side of surgery, and medical record number. The variety of ways in which each diagnosis could be represented was recorded, as a means of demonstrating the complexity of machine interpretation of free text. RESULTS: There was widespread variation in how pathologists reported common pathologic diagnoses. We report, for example, 124 ways of saying invasive ductal carcinoma and 95 ways of saying invasive lobular carcinoma. There were >4000 ways of saying invasive ductal carcinoma was not present. Natural language processor sensitivity and specificity were 99.1% and 96.5% when compared to expert human coders. CONCLUSION: We have demonstrated how a large body of free text medical information such as seen in breast pathology reports, can be converted to a machine readable format using natural language processing, and described the inherent complexities of the task.
AU  - Buckley, J. M.
AU  - Coopey, S. B.
AU  - Sharko, J.
AU  - Polubriaginof, F.
AU  - Drohan, B.
AU  - Belli, A. K.
AU  - Kim, E. M.
AU  - Garber, J. E.
AU  - Smith, B. L.
AU  - Gadd, M. A.
AU  - Specht, M. C.
AU  - Roche, C. A.
AU  - Gudewicz, T. M.
AU  - Hughes, K. S.
DA  - 2012
DO  - 10.4103/2153-3539.97788. Epub 2012 Jun 30.
KW  - eppi-reviewer4
N1  - 21445352
5076
PY  - 2012
SN  - 2153-3539 (Electronic)
SP  - 23
ST  - The feasibility of using natural language processing to extract clinical information from breast pathology reports
T2  - J Pathol Inform
TI  - The feasibility of using natural language processing to extract clinical information from breast pathology reports
UR  - http://www.jpathinformatics.org/article.asp?issn=2153-3539;year=2012;volume=3;issue=1;spage=23;epage=23;aulast=Buckley
VL  - 3
ID  - 142
ER  - 


TY  - JOUR
AB  - OBJECTIVES: Data extraction from original study reports is a time-consuming, error-prone process in systematic review development. Information extraction (IE) systems have the potential to assist humans in the extraction task, however majority of IE systems were not designed to work on Portable Document Format (PDF) document, an important and common extraction source for systematic review. In a PDF document, narrative content is often mixed with publication metadata or semi-structured text, which add challenges to the underlining natural language processing algorithm. Our goal is to categorize PDF texts for strategic use by IE systems. METHODS: We used an open-source tool to extract raw texts from a PDF document and developed a text classification algorithm that follows a multi-pass sieve framework to automatically classify PDF text snippets (for brevity, texts) into TITLE, ABSTRACT, BODYTEXT, SEMISTRUCTURE, and METADATA categories. To validate the algorithm, we developed a gold standard of PDF reports that were included in the development of previous systematic reviews by the Cochrane Collaboration. In a two-step procedure, we evaluated (1) classification performance, and compared it with machine learning classifier, and (2) the effects of the algorithm on an IE system that extracts clinical outcome mentions. RESULTS: The multi-pass sieve algorithm achieved an accuracy of 92.6%, which was 9.7% (p<0.001) higher than the best performing machine learning classifier that used a logistic regression algorithm. F-measure improvements were observed in the classification of TITLE (+15.6%), ABSTRACT (+54.2%), BODYTEXT (+3.7%), SEMISTRUCTURE (+34%), and MEDADATA (+14.2%). In addition, use of the algorithm to filter semi-structured texts and publication metadata improved performance of the outcome extraction system (F-measure +4.1%, p=0.002). It also reduced of number of sentences to be processed by 44.9% (p<0.001), which corresponds to a processing time reduction of 50% (p=0.005). CONCLUSIONS: The rule-based multi-pass sieve framework can be used effectively in categorizing texts extracted from PDF documents. Text classification is an important prerequisite step to leverage information extraction from PDF documents.
AU  - Bui, D. D.
AU  - Del, Fiol
AU  - Jonnalagadda, S.
DA  - 2016
DO  - 10.1016/j.jbi.2016.03.026. Epub 2016 Apr 1.
KW  - eppi-reviewer4
N1  - 21447291
3832
PY  - 2016
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 141-8
ST  - PDF text classification to leverage information extraction from publication reports
T2  - J Biomed Inform
TI  - PDF text classification to leverage information extraction from publication reports
UR  - http://ac.els-cdn.com/S153204641630017X/1-s2.0-S153204641630017X-main.pdf?_tid=b40958b0-5416-11e6-8e16-00000aacb361&acdnat=1469636896_61fc7b6e59e00ce554b689a994ec20c1
VL  - 61
ID  - 111
ER  - 


TY  - JOUR
AB  - Genetic association studies have rapidly become a major tool for identifying the genetic basis of common human diseases. The advent of cost-effective genotyping coupled with large collections of samples linked to clinical outcomes and quantitative traits now make it possible to systematically characterize genotype-phenotype relationships in diverse populations and extensive datasets. To capitalize on these advancements, the Epidemiologic Architecture for Genes Linked to Environment (EAGLE) project, as part of the collaborative Population Architecture using Genomics and Epidemiology (PAGE) study, accesses two collections: the National Health and Nutrition Examination Surveys (NHANES) and BioVU, Vanderbilt University's biorepository linked to de-identified electronic medical records. We describe herein the workflows for accessing and using the epidemiologic (NHANES) and clinical (BioVU) collections, where each workflow has been customized to reflect the content and data access limitations of each respective source. We also describe the process by which these data are generated, standardized, and shared for meta-analysis among the PAGE study sites. As a specific example of the use of BioVU, we describe the data mining efforts to define cases and controls for genetic association studies of common cancers in PAGE. Collectively, the efforts described here are a generalized outline for many of the successful approaches that can be used in the era of high-throughput genotype-phenotype associations for moving biomedical discovery forward to new frontiers of data generation and analysis.
AU  - Bush, W. S.
AU  - Boston, J.
AU  - Pendergrass, S. A.
AU  - Dumitrescu, L.
AU  - Goodloe, R.
AU  - Brown-Gentry, K.
AU  - Wilson, S.
AU  - McClellan, B.
AU  - Torstenson, E.
AU  - Basford, M. A.
AU  - Spencer, K. L.
AU  - Ritchie, M. D.
AU  - Crawford, D. C.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21445042
4889
Bush, William S Boston, Jonathan Pendergrass, Sarah A Dumitrescu, Logan Goodloe, Robert Brown-Gentry, Kristin Wilson, Sarah McClellan, Bob Torstenson, Eric Basford, Melissa A Spencer, Kylee L Ritchie, Marylyn D Crawford, Dana C T32 GM080178/GM/NIGMS NIH HHS/United States U01 HG004798/HG/NHGRI NIH HHS/United States UL1 TR000445/TR/NCATS NIH HHS/United States Research Support, American Recovery and Reinvestment Act Research Support, N.I.H., Extramural Research Support, Non-U.S. Gov't United States Pac Symp Biocomput. 2013:373-84.
PY  - 2013
SN  - 2335-6936 (Print)
SP  - 373-84
ST  - Enabling high-throughput genotype-phenotype associations in the Epidemiologic Architecture for Genes Linked to Environment (EAGLE) project as part of the Population Architecture using Genomics and Epidemiology (PAGE) study
T2  - Pac Symp Biocomput
TI  - Enabling high-throughput genotype-phenotype associations in the Epidemiologic Architecture for Genes Linked to Environment (EAGLE) project as part of the Population Architecture using Genomics and Epidemiology (PAGE) study
ID  - 112
ER  - 


TY  - JOUR
AB  - BACKGROUND: Data in medical records have in part been recorded in structured and coded forms for some decades. However, the patient history is as yet largely recorded in an uncoded format. There is a need to consider the optimal balance of use of free text and coded data in the patient history. This review protocol summarises our plans to identify, critically appraise and synthesise evidence relating to approaches taken to introduce structure and coding within patient histories in electronic health records, and the empirically demonstrated benefits and risks of structuring and coding of patient histories in health records. OBJECTIVES: To determine how structured and coded data are being introduced for the recording of patient histories, the benefits observed where structuring and coding have been introduced and the risks encountered when structuring and coding are introduced. METHODS: We will search the following databases for evidence of published and unpublished material: CINAHL; EMBASE; Google Scholar; IndMED; LILACS; MEDLINE; NIHR; Paklit and PsycINFO. We will, depending on the study designs employed, use the Cochrane EPOC, Joanna Briggs Institute (JBI) and Newcastle-Ottawa instruments to critically appraise studies. Data synthesis is likely to be undertaken using a narrative approach, although meta-analysis will also be undertaken if appropriate and if the data allow this. RESULTS: This protocol should represent a reproducible approach to reviewing the literature regarding structuring and coding in patient histories. We anticipate that we will be able to report results in early 2011. CONCLUSION: The review should offer increased clarity and direction on the optimal balance between structuring/coding and free text recording of data relating to the patient history.
AU  - Byrne, E.
AU  - Fernando, B.
AU  - Kalra, D.
AU  - Sheikh, A.
DA  - 2010
IS  - 3
KW  - eppi-reviewer4
N1  - 21443815
5885
Byrne, Emma Fernando, Bernard Kalra, Dipak Sheikh, Aziz England Inform Prim Care. 2010;18(3):197-203.
PY  - 2010
SN  - 1476-0320 (Print) 1475-9985 (Linking)
SP  - 197-203
ST  - The benefits and risks of structuring and coding of patient histories in the electronic clinical record: protocol for a systematic review
T2  - Inform Prim Care
TI  - The benefits and risks of structuring and coding of patient histories in the electronic clinical record: protocol for a systematic review
VL  - 18
ID  - 19
ER  - 


TY  - JOUR
AB  - The migration of imaging reports to electronic medical record systems holds great potential in terms of advancing radiology research and practice by leveraging the large volume of data continuously being updated, integrated, and shared. However, there are significant challenges as well, largely due to the heterogeneity of how these data are formatted. Indeed, although there is movement toward structured reporting in radiology (ie, hierarchically itemized reporting with use of standardized terminology), the majority of radiology reports remain unstructured and use free-form language. To effectively "mine" these large datasets for hypothesis testing, a robust strategy for extracting the necessary information is needed. Manual extraction of information is a time-consuming and often unmanageable task. "Intelligent" search engines that instead rely on natural language processing (NLP), a computer-based approach to analyzing free-form text or speech, can be used to automate this data mining task. The overall goal of NLP is to translate natural human language into a structured format (ie, a fixed collection of elements), each with a standardized set of choices for its value, that is easily manipulated by computer programs to (among other things) order into subcategories or query for the presence or absence of a finding. The authors review the fundamentals of NLP and describe various techniques that constitute NLP in radiology, along with some key applications.
AU  - Cai, T.
AU  - Giannopoulos, A. A.
AU  - Yu, S.
AU  - Kelil, T.
AU  - Ripley, B.
AU  - Kumamaru, K. K.
AU  - Rybicki, F. J.
AU  - Mitsouras, D.
DA  - 2016
DO  - 10.1148/rg.2016150080.
IS  - 1
KW  - eppi-reviewer4
N1  - 21446940
3847
PY  - 2016
SN  - 1527-1323 (Electronic) 0271-5333 (Linking)
SP  - 176-91
ST  - Natural Language Processing Technologies in Radiology Research and Clinical Applications
T2  - Radiographics
TI  - Natural Language Processing Technologies in Radiology Research and Clinical Applications
VL  - 36
ID  - 20
ER  - 


TY  - CHAP
A2  - Gao, J.
A2  - Dubitzky, W.
A2  - Wu, C.
A2  - Liebman, M.
A2  - Alhaij, R.
A2  - Ungar, L.
A2  - Christianson, A.
A2  - Hu, X.
AB  - The ability to connect the dots in structured background knowledge and also across scientific literature has been demonstrated as a critical aspect of knowledge discovery. It is not unreasonable therefore to expect that connecting-the-dots across massive amounts of healthcare data may also lead to new insights that could impact diagnosis, treatment and overall patient care. Of critical importance is the observation that while structured Electronic Medical Records (EMR) are useful sources of health information, it is often the unstructured clinical texts such as progress notes and discharge summaries that contain rich, updated and granular information. Hence, by coupling structured EMR data with data from unstructured clinical texts, more holistic patient records, needed for connecting the dots, can be obtained. Unfortunately, free-text progress notes are fraught with a lack of proper grammatical structure, and contain liberal use of jargon and abbreviations, together with frequent misspellings. While these notes still serve their intended purpose for medical care, automatically extracting semantic information from them is a complex task. Overcoming this complexity could mean that evidence-based support for structured EMR data using unstructured clinical texts, can be provided. In this work therefore, we explore a pattern-based approach for extracting Smoker Semantic Types (SST) from unstructured clinical notes, in order to enable evidence-based resolution of SSTs asserted in structured EMRs using SSTs extracted from unstructured clinical notes. Our findings support the notion that information present in unstructured clinical text can be used to complement structured healthcare data. This is a crucial observation towards creating comprehensive longitudinal patient models for connecting-the-dots and providing better overall patient care.
AU  - Cameron, D.
AU  - Bhagwan, V.
AU  - Sheth, A. P.
KW  - eppi-reviewer4
N1  - 21448476
8862
Cameron, Delroy Bhagwan, Varun Sheth, Amit P. BIBMW IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW) OCT 04-07, 2012 Philadelphia, PA IEEE, IEEE Comp Soc (CS), Natl Sci Fdn (NSF), Omic Soft Corp, IEEE Comp Soc Tech Comm Bioinformat
PY  - 2012
SN  - 2163-6966 978-1-4673-2746-6; 978-1-4673-2745-9
ST  - Towards Comprehensive Longitudinal Healthcare Data Capture
T2  - 2012 Ieee International Conference on Bioinformatics and Biomedicine Workshops
TI  - Towards Comprehensive Longitudinal Healthcare Data Capture
UR  - <Go to ISI>://WOS:000320379600034
ID  - 113
ER  - 


TY  - JOUR
AB  - OBJECTIVES: This paper presents a methodology for recovering and decomposing Swanson's Raynaud Syndrome-Fish Oil hypothesis semi-automatically. The methodology leverages the semantics of assertions extracted from biomedical literature (called semantic predications) along with structured background knowledge and graph-based algorithms to semi-automatically capture the informative associations originally discovered manually by Swanson. Demonstrating that Swanson's manually intensive techniques can be undertaken semi-automatically, paves the way for fully automatic semantics-based hypothesis generation from scientific literature. METHODS: Semantic predications obtained from biomedical literature allow the construction of labeled directed graphs which contain various associations among concepts from the literature. By aggregating such associations into informative subgraphs, some of the relevant details originally articulated by Swanson have been uncovered. However, by leveraging background knowledge to bridge important knowledge gaps in the literature, a methodology for semi-automatically capturing the detailed associations originally explicated in natural language by Swanson, has been developed. RESULTS: Our methodology not only recovered the three associations commonly recognized as Swanson's hypothesis, but also decomposed them into an additional 16 detailed associations, formulated as chains of semantic predications. Altogether, 14 out of the 19 associations that can be attributed to Swanson were retrieved using our approach. To the best of our knowledge, such an in-depth recovery and decomposition of Swanson's hypothesis has never been attempted. CONCLUSION: In this work therefore, we presented a methodology to semi-automatically recover and decompose Swanson's RS-DFO hypothesis using semantic representations and graph algorithms. Our methodology provides new insights into potential prerequisites for semantics-driven Literature-Based Discovery (LBD). Based on our observations, three critical aspects of LBD include: (1) the need for more expressive representations beyond Swanson's ABC model; (2) an ability to accurately extract semantic information from text; and (3) the semantic integration of scientific literature and structured background knowledge.
AU  - Cameron, D.
AU  - Bodenreider, O.
AU  - Yalamanchili, H.
AU  - Danh, T.
AU  - Vallabhaneni, S.
AU  - Thirunarayan, K.
AU  - Sheth, A. P.
AU  - Rindflesch, T. C.
DA  - 2013
DO  - 10.1016/j.jbi.2012.09.004. Epub 2012 Sep 28.
IS  - 2
KW  - eppi-reviewer4
N1  - 21445675
5045
PY  - 2013
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 238-51
ST  - A graph-based recovery and decomposition of Swanson's hypothesis using semantic predications
T2  - J Biomed Inform
TI  - A graph-based recovery and decomposition of Swanson's hypothesis using semantic predications
UR  - http://ac.els-cdn.com/S1532046412001517/1-s2.0-S1532046412001517-main.pdf?_tid=bc744c3a-5416-11e6-b1da-00000aab0f01&acdnat=1469636910_207d171078d2c5b3a56f4ad7305febfb
VL  - 46
ID  - 7
ER  - 


TY  - BOOK
A2  - Cordeiro, J.
A2  - Filipe, J.
A2  - Hammoudi, S.
AB  - Nowadays a huge amount of raw medical data is generated. These data, analyzed with data mining techniques, could be used to produce new knowledge. Unluckily such tasks need skilled data analysts, and not so many researchers in medical field are also data mining experts. In this paper we present a web based system for knowledge discovery assistance in Medicine able to advice a medical researcher in this kind of tasks. The experiment specifications are expressed in a formal language we have defined. The system GUI helps the user in the their composition. The system plans a Knowledge Discovery Process (KDP). The KDP is designed on the basis of rules in a knowledge base. Finally the system executes the KDP and produces a model as result. The system works through the co-operation of different web services specialized in different tasks. The choice of web services is based on the semantic of their functionalities, according to a common OWL ontology. The system is still under development.
AU  - Cannella, V.
AU  - Russo, G.
AU  - Pirrone, R.
DA  - 2008
KW  - eppi-reviewer4
N1  - 21448483
9449
Cannella, Vincenzo Russo, Giuseppe Pirrone, Roberto 4th International Conference on Web Information Systems and Technologies MAY 04-07, 2008 Funchal, PORTUGAL Inst Syst & Technologies Informat, Control & Commun, Univ Madeira, Workflow Management Coalit Pirrone, Roberto/0000-0001-9453-510X
PY  - 2008
SN  - 978-989-8111-26-5
SP  - 129-134
ST  - Towards MKDA: A data mining semantic web service
T2  - Webist 2008: Proceedings of the Fourth International Conference on Web Information Systems and Technologies, Vol 1
TI  - Towards MKDA: A data mining semantic web service
UR  - <Go to ISI>://WOS:000259260900019
ID  - 53
ER  - 


TY  - RPRT
AB  - RIVF09 is hosted by the Danang University of Technology; sponsored by the IEEE Vietnam Section in cooperation with IEEE Region 10. Topics covered are 1) Information and Knowledge Management: Integrated information systems, Semantic concepts and ontologies, Metadata management and standards, Multimedia systems, E-commerce and business integration solutions, Architecture, Security & Performance of information systems, Applications in public health , logistics, etc., 2) Computational Intelligence: Language and speech processing, Knowledge representation and reasoning, Soft computing, Knowledge discovery and data mining, Machine learning, Computational biology, Agent-based systems, Semantic web, Information extraction and retrieval, 3) Communication and Networking: Network architecture, Network, Mobile and ubiquitous computing, Quantum networks and communications, 4). Modeling and Computer Simulation: Data visualization, Image processing, Virtual reality, Behaviour simulation, Scientific computing, Human-computer interface, 5) Applied Operational Research and Optimization: Linear programming, Integer programming, Dynamic programming, Constraint propagation, Heuristics, Applications in control, Production, Scheduling.
AU  - Cao, Tru
AU  - Kutsche, Ralf-Detlef
AU  - Demaille, Akim
DA  - 2009
KW  - eppi-reviewer4
N1  - 21509265
10399
PB  - JAPAN ADVANCED INST OF SCIENCE AND TECHNOLOGY ISHIKAWA (JAPAN)
PY  - 2009
RP  - 21509265
10399
SN  - ADA506140, AOARD, XC, CSP-091033, AOARD, FA2386-09-1-1033
SP  - 374
ST  - Proceedings of the IEEE-RIVF International Conference on Computing & Communication Technologies (7th) held in Danang, Vietnam on 13-17 July 2009
TI  - Proceedings of the IEEE-RIVF International Conference on Computing & Communication Technologies (7th) held in Danang, Vietnam on 13-17 July 2009
UR  - http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=5174598
ID  - 8
ER  - 


TY  - RPRT
AB  - Topics include: Theoretic foundations, Novel models and algorithms, Association analysis, Clustering, Classification Statistical methods for data mining, Data pre-processing, Feature extraction and selection Post-prcessing including quality assessment and validation, Mining heterogeneous/multi-source data, Mining sequential data Mining spatial and temporal data, Mining unstructured and semi-structured data, Mining graph and network data Mining social networks, Mining high dimensional data, Mining uncertain data, Mining imbalanced data Mining dynamic/streaming data, Mining behavioral data, Mining multimedia data, Mining scientific data Privacy preserving data mining, Anomaly detection, Fraud and risk analysis, Security and intrusion detection Visual data mining, Interactive and online mining,Ubiquitous knowledge discovery and agent-based data mining Integration of data warehousing, OLAP and data mining, Parallel, distributed, and cloud-based high performance data mining Opinion mining and sentiment analysis, Human, domain, organizational and social factors in data mining Applications to healthcare, bioinformatics, computational chemistry, finance, eco-informatics, marketing, gaming, etc.
AU  - Cao, Tru
AU  - Lim, Ee-Peng
AU  - Zhou, Zhi-Hua
AU  - Ho, Tu-Bao
AU  - Cheung, David
AU  - Motoda, Hiroshi
DA  - 2015
KW  - eppi-reviewer4
N1  - 21509194
11283
PB  - JAPAN ADVANCED INST OF SCIENCE AND TECHNOLOGY ISHIKAWA (JAPAN)
PY  - 2015
RP  - 21509194
11283
SN  - ADA627674, AOARD, XC, CSP-1510A023, AOARD
SP  - 797
ST  - The Pacific-Asia Conference on Knowledge Discovery and Data Mining (19th), PAKDD 2015, Held in Ho Chi Minh City, Vietnam on May 19-22, 2015
TI  - The Pacific-Asia Conference on Knowledge Discovery and Data Mining (19th), PAKDD 2015, Held in Ho Chi Minh City, Vietnam on May 19-22, 2015
ID  - 175
ER  - 


TY  - JOUR
AB  - Clustering real-world data is a challenging task, since many real-data collections are characterized by an inherent sparseness and variable distribution. An appealing domain that generates such data collections is the medical care scenario where collected data include a large cardinality of patient records and a variety of medical treatments usually adopted for a given disease pathology. This paper proposes a two-phase data mining methodology (MLC) to iteratively analyze different dataset portions and locally identify groups of objects with common properties. Discovered cohesive clusters are then analyzed using sequential patterns to characterize temporal relationships among data features. To support an automatic classification of new data objects within one of the discovered groups, a classification model is created starting from the computed cluster set. A mobile application has been also designed and developed to visualize and update data under analysis as well as categorizing new unlabeled data objects. The experimental evaluation conducted on real datasets in the medical care scenario showed the effectiveness of MLC to discover interesting knowledge items and to easily exploit them through a mobile application. Results have been also discussed from a medical perspective. (C) 2016 Elsevier Ltd. All rights reserved.
AU  - Cerquitelli, T.
AU  - Chiusano, S.
AU  - Xiao, X.
DA  - 2016
KW  - eppi-reviewer4
N1  - 21445241
7676
Cerquitelli, Tania Chiusano, Silvia Xiao, Xin
PY  - 2016
SN  - 0957-4174
SP  - 297-312
ST  - Exploiting clustering algorithms in a multiple-level fashion: A comparative study in the medical care scenario
T2  - Expert Systems with Applications
TI  - Exploiting clustering algorithms in a multiple-level fashion: A comparative study in the medical care scenario
UR  - <Go to ISI>://WOS:000374811000023
VL  - 55
ID  - 37
ER  - 


TY  - JOUR
AB  - Many previous studies have employed predictive models for a specific disease, but fail to note that humans often suffer from not only one disease, but associated diseases as well. Because these associated multiple diseases might have reciprocal effects, and abnormalities in physiological indicators can indicate multiple associated diseases, common risk factors can be used to predict the multiple associated diseases. This approach provides a more effective and comprehensive forecasting mechanism for preventive medicine. This paper proposes a two-phase analysis procedure to simultaneously predict hypertension and hyperlipidemia. Firstly, we used six data mining approaches to select the individual risk factors of these two diseases, and then determined the common risk factors using the voting principle. Next, we used the Multivariate Adaptive Regression Splines (MARS) method to construct a multiple predictive model for hypertension and hyperlipidemia. This study uses data from a physical examination center database in Taiwan that includes 2048 subjects. The proposed analysis procedure shows that the common risk factors of hypertension and hyperlipidemia are Systolic Blood Pressure (SBP), Triglycerides, Uric Acid (UA), Glutamate Pyruvate Transaminase (GPT), and gender. The proposed multi-diseases predictor method has a classification accuracy rate of 93.07%. The results of this paper provide an effective and appropriate methodology for simultaneously predicting hypertension and hyperlipidemia. (C) 2010 Elsevier Ltd. All rights reserved.
AU  - Chang, C. D.
AU  - Wang, C. C.
AU  - Jiang, B. C.
DA  - 2011
IS  - 5
KW  - eppi-reviewer4
N1  - 21448665
9523
Chang, Cheng-Ding Wang, Chien-Chih Jiang, Bernard C. Wang, Chien-Chih/0000-0002-7869-3965
PY  - 2011
SN  - 0957-4174
SP  - 5507-5513
ST  - Using data mining techniques for multi-diseases prediction modeling of hypertension and hyperlipidemia by common risk factors
T2  - Expert Systems with Applications
TI  - Using data mining techniques for multi-diseases prediction modeling of hypertension and hyperlipidemia by common risk factors
UR  - <Go to ISI>://WOS:000287419900094
VL  - 38
ID  - 42
ER  - 


TY  - JOUR
AB  - Comparative effectiveness research (CER) provides evidence for the relative effectiveness and risks of different treatment options and informs decisions made by healthcare providers, payers, and pharmaceutical companies. CER data come from retrospective analyses as well as prospective clinical trials. Here, we describe the development of a text-mining pipeline based on natural language processing (NLP) that extracts key information from three different trial data sources: NIH ClinicalTrials.gov, WHO International Clinical Trials Registry Platform (ICTRP), and Citeline Trialtrove. The pipeline leverages tailored terminologies to produce an integrated and structured output, capturing any trials in which pharmaceutical products of interest are compared with another therapy. The timely information alerts generated by this system provide the earliest and most complete picture of emerging clinical research.
AU  - Chang, M. P.
AU  - Chang, M.
AU  - Reed, J. Z.
AU  - Milward, D.
AU  - Xu, J. J.
AU  - Cornell, W. D.
DA  - 2016
IS  - 3
KW  - eppi-reviewer4
N1  - 21444692
7722
Chang, Meiping Chang, Man Reed, Jane Z. Milward, David Xu, Jinghai James Cornell, Wendy D.
PY  - 2016
SN  - 1359-6446
SP  - 473-480
ST  - Developing timely insights into comparative effectiveness research with a text-mining pipeline
T2  - Drug Discovery Today
TI  - Developing timely insights into comparative effectiveness research with a text-mining pipeline
UR  - <Go to ISI>://WOS:000373750500013
http://ac.els-cdn.com/S1359644616000325/1-s2.0-S1359644616000325-main.pdf?_tid=c0b39c42-5416-11e6-9990-00000aacb361&acdnat=1469636918_15915ab634920e3cf16546367027d466
VL  - 21
ID  - 64
ER  - 


TY  - JOUR
AB  - Adverse drug events (ADEs) are a public health issue. Their detection usually relies on voluntary reporting or medical chart reviews. The objective of this paper is to automatically detect cases of ADEs by data mining. 115,447 complete past hospital stays are extracted from six French, Danish, and Bulgarian hospitals using a common data model including diagnoses, drug administrations, laboratory results, and free-text records. Different kinds of outcomes are traced, and supervised rule induction methods (decision trees and association rules) are used to discover ADE detection rules, with respect to time constraints. The rules are then filtered, validated, and reorganized by a committee of experts. The rules are described in a rule repository, and several statistics are automatically computed in every medical department, such as the confidence, relative risk, and median delay of outcome appearance. 236 validated ADE-detection rules are discovered; they enable to detect 27 different kinds of outcomes. The rules use a various number of conditions related to laboratory results, diseases, drug administration, and demographics. Some rules involve innovative conditions, such as drug discontinuations.
AU  - Chazard, E.
AU  - Ficheur, G.
AU  - Bernonville, S.
AU  - Luyckx, M.
AU  - Beuscart, R.
DA  - 2011
DO  - 10.1109/TITB.2011.2165727. Epub 2011 Aug 22.
IS  - 6
KW  - eppi-reviewer4
N1  - 21444544
5412
PY  - 2011
SN  - 1558-0032 (Electronic) 1089-7771 (Linking)
SP  - 823-30
ST  - Data mining to generate adverse drug events detection rules
T2  - IEEE Trans Inf Technol Biomed
TI  - Data mining to generate adverse drug events detection rules
UR  - http://ieeexplore.ieee.org/ielx5/4233/6083501/05995169.pdf?tp=&arnumber=5995169&isnumber=6083501
VL  - 15
ID  - 65
ER  - 


TY  - JOUR
AB  - Recent initiatives have emphasized the potential role of Electronic Health Record (EHR) systems for improving tobacco use assessment and cessation. In support of these efforts, the goal of the present study was to examine tobacco use documentation in the EHR with an emphasis on free-text. Three coding schemes were developed and applied to analyze 525 tobacco use entries, including structured fields and a free-text comment field, from the social history module of an EHR system to characterize: (1) potential reasons for using free-text, (2) contents within the free-text, and (3) data quality issues. Free-text was most commonly used due to limitations for describing tobacco use amount (23.2%), frequency (26.9%), and start or quit dates (28.2%) as well as secondhand smoke exposure (17.9%) using a variety of words and phrases. The collective results provide insights for informing system enhancements, user training, natural language processing, and standards for tobacco use documentation.
AU  - Chen, E. S.
AU  - Carter, E. W.
AU  - Sarkar, I. N.
AU  - Winden, T. J.
AU  - Melton, G. B.
DA  - 2014
KW  - eppi-reviewer4
N1  - 21445224
4602
Chen, Elizabeth S Carter, Elizabeth W Sarkar, Indra Neil Winden, Tamara J Melton, Genevieve B R01 LM011364/LM/NLM NIH HHS/United States R01LM011364/LM/NLM NIH HHS/United States Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2014 Nov 14;2014:366-74. eCollection 2014.
PY  - 2014
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 366-74
ST  - Examining the use, contents, and quality of free-text tobacco use documentation in the Electronic Health Record
T2  - AMIA Annu Symp Proc
TI  - Examining the use, contents, and quality of free-text tobacco use documentation in the Electronic Health Record
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4419995/pdf/1985098.pdf
VL  - 2014
ID  - 143
ER  - 


TY  - JOUR
AB  - Through Natural Language Processing (NLP) techniques, information can be extracted from clinical narratives for a variety of applications (e.g., patient management). While the complex and nested output of NLP systems can be expressed in standard formats, such as the eXtensible Markup Language (XML), these representations may not be directly suitable for certain end-users or applications. The availability of a aeuro tabular' format that simplifies the content and structure of NLP output may facilitate the dissemination and use by users who are more familiar with common spreadsheet, database, or statistical tools. In this paper, we describe the knowledge-based design of a tabular representation for NLP output and development of a transformation program for the structured output of MedLEE, an NLP system at our institution. Through an evaluation, we found that the simplified tabular format is comparable to existing more complex NLP formats in effectiveness for identifying clinical conditions in narrative reports.
AU  - Chen, E. S.
AU  - Hripcsak, G.
AU  - Friedman, C.
DA  - 2006
KW  - eppi-reviewer4
N1  - 21444825
6560
Chen, Elizabeth S Hripcsak, George Friedman, Carol LM006910/LM/NLM NIH HHS/United States LM007659/LM/NLM NIH HHS/United States LM008635/LM/NLM NIH HHS/United States R01 LM006910/LM/NLM NIH HHS/United States R01 LM007659/LM/NLM NIH HHS/United States R01 LM008635/LM/NLM NIH HHS/United States Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2006:126-30.
PY  - 2006
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 126-30
ST  - Disseminating natural language processed clinical narratives
T2  - AMIA Annu Symp Proc
TI  - Disseminating natural language processed clinical narratives
ID  - 169
ER  - 


TY  - JOUR
AB  - The growing amount and availability of electronic health record (EHR) data present enhanced opportunities for discovering new knowledge about diseases. In the past decade, there has been an increasing number of data and text mining studies focused on the identification of disease associations (e.g., disease-disease, disease-drug, and disease-gene) in structured and unstructured EHR data. This chapter presents a knowledge discovery framework for mining the EHR for disease knowledge and describes each step for data selection, preprocessing, transformation, data mining, and interpretation/validation. Topics including natural language processing, standards, and data privacy and security are also discussed in the context of this framework.
AU  - Chen, E. S.
AU  - Sarkar, I. N.
DA  - 2014
DO  - 10.1007/978-1-4939-0709-0_15.
KW  - eppi-reviewer4
N1  - 21446710
4455
PY  - 2014
SN  - 1940-6029 (Electronic) 1064-3745 (Linking)
SP  - 269-86
ST  - Mining the electronic health record for disease knowledge
T2  - Methods Mol Biol
TI  - Mining the electronic health record for disease knowledge
VL  - 1159
ID  - 66
ER  - 


TY  - JOUR
AB  - Nationwide Children's Hospital established an i2b2 (Informatics for Integrating Biology & the Bed-side) application for sleep disorder cohort identification. Discrete data were gleaned from semi-structured sleep study reports. The system showed to work more efficiently than the traditional manual chart review method, and it also enabled searching capabilities that were previously not possible. Objective: We report on the development and implementation of the sleep disorder i2b2 cohort identification system using natural language processing of semi-structured documents. Methods: We developed a natural language processing approach to automatically parse concepts and their values from semi-structured sleep study documents. Two parsers were developed: a regular expression parser for extracting numeric concepts and a NLP based tree parser for extracting textual concepts. Concepts were further organized into i2b2 ontologies based on document structures and in-domain knowledge. Results: 26,550 concepts were extracted with 99% being textual concepts. 1.01 million facts were extracted from sleep study documents such as demographic information, sleep study lab results, medications, procedures, diagnoses, among others. The average accuracy of terminology parsing was over 83% when comparing against those by experts. The system is capable of capturing both standard and non-standard terminologies. The time for cohort identification has been reduced significantly from a few weeks to a few seconds. Conclusion: Natural language processing was shown to be powerful for quickly converting large amount of semi-structured or unstructured clinical data into discrete concepts, which in combination of intuitive domain specific ontologies, allows fast and effective interactive cohort identification through the i2b2 platform for research and clinical use.
AU  - Chen, W.
AU  - Kowatch, R.
AU  - Lin, S.
AU  - Splaingard, M.
AU  - Huang, Y.
DA  - 2015
IS  - 2
KW  - eppi-reviewer4
N1  - 21446156
8056
Chen, W. Kowatch, R. Lin, S. Splaingard, M. Huang, Y.
PY  - 2015
SN  - 1869-0327
SP  - 345-363
ST  - Interactive Cohort Identification of Sleep Disorder Patients Using Natural Language Processing and i2b2
T2  - Applied Clinical Informatics
TI  - Interactive Cohort Identification of Sleep Disorder Patients Using Natural Language Processing and i2b2
UR  - <Go to ISI>://WOS:000358661700002
https://aci.schattauer.de/contents/archive/issue/2201/manuscript/24410/download.html
VL  - 6
ID  - 205
ER  - 


TY  - JOUR
AB  - Systems approaches to analyzing disease phenotype networks in combination with protein functional interaction networks have great potential in illuminating disease pathophysiological mechanisms. While many genetic networks are readily available, disease phenotype networks remain largely incomplete. In this study, we built a large-scale Disease Manifestation Network (DMN) from 50,543 highly accurate disease-manifestation semantic relationships in the United Medical Language System (UMLS). Our new phenotype network contains 2305 nodes and 373,527 weighted edges to represent the disease phenotypic similarities. We first compared DMN with the networks representing genetic relationships among diseases, and demonstrated that the phenotype clustering in DMN reflects common disease genetics. Then we compared DMN with a widely-used disease phenotype network in previous gene discovery studies, called mimMiner, which was extracted from the textual descriptions in Online Mendelian Inheritance in Man (OMIM). We demonstrated that DMN contains different knowledge from the existing phenotype data source. Finally, a case study on Marfan syndrome further proved that DMN contains useful information and can provide leads to discover unknown disease causes. Integrating DMN in systems approaches with mimMiner and other data offers the opportunities to predict novel disease genetics. We made DMN publicly available at nlp/case.edu/public/data/DMN.
AU  - Chen, Y.
AU  - Zhang, X.
AU  - Zhang, G. Q.
AU  - Xu, R.
DA  - 2015
DO  - 10.1016/j.jbi.2014.09.007. Epub 2014 Sep 30.
KW  - eppi-reviewer4
N1  - 21444185
4259
PY  - 2015
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 113-20
ST  - Comparative analysis of a novel disease phenotype network based on clinical manifestations
T2  - J Biomed Inform
TI  - Comparative analysis of a novel disease phenotype network based on clinical manifestations
UR  - http://ac.els-cdn.com/S1532046414002172/1-s2.0-S1532046414002172-main.pdf?_tid=c4e43498-5416-11e6-9bb9-00000aab0f6c&acdnat=1469636925_d0848a3efa1bd7600d97bd5d2808e935
VL  - 53
ID  - 38
ER  - 


TY  - JOUR
AB  - RATIONALE AND OBJECTIVES: The Liver Imaging Reporting and Data System (LI-RADS) can enhance communication between radiologists and clinicians if applied consistently. We identified an institutional need to improve liver imaging report standardization and developed handheld and desktop software to serve this purpose. MATERIALS AND METHODS: We developed two complementary applications that implement the LI-RADS schema. A mobile application for iOS devices written in the Objective-C language allows for rapid characterization of hepatic observations under a variety of circumstances. A desktop application written in the Java language allows for comprehensive observation characterization and standardized report text generation. We chose the applications' languages and feature sets based on the computing resources of target platforms, anticipated usage scenarios, and ease of application installation, deployment, and updating. RESULTS: Our primary results are the publication of the core source code implementing the LI-RADS algorithm and the availability of the applications for use worldwide via our website, http://www.liradsapp.com/. The Java application is free open-source software that can be integrated into nearly any vendor's reporting system. The iOS application is distributed through Apple's iTunes App Store. Observation categorizations of both programs have been manually validated to be correct. The iOS application has been used to characterize liver tumors during multidisciplinary conferences of our institution, and several faculty members, fellows, and residents have adopted the generated text of Java application into their diagnostic reports. CONCLUSIONS: Although these two applications were developed for the specific reporting requirements of our liver tumor service, we intend to apply this development model to other diseases as well. Through semiautomated structured report generation and observation characterization, we aim to improve patient care while increasing radiologist efficiency.
AU  - Clark, T. J.
AU  - McNeeley, M. F.
AU  - Maki, J. H.
DA  - 2014
DO  - 10.1016/j.acra.2013.12.014.
IS  - 4
KW  - eppi-reviewer4
N1  - 21444633
4512
PY  - 2014
SN  - 1878-4046 (Electronic) 1076-6332 (Linking)
SP  - 491-506
ST  - Design and implementation of handheld and desktop software for the structured reporting of hepatic masses using the LI-RADS schema
T2  - Acad Radiol
TI  - Design and implementation of handheld and desktop software for the structured reporting of hepatic masses using the LI-RADS schema
UR  - http://ac.els-cdn.com/S107663321300593X/1-s2.0-S107663321300593X-main.pdf?_tid=c9d54730-5416-11e6-abfb-00000aacb360&acdnat=1469636933_41eaf6fba4fc7cd96ec5f93beb9d959a
VL  - 21
ID  - 9
ER  - 


TY  - JOUR
AB  - Background: Juvenile idiopathic arthritis is the most common rheumatic disease in children. Chronic uveitis is a common and serious comorbid condition of juvenile idiopathic arthritis, with insidious presentation and potential to cause blindness. Knowledge of clinical associations will improve risk stratification. Based on clinical observation, we hypothesized that allergic conditions are associated with chronic uveitis in juvenile idiopathic arthritis patients. Methods: This study is a retrospective cohort study using Stanford's clinical data warehouse containing data from Lucile Packard Children's Hospital from 2000-2011 to analyze patient characteristics associated with chronic uveitis in a large juvenile idiopathic arthritis cohort. Clinical notes in patients under 16 years of age were processed via a validated text analytics pipeline. Bivariate-associated variables were used in a multivariate logistic regression adjusted for age, gender, and race. Previously reported associations were evaluated to validate our methods. The main outcome measure was presence of terms indicating allergy or allergy medications use overrepresented in juvenile idiopathic arthritis patients with chronic uveitis. Residual text features were then used in unsupervised hierarchical clustering to compare clinical text similarity between patients with and without uveitis. Results: Previously reported associations with uveitis in juvenile idiopathic arthritis patients (earlier age at arthritis diagnosis, oligoarticular-onset disease, antinuclear antibody status, history of psoriasis) were reproduced in our study. Use of allergy medications and terms describing allergic conditions were independently associated with chronic uveitis. The association with allergy drugs when adjusted for known associations remained significant (OR 2.54, 95% Cl 1.22-5.4). Conclusions: This study shows the potential of using a validated text analytics pipeline on clinical data warehouses to examine practice-based evidence for evaluating hypotheses formed during patient care. Our study reproduces four known associations with uveitis development in juvenile idiopathic arthritis patients, and reports a new association between allergic conditions and chronic uveitis in juvenile idiopathic arthritis patients.
AU  - Cole, T. S.
AU  - Frankovich, J.
AU  - Iyer, S.
AU  - LePendu, P.
AU  - Bauer-Mehren, A.
AU  - Shah, N. H.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21447550
8424
Cole, Tyler S. Frankovich, Jennifer Iyer, Srinivasan LePendu, Paea Bauer-Mehren, Anna Shah, Nigam H.
PY  - 2013
SN  - 1546-0096
ST  - Profiling risk factors for chronic uveitis in juvenile idiopathic arthritis: a new model for EHR-based research
T2  - Pediatric Rheumatology
TI  - Profiling risk factors for chronic uveitis in juvenile idiopathic arthritis: a new model for EHR-based research
UR  - <Go to ISI>://WOS:000328822300001
VL  - 11
ID  - 67
ER  - 


TY  - JOUR
AB  - Automated encoding of free-text clinical narratives using concepts from terminological systems is widely performed. However, the majority of natural language processing (NLP) tools and terminological systems involve the English language. As parts of the NLP process are language independent, and tools for various languages are available, an overview is needed to determine the applicability to performing NLP of Dutch medical texts. To this end an inventory of tools is created. A literature study and internet search were performed to describe available components for a Dutch NLP system, enabling to encode Dutch text as structured SNOMED CT output without the need to translate SNOMED CT in Dutch. We have found 31 papers, describing a variety of NLP frameworks and tools for the various NLP components for processing English and Dutch free text. Most of them are suitable for English free text, some of them are (also) usable for Dutch. To enable automated encoding of Dutch free text narratives, further research is needed to create a spelling checker, a negation detector, a domain-specific abbreviation/acronym list, and a concept mapper (to map Dutch terms to concepts in a terminological system). Furthermore evaluation of performance for the Dutch 'medical' language is needed.
AU  - Cornet, R.
AU  - Van, Eldik
AU  - De, Keizer
DA  - 2012
KW  - eppi-reviewer4
N1  - 21446202
5098
Cornet, Ronald Van Eldik, Armand De Keizer, Nicolette Netherlands Stud Health Technol Inform. 2012;180:245-9.
PY  - 2012
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 245-9
ST  - Inventory of tools for Dutch clinical language processing
T2  - Stud Health Technol Inform
TI  - Inventory of tools for Dutch clinical language processing
VL  - 180
ID  - 206
ER  - 


TY  - JOUR
AB  - High amount of relevant information is contained in reports stored in the electronic patient records and associated metadata. R-oogle is a project aiming at developing information retrieval engines adapted to these reports and designed for clinicians. The system consists in a data warehouse (full-text reports and structured data) imported from two different hospital information systems. Information retrieval is performed using metadata-based semantic and full-text search methods (as Google). Applications may be biomarkers identification in a translational approach, search of specific cases, and constitution of cohorts, professional practice evaluation, and quality control assessment.
AU  - Cuggia, M.
AU  - Garcelon, N.
AU  - Campillo-Gimenez, B.
AU  - Bernicot, T.
AU  - Laurent, J. F.
AU  - Garin, E.
AU  - Happe, A.
AU  - Duvauferrier, R.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21447905
5402
Cuggia, Marc Garcelon, Nicolas Campillo-Gimenez, Boris Bernicot, Thomas Laurent, Jean-Francois Garin, Etienne Happe, Andre Duvauferrier, Regis Research Support, Non-U.S. Gov't Netherlands Stud Health Technol Inform. 2011;169:584-8.
PY  - 2011
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 584-8
ST  - Roogle: an information retrieval engine for clinical data warehouse
T2  - Stud Health Technol Inform
TI  - Roogle: an information retrieval engine for clinical data warehouse
VL  - 169
ID  - 39
ER  - 


TY  - JOUR
AB  - Sudden Unexpected Death in Epilepsy (SUDEP) is a poorly understood phenomenon. Patient cohorts to power statistical studies in SUDEP need to be drawn from multiple centers due to the low rate of reported SUDEP incidences. But the current practice of manual chart review of Epilepsy Monitoring Units (EMU) patient discharge summaries is time-consuming, tedious, and not scalable for large studies. To address this challenge in the multi-center NIH-funded Prevention and Risk Identification of SUDEP Mortality (PRISM) Project, we have developed the Epilepsy Data Extraction and Annotation (EpiDEA) system for effective processing of discharge summaries. EpiDEA uses a novel Epilepsy and Seizure Ontology (EpSO), which has been developed based on the International League Against Epilepsy (ILAE) classification system, as the core knowledge resource. By extending the cTAKES natural language processing tool developed at the Mayo Clinic, EpiDEA implements specialized functions to address the unique challenges of processing epilepsy and seizure-related clinical free text in discharge summaries. The EpiDEA system was evaluated on a corpus of 104 discharge summaries from the University Hospitals Case Medical Center EMU and achieved an overall precision of 93.59% and recall of 84.01% with an F-measure of 88.53%. The results were compared against a gold standard created by two epileptologists. We demonstrate the use of EpiDEA for cohort identification through use of an intuitive visual query interface that can be directly used by clinical researchers.
AU  - Cui, L.
AU  - Bozorgi, A.
AU  - Lhatoo, S. D.
AU  - Zhang, G. Q.
AU  - Sahoo, S. S.
DA  - 2012
KW  - eppi-reviewer4
N1  - 21445081
4925
Cui, Licong Bozorgi, Alireza Lhatoo, Samden D Zhang, Guo-Qiang Sahoo, Satya S 1-P20-NS076965-01/NS/NINDS NIH HHS/United States P20 NS076965/NS/NINDS NIH HHS/United States UL1 RR024989/RR/NCRR NIH HHS/United States UL1 TR000439/TR/NCATS NIH HHS/United States UL1TR000439/TR/NCATS NIH HHS/United States Evaluation Studies Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2012;2012:1191-200. Epub 2012 Nov 3.
PY  - 2012
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1191-200
ST  - EpiDEA: extracting structured epilepsy and seizure information from patient discharge summaries for cohort identification
T2  - AMIA Annu Symp Proc
TI  - EpiDEA: extracting structured epilepsy and seizure information from patient discharge summaries for cohort identification
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540531/pdf/amia_2012_symp_1191.pdf
VL  - 2012
ID  - 154
ER  - 


TY  - JOUR
AB  - This software article describes the GATE family of open source text analysis tools and processes. GATE is one of the most widely used systems of its type with yearly download rates of tens of thousands and many active users in both academic and industrial contexts. In this paper we report three examples of GATE-based systems operating in the life sciences and in medicine. First, in genome-wide association studies which have contributed to discovery of a head and neck cancer mutation association. Second, medical records analysis which has significantly increased the statistical power of treatment/outcome models in the UK's largest psychiatric patient cohort. Third, richer constructs in drug-related searching. We also explore the ways in which the GATE family supports the various stages of the lifecycle present in our examples. We conclude that the deployment of text mining for document abstraction or rich search and navigation is best thought of as a process, and that with the right computational tools and data collection strategies this process can be made defined and repeatable. The GATE research programme is now 20 years old and has grown from its roots as a specialist development tool for text processing to become a rather comprehensive ecosystem, bringing together software developers, language engineers and research staff from diverse fields. GATE now has a strong claim to cover a uniquely wide range of the lifecycle of text analysis systems. It forms a focal point for the integration and reuse of advances that have been made by many people (the majority outside of the authors' own group) who work in text processing for biomedicine and other areas. GATE is available online < 1 > under GNU open source licences and runs on all major operating systems. Support is available from an active user and developer community and also on a commercial basis.
AU  - Cunningham, H.
AU  - Tablan, V.
AU  - Roberts, A.
AU  - Bontcheva, K.
DA  - 2013
IS  - 2
KW  - eppi-reviewer4
N1  - 21445632
8562
Cunningham, Hamish Tablan, Valentin Roberts, Angus Bontcheva, Kalina
PY  - 2013
SN  - 1553-7358
ST  - Getting More Out of Biomedical Documents with GATE's Full Lifecycle Open Source Text Analytics
T2  - Plos Computational Biology
TI  - Getting More Out of Biomedical Documents with GATE's Full Lifecycle Open Source Text Analytics
UR  - <Go to ISI>://WOS:000315708600001
VL  - 9
ID  - 132
ER  - 


TY  - JOUR
AB  - PURPOSE OF REVIEW: More data are anticipated from the expected increase in use of electronic health records (EHRs). Upcoming initiatives require reporting of quality measures, meaningful use of clinical decision support, alert systems, and pharmacovigilance - knowledge resulting through use of EHRs. Data mining is a new tool that will help us manage information and derive knowledge from these data, and is a part of evolving new disciplines of informatics and knowledge management. RECENT FINDINGS: Studies are reported from smaller clinic data marts to larger repositories and warehouses in various health systems, biomedical registries, and the medical literature on the Internet. Data mining technologies show promise and challenges. Outcome measures as structured data and narrative text can be mined with human assistance and newer automated natural language processing software. Despite advances, the growing diversity of clinic EHRs lack integration and interoperability with Internet-based biomedical databases. SUMMARY: Allergists have the capability to mine clinic EHRs to discover new information, which may be hidden in charts. A central allergy computer can serve not just as a registry but also allows functionalities to enable EHRs' meaningful use. Harmonization of technological and organizational standards will allow seamless use of new natural language processing (NLP) tools and ontologies through a semantic web.
AU  - Dalan, D.
DA  - 2010
DO  - 10.1097/ACI.0b013e328337bce6.
IS  - 3
KW  - eppi-reviewer4
N1  - 21444065
5831
PY  - 2010
SN  - 1473-6322 (Electronic) 1473-6322 (Linking)
SP  - 171-7
ST  - Clinical data mining and research in the allergy office
T2  - Curr Opin Allergy Clin Immunol
TI  - Clinical data mining and research in the allergy office
UR  - http://ovidsp.tx.ovid.com/ovftpdfs/FPDDNCIBFGEADD00/fs047/ovft/live/gv024/00130832/00130832-201006000-00004.pdf
VL  - 10
ID  - 79
ER  - 


TY  - JOUR
AB  - Radiology departments are a rich source of information in the form of digital radiology reports and images obtained in patients with a wide spectrum of clinical conditions. A free text radiology report and image search application known as Render was created to allow users to find pertinent cases for a variety of purposes. Render is a radiology report and image repository that pools researchable information derived from multiple systems in near real time with use of (a) Health Level 7 links for radiology information system data, (b) periodic file transfers from the picture archiving and communication system, and (c) the results of natural language processing (NLP) analysis. Users can perform more structured and detailed searches with this application by combining different imaging and patient characteristics such as examination number; patient age, gender, and medical record number; and imaging modality. Use of NLP analysis allows a more effective search for reports with positive findings, resulting in the retrieval of more cases and terms having greater relevance. From the retrieved results, users can save images, bookmark examinations, and navigate to an external search engine such as Google. Render has applications in the fields of radiology education, research, and clinical decision support.
AU  - Dang, P. A.
AU  - Kalra, M. K.
AU  - Schultz, T. J.
AU  - Graham, S. A.
AU  - Dreyer, K. J.
DA  - 2009
DO  - 10.1148/rg.295085036. Epub 2009 Jun 29.
IS  - 5
KW  - eppi-reviewer4
N1  - 21446056
6009
PY  - 2009
SN  - 1527-1323 (Electronic) 0271-5333 (Linking)
SP  - 1233-46
ST  - Informatics in radiology: Render: an online searchable radiology study repository
T2  - Radiographics
TI  - Informatics in radiology: Render: an online searchable radiology study repository
VL  - 29
ID  - 185
ER  - 


TY  - JOUR
AB  - The Clinical Outcomes Assessment Toolkit (COAT) was created through a collaboration between the University of California, Los Angeles and Brigham and Women's Hospital to address the challenge of gathering, formatting, and abstracting data for clinical outcomes and performance measurement research. COAT provides a framework for the development of information pipelines to transform clinical data from its original structured, semi-structured, and unstructured forms to a standardized format amenable to statistical analysis. This system includes a collection of clinical data structures, reusable utilities for information analysis and transformation, and a graphical user interface through which pipelines can be controlled and their results audited by nontechnical users. The COAT architecture is presented, as well as two case studies of current implementations in the domain of prostate cancer outcomes assessment.
AU  - D'Avolio, L. W.
AU  - Bui, A. A.
DA  - 2008
DO  - 10.1197/jamia.M2550. Epub 2008 Feb 28.
IS  - 3
KW  - eppi-reviewer4
N1  - 21444069
6323
PY  - 2008
SN  - 1067-5027 (Print) 1067-5027 (Linking)
SP  - 333-40
ST  - The Clinical Outcomes Assessment Toolkit: a framework to support automated clinical records-based outcomes assessment and performance measurement research
T2  - J Am Med Inform Assoc
TI  - The Clinical Outcomes Assessment Toolkit: a framework to support automated clinical records-based outcomes assessment and performance measurement research
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2410004/pdf/333.S1067502708000157.main.pdf
VL  - 15
ID  - 21
ER  - 


TY  - JOUR
AB  - Even though cancer research has traditionally been clinical and biological in nature, in recent years data driven analytic studies have become a common complement. In medical domains where data and analytics driven research is successfully applied, new and novel research directions are identified to further advance the clinical and biological studies. In this research, we used three popular data mining techniques (decision trees, artificial neural networks and support vector machines) along with the most commonly used statistical analysis technique logistic regression to develop prediction models for prostate cancer survivability. The data set contained around 120 000 records and 77 variables. A k-fold cross-validation methodology was used in model building, evaluation and comparison. The results showed that support vector machines are the most accurate predictor (with a test set accuracy of 92.85%) for this domain, followed by artificial neural networks and decision trees.
AU  - Delen, D.
DA  - 2009
IS  - 1
KW  - eppi-reviewer4
N1  - 21443531
9061
Delen, Dursun
PY  - 2009
SN  - 0266-4720
SP  - 100-112
ST  - Analysis of cancer data: a data mining approach
T2  - Expert Systems
TI  - Analysis of cancer data: a data mining approach
UR  - <Go to ISI>://WOS:000262951800010
VL  - 26
ID  - 43
ER  - 


TY  - JOUR
AB  - The combination of improved genomic analysis methods, decreasing genotyping costs, and increasing computing resources has led to an explosion of clinical genomic knowledge in the last decade. Similarly, healthcare systems are increasingly adopting robust electronic health record (EHR) systems that not only can improve health care, but also contain a vast repository of disease and treatment data that could be mined for genomic research. Indeed, institutions are creating EHR-linked DNA biobanks to enable genomic and pharmacogenomic research, using EHR data for phenotypic information. However, EHRs are designed primarily for clinical care, not research, so reuse of clinical EHR data for research purposes can be challenging. Difficulties in use of EHR data include: data availability, missing data, incorrect data, and vast quantities of unstructured narrative text data. Structured information includes billing codes, most laboratory reports, and other variables such as physiologic measurements and demographic information. Significant information, however, remains locked within EHR narrative text documents, including clinical notes and certain categories of test results, such as pathology and radiology reports. For relatively rare observations, combinations of simple free-text searches and billing codes may prove adequate when followed by manual chart review. However, to extract the large cohorts necessary for genome-wide association studies, natural language processing methods to process narrative text data may be needed. Combinations of structured and unstructured textual data can be mined to generate high-validity collections of cases and controls for a given condition. Once high-quality cases and controls are identified, EHR-derived cases can be used for genomic discovery and validation. Since EHR data includes a broad sampling of clinically-relevant phenotypic information, it may enable multiple genomic investigations upon a single set of genotyped individuals. This chapter reviews several examples of phenotype extraction and their application to genetic research, demonstrating a viable future for genomic discovery using EHR-linked data.
AU  - Denny, J. C.
DA  - 2012
DO  - 10.1371/journal.pcbi.1002823. Epub 2012 Dec 27.
IS  - 12
KW  - eppi-reviewer4
N1  - 21443993
4936
PY  - 2012
SN  - 1553-7358 (Electronic) 1553-734X (Linking)
SP  - e1002823
ST  - Chapter 13: Mining electronic health records in the genomics era
T2  - PLoS Comput Biol
TI  - Chapter 13: Mining electronic health records in the genomics era
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531280/pdf/pcbi.1002823.pdf
VL  - 8
ID  - 123
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Ambiguous definitions of quality measures in natural language impede their automated computability and also the reproducibility, validity, timeliness, traceability, comparability, and interpretability of computed results. Therefore, quality measures should be formalized before their release. We have previously developed and successfully applied a method for clinical indicator formalization (CLIF). The objective of our present study is to test whether CLIF is generalizable--that is, applicable to a large set of heterogeneous measures of different types and from various domains. MATERIALS AND METHODS: We formalized the entire set of 159 Dutch quality measures for general practice, which contains structure, process, and outcome measures and covers seven domains. We relied on a web-based tool to facilitate the application of our method. Subsequently, we computed the measures on the basis of a large database of real patient data. RESULTS: Our CLIF method enabled us to fully formalize 100% of the measures. Owing to missing functionality, the accompanying tool could support full formalization of only 86% of the quality measures into Structured Query Language (SQL) queries. The remaining 14% of the measures required manual application of our CLIF method by directly translating the respective criteria into SQL. The results obtained by computing the measures show a strong correlation with results computed independently by two other parties. CONCLUSIONS: The CLIF method covers all quality measures after having been extended by an additional step. Our web tool requires further refinement for CLIF to be applied completely automatically. We therefore conclude that CLIF is sufficiently generalizable to be able to formalize the entire set of Dutch quality measures for general practice.
AU  - Dentler, K.
AU  - Numans, M. E.
AU  - ten, Teije
AU  - Cornet, R.
AU  - de, Keizer
AU  - N, F.
DA  - 2014
DO  - 10.1136/amiajnl-2013-001921. Epub 2013 Nov 5.
IS  - 2
KW  - eppi-reviewer4
N1  - 21445435
4644
PY  - 2014
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 285-91
ST  - Formalization and computation of quality measures based on electronic medical records
T2  - J Am Med Inform Assoc
TI  - Formalization and computation of quality measures based on electronic medical records
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3932459/pdf/amiajnl-2013-001921.pdf
VL  - 21
ID  - 163
ER  - 


TY  - JOUR
AB  - BACKGROUND: The field of clinical research informatics includes creation of clinical data repositories (CDRs) used to conduct quality improvement (QI) activities and comparative effectiveness research (CER). Ideally, CDR data are accurately and directly abstracted from disparate electronic health records (EHRs), across diverse health-systems. OBJECTIVE: Investigators from Washington State's Surgical Care Outcomes and Assessment Program (SCOAP) Comparative Effectiveness Research Translation Network (CERTAIN) are creating such a CDR. This manuscript describes the automation and validation methods used to create this digital infrastructure. METHODS: SCOAP is a QI benchmarking initiative. Data are manually abstracted from EHRs and entered into a data management system. CERTAIN investigators are now deploying Caradigm's Amalga tool to facilitate automated abstraction of data from multiple, disparate EHRs. Concordance is calculated to compare data automatically to manually abstracted. Performance measures are calculated between Amalga and each parent EHR. Validation takes place in repeated loops, with improvements made over time. When automated abstraction reaches the current benchmark for abstraction accuracy - 95% - itwill 'go-live' at each site. PROGRESS TO DATE: A technical analysis was completed at 14 sites. Five sites are contributing; the remaining sites prioritized meeting Meaningful Use criteria. Participating sites are contributing 15-18 unique data feeds, totaling 13 surgical registry use cases. Common feeds are registration, laboratory, transcription/dictation, radiology, and medications. Approximately 50% of 1,320 designated data elements are being automatically abstracted-25% from structured data; 25% from text mining. CONCLUSION: In semi-automating data abstraction and conducting a rigorous validation, CERTAIN investigators will semi-automate data collection to conduct QI and CER, while advancing the Learning Healthcare System.
AU  - Devine, E. B.
AU  - Capurro, D.
AU  - van, Eaton
AU  - Alfonso-Cristancho, R.
AU  - Devlin, A.
AU  - Yanez, N. D.
AU  - Yetisgen-Yildiz, M.
AU  - Flum, D. R.
AU  - Tarczy-Hornoch, P.
DA  - 2013
DO  - 10.13063/2327-9214.1025. eCollection 2013.
IS  - 1
KW  - eppi-reviewer4
N1  - 21447493
4951
PY  - 2013
SN  - 2327-9214 (Electronic) 2327-9214 (Linking)
SP  - 1025
ST  - Preparing Electronic Clinical Data for Quality Improvement and Comparative Effectiveness Research: The SCOAP CERTAIN Automation and Validation Project
T2  - EGEMS (Wash DC)
TI  - Preparing Electronic Clinical Data for Quality Improvement and Comparative Effectiveness Research: The SCOAP CERTAIN Automation and Validation Project
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4371452/pdf/egems1025.pdf
VL  - 1
ID  - 197
ER  - 


TY  - RPRT
AB  - Building on the momentum generated by the spectacular success of the Joint Conference on Lexical and Computational Semantics (*SEM) in 2012, bringing together the ACL SIGLEX and ACL SIGSEM communities, we are delighted to bring to you the second edition of the conference, as a top-tier showcase of the latest research in computational semantics. We accepted 14 papers (11 long and 3 short) for publication at the conference, out of a possible 45 submissions (a 31% acceptance rate). This is on par with some of the most competitive conferences in computational linguistics, and we are confident will set the stage for a scintillating conference. This year, we started a tradition that we intend to maintain in all future iterations of the conference in integrating a shared task into the conference. The shared task was selected by an independent committee comprising members from SIGLEX and SIGSEM, based on an open call for proposals, and revolved around Semantic Textual Similarity (STS). The task turned out to be a huge success with 34 teams participating, submitting a total of 103 system runs.
AU  - Diab, Mona
AU  - Baldwin, Timothy
AU  - Baroni, Marco
DA  - 2013
KW  - eppi-reviewer4
N1  - 21508906
11346
PB  - ASSOCIATION FOR COMPUTATIONAL LINGUISTICS STROUDSBURG PA
PY  - 2013
RP  - 21508906
11346
SN  - ADA586391, XD, DARPA
SP  - 367
ST  - Joint Conference on Lexical and Computational Semantics (2nd) (SEM 2013) Held in Atlanta, Georgia on June 13-14, 2013. Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity
TI  - Joint Conference on Lexical and Computational Semantics (2nd) (SEM 2013) Held in Atlanta, Georgia on June 13-14, 2013. Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity
ID  - 2
ER  - 


TY  - JOUR
AB  - In contemporary electronic medical records much of the clinically important data-signs and symptoms, symptom severity, disease status, etc.-are not provided in structured data fields but rather are encoded in clinician-generated narrative text. Natural language processing (NLP) provides a means of unlocking this important data source for applications in clinical decision support, quality assurance, and public health. This chapter provides an overview of representative NLP systems in biomedicine based on a unified architectural view. A general architecture in an NLP system consists of two main components: background knowledge that includes biomedical knowledge resources and a framework that integrates NLP tools to process text. Systems differ in both components, which we review briefly. Additionally, the challenge facing current research efforts in biomedical NLP includes the paucity of large, publicly available annotated corpora, although initiatives that facilitate data sharing, system evaluation, and collaborative work between researchers in clinical NLP are starting to emerge.
AU  - Doan, S.
AU  - Conway, M.
AU  - Phuong, T. M.
AU  - Ohno-Machado, L.
DA  - 2014
DO  - 10.1007/978-1-4939-0847-9_16.
KW  - eppi-reviewer4
N1  - 21446939
4442
PY  - 2014
SN  - 1940-6029 (Electronic) 1064-3745 (Linking)
SP  - 275-94
ST  - Natural language processing in biomedicine: a unified system architecture overview
T2  - Methods Mol Biol
TI  - Natural language processing in biomedicine: a unified system architecture overview
VL  - 1168
ID  - 68
ER  - 


TY  - CHAP
A2  - Bremer, E. G.
A2  - Hakenberg, J.
A2  - Han, E. H. S.
A2  - Berrar, D.
A2  - Dubitzky, W.
AB  - As genomic research advances, the knowledge discovery from a large collection of scientific papers becomes more important for efficient biological and biomedical research. Even though current databases continue to update new protein-protein interactions, valuable information still remains in biomedical literature. Thus data mining techniques are required to extract the information. In this paper, we present a tree kernel-based method to mine protein-protein interactions from biomedical literature. The tree kernel is designed to consider grammatical structures for given sentences. A support vector machine classifier is combined with the tree kernel and trained on predefined interaction corpus and set of interaction patterns. Experimental results show that the proposed method gives promising results by utilizing the structure patterns.
AU  - Eom, J. H.
AU  - Kim, S.
AU  - Kim, S. H.
AU  - Zhang, B. T.
KW  - eppi-reviewer4
N1  - 21448529
10273
Eom, JH Kim, S Kim, SH Zhang, BT International Workshop on Knowledge Disvovery in Life Science Literature APR 09, 2006 Sinaia, SINGAPORE SPSS Inc, DataMiningGrid Consortium, Univ Minnesota, iXmatch Inc, Childrens Memorial Hosp, NW Univ zu Berlin, Univ Ulster
PY  - 2006
SN  - 0302-9743 3-540-32809-2
SP  - 42-52
ST  - A tree kernel-based method for protein-protein interaction mining from biomedical literature
T2  - Knowledge Discovery in Life Science Literature, Proceedings
TI  - A tree kernel-based method for protein-protein interaction mining from biomedical literature
UR  - <Go to ISI>://WOS:000237198800004
VL  - 3886
ID  - 186
ER  - 


TY  - JOUR
AB  - BACKGROUND: Data collected for medical, filing and administrative purposes in electronic patient records (EPRs) represent a rich source of individualised clinical data, which has great potential for improved detection of patients experiencing adverse drug reactions (ADRs), across all approved drugs and across all indication areas. OBJECTIVES: The aim of this study was to take advantage of techniques for temporal data mining of EPRs in order to detect ADRs in a patient- and dose-specific manner. METHODS: We used a psychiatric hospital's EPR system to investigate undesired drug effects. Within one workflow the method identified patient-specific adverse events (AEs) and links these to specific drugs and dosages in a temporal manner, based on integration of text mining results and structured data. The structured data contained precise information on drug identity, dosage and strength. RESULTS: When applying the method to the 3,394 patients in the cohort, we identified AEs linked with a drug in 2,402 patients (70.8 %). Of the 43,528 patient-specific drug substances prescribed, 14,736 (33.9 %) were linked with AEs. From these links we identified multiple ADRs (p < 0.05) and found them to occur at similar frequencies, as stated by the manufacturer and in the literature. We showed that drugs displaying similar ADR profiles share targets, and we compared submitted spontaneous AE reports with our findings. For nine of the ten most prescribed antipsychotics in the patient population, larger doses were prescribed to sedated patients than non-sedated patients; five antipsychotics [corrected] exhibited a significant difference (p<0.05). Finally, we present two cases (p < 0.05) identified by the workflow. The method identified the potentially fatal AE QT prolongation caused by methadone, and a non-described likely ADR between levomepromazine and nightmares found among the hundreds of identified novel links between drugs and AEs (p < 0.05). CONCLUSIONS: The developed method can be used to extract dose-dependent ADR information from already collected EPR data. Large-scale AE extraction from EPRs may complement or even replace current drug safety monitoring methods in the future, reducing or eliminating manual reporting and enabling much faster ADR detection.
AU  - Eriksson, R.
AU  - Werge, T.
AU  - Jensen, L. J.
AU  - Brunak, S.
DA  - 2014
DO  - 10.1007/s40264-014-0145-z.
IS  - 4
KW  - eppi-reviewer4
N1  - 21444872
4500
PY  - 2014
SN  - 0114-5916 (Print) 0114-5916 (Linking)
SP  - 237-47
ST  - Dose-specific adverse drug reaction identification in electronic patient records: temporal data mining in an inpatient psychiatric population
T2  - Drug Saf
TI  - Dose-specific adverse drug reaction identification in electronic patient records: temporal data mining in an inpatient psychiatric population
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3975083/pdf/40264_2014_Article_145.pdf
VL  - 37
ID  - 189
ER  - 


TY  - JOUR
AB  - Post-marketing pharmacovigilance is important for public health, as many Adverse Drug Events (ADEs) are unknown when those drugs were approved for marketing. However, due to the large number of reported drugs and drug combinations, detecting ADE signals by mining these reports is becoming a challenging task in terms of computational complexity. Recently, a parallel programming model, MapReduce has been introduced by Google to support large-scale data intensive applications. In this study, we proposed a MapReduce-based algorithm, for common ADE detection approach, Proportional Reporting Ratio (PRR), and tested it in mining spontaneous ADE reports from FDA. The purpose is to investigate the possibility of using MapReduce principle to speed up biomedical data mining tasks using this pharmacovigilance case as one specific example. The results demonstrated that MapReduce programming model could improve the performance of common signal detection algorithm for pharmacovigilance in a distributed computation environment at approximately liner speedup rates.
AU  - Fan, K.
AU  - Sun, X.
AU  - Tao, Y.
AU  - Xu, L.
AU  - Wang, C.
AU  - Mao, X.
AU  - Peng, B.
AU  - Pan, Y.
DA  - 2010
KW  - eppi-reviewer4
N1  - 21445761
5557
Fan, Kai Sun, Xingzhi Tao, Ying Xu, Linhao Wang, Chen Mao, Xianling Peng, Bo Pan, Yue United States AMIA Annu Symp Proc. 2010 Nov 13;2010:902-6.
PY  - 2010
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 902-6
ST  - High-Performance Signal Detection for Adverse Drug Events using MapReduce Paradigm
T2  - AMIA Annu Symp Proc
TI  - High-Performance Signal Detection for Adverse Drug Events using MapReduce Paradigm
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041463/pdf/amia-2010_sympproc_0902.pdf
VL  - 2010
ID  - 22
ER  - 


TY  - RPRT
AB  - The promise of the benefits of fully integrated electronic health care systems can only be realized if the quality of emerging large medical databases can be characterized and the meaning of the data understood. For this purpose, the effective visualization of large and complex health data for timely decision making is critical. Our long-term goal is to improve the usability of emerging large scale clinical data sets by developing effective and efficient open-source systems for health data analytics and visualization tools for clinicians, healthcare professionals, administrators, and patients. The objective of this application is to develop a prototype system to test the effectiveness of this approach on a large scale health care database that is currently available at Regenstrief Institute. We have reached this objective with the following specific accomplishments: Built a relational database as the representation of a health concept space, extracted from the NCD dataset, Natural Language Processing techniques were carried out to process 325791 clinical notes to extract new terms including diseases, symptoms, and mental and risky behaviors, Data mining techniques were applied to extract associations between terms in the concept space, and to discover new cluster terms, Designed and implemented a suite of novel visualization algorithms that allows the users to interactively explore the data based on the user selected terms and filters, Designed and implemented a web based graphical user interface for the prototype system, and Designed and tested an evaluation procedure for health data visualization system. This visualization framework offers a real time and web-based solution for the effective use of large scale military electronic health record systems by allowing system level integration of the human' visual capabilities into the overall health data based decision making system.
AU  - Fang, Shiaofen
AU  - Palakal, Mathew
AU  - Xia, Yuni
AU  - Grannis Shaun, J.
AU  - Williams Jennifer, L.
DA  - 2014
KW  - eppi-reviewer4
N1  - 21508729
11374
PB  - INDIANA UNIV INDIANAPOLIS
PY  - 2014
RP  - 21508729
11374
SN  - ADA624344, XA, USAMRMC, W81XWH-13-1-0020
SP  - 79
ST  - Health-Terrain: Visualizing Large Scale Health Data
TI  - Health-Terrain: Visualizing Large Scale Health Data
ID  - 95
ER  - 


TY  - JOUR
AB  - BACKGROUND: Patient histories in electronic health records currently exist mainly in free text format thereby limiting the possibility that decision support technology may contribute to the accuracy and timeliness of clinical diagnoses. Structuring and/or coding make patient histories potentially computable. METHODS: A systematic review was undertaken of the benefits and risks of structuring and/or coding patient history by searching nine international databases for published and unpublished studies over the period 1990-2010. The focus was on the current patient history, defined as information reported by a patient or the patient's caregiver about the patient's present health situation and health status. Findings were synthesised through a theoretically based textural analysis. FINDINGS: Of the 9207 potentially eligible papers identified, 10 studies satisfied the eligibility criteria. There was evidence of a modest number of benefits associated with structuring the current patient history, including obtaining more complete clinical histories, improved accuracy of patient self-documented histories, and better associated decision-making by professionals. However, no studies demonstrated any resulting improvements in patient care or outcomes. When more detailed records were obtained through the use of a structured format no attempt was made to confirm if this additional information was clinically useful. No studies investigated possible risks associated with structuring the patient history. No studies examined coding of the patient history. CONCLUSIONS: There is an insufficient evidence base for sound policy making on the benefits and risks of structuring and/or coding patient history. The authors suggest this field of enquiry warrants further investigation given the interest in use of decision support technology to aid diagnoses.
AU  - Fernando, B.
AU  - Kalra, D.
AU  - Morrison, Z.
AU  - Byrne, E.
AU  - Sheikh, A.
DA  - 2012
DO  - 10.1136/bmjqs-2011-000450. Epub 2012 Feb 10.
IS  - 4
KW  - eppi-reviewer4
N1  - 21443816
5256
PY  - 2012
SN  - 2044-5423 (Electronic) 2044-5415 (Linking)
SP  - 337-46
ST  - Benefits and risks of structuring and/or coding the presenting patient history in the electronic health record: systematic review
T2  - BMJ Qual Saf
TI  - Benefits and risks of structuring and/or coding the presenting patient history in the electronic health record: systematic review
UR  - http://qualitysafety.bmj.com/content/21/4/337.long
VL  - 21
ID  - 90
ER  - 


TY  - JOUR
AB  - The use of text mining and supervised machine learning algorithms on biomedical databases has become increasingly common. However, a question remains: How much data must be annotated to create a suitable training set for a machine learning classifier? In prior research with active learning in medical text classification, we found evidence that not only sample size but also some of the intrinsic characteristics of the texts being analyzed-such as the size of the vocabulary and the length of a document-may also influence the resulting classifier's performance. This study is an attempt to create a regression model to predict performance based on sample size and other text features. While the model needs to be trained on existing datasets, we believe it is feasible to predict performance without obtaining annotations from new datasets once the model is built.
AU  - Figueroa, R. L.
AU  - Zeng-Treitler, Q.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21448378
4738
Figueroa, Rosa L Zeng-Treitler, Qing Netherlands Stud Health Technol Inform. 2013;192:1193.
PY  - 2013
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 1193
ST  - Text classification performance: is the sample size the only factor to be considered?
T2  - Stud Health Technol Inform
TI  - Text classification performance: is the sample size the only factor to be considered?
VL  - 192
ID  - 80
ER  - 


TY  - JOUR
AB  - Statistical text mining and natural language processing have been shown to be effective for extracting useful information from medical documents. However, neither technique is effective at extracting the information stored in semi-structure text elements. A prototype system (TagLine) was developed to extract information from the semi-structured text using machine learning and a rule based annotator. Features for the learning machine were suggested by prior work, and by examining text, and selecting attributes that help distinguish classes of text lines. Classes were derived empirically from text and guided by an ontology developed by the VHA's Consortium for Health Informatics Research (CHIR). Decision trees were evaluated for class predictions on 15,103 lines of text achieved an overall accuracy of 98.5 percent. The class labels applied to the lines were then used for annotating semi-structured text elements. TagLine achieved F-measure over 0.9 for each of the structures, which included tables, slots and fillers.
AU  - Finch, D. K.
AU  - McCart, J. A.
AU  - Luther, S. L.
DA  - 2014
KW  - eppi-reviewer4
N1  - 21448342
4601
Finch, Dezon K McCart, James A Luther, Stephen L Research Support, U.S. Gov't, Non-P.H.S. United States AMIA Annu Symp Proc. 2014 Nov 14;2014:534-43. eCollection 2014.
PY  - 2014
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 534-43
ST  - TagLine: Information Extraction for Semi-Structured Text in Medical Progress Notes
T2  - AMIA Annu Symp Proc
TI  - TagLine: Information Extraction for Semi-Structured Text in Medical Progress Notes
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4419992/pdf/1985993.pdf
VL  - 2014
ID  - 190
ER  - 


TY  - JOUR
AB  - BACKGROUND: The aim of this study was to build electronic algorithms using a combination of structured data and natural language processing (NLP) of text notes for potential safety surveillance of 9 postoperative complications. METHODS: Postoperative complications from 6 medical centers in the Southeastern United States were obtained from the Veterans Affairs Surgical Quality Improvement Program (VASQIP) registry. Development and test datasets were constructed using stratification by facility and date of procedure for patients with and without complications. Algorithms were developed from VASQIP outcome definitions using NLP-coded concepts, regular expressions, and structured data. The VASQIP nurse reviewer served as the reference standard for evaluating sensitivity and specificity. The algorithms were designed in the development and evaluated in the test dataset. RESULTS: Sensitivity and specificity in the test set were 85% and 92% for acute renal failure, 80% and 93% for sepsis, 56% and 94% for deep vein thrombosis, 80% and 97% for pulmonary embolism, 88% and 89% for acute myocardial infarction, 88% and 92% for cardiac arrest, 80% and 90% for pneumonia, 95% and 80% for urinary tract infection, and 77% and 63% for wound infection, respectively. A third of the complications occurred outside of the hospital setting. CONCLUSIONS: Computer algorithms on data extracted from the electronic health record produced respectable sensitivity and specificity across a large sample of patients seen in 6 different medical centers. This study demonstrates the utility of combining NLP with structured data for mining the information contained within the electronic health record.
AU  - FitzHenry, F.
AU  - Murff, H. J.
AU  - Matheny, M. E.
AU  - Gentry, N.
AU  - Fielstein, E. M.
AU  - Brown, S. H.
AU  - Reeves, R. M.
AU  - Aronsky, D.
AU  - Elkin, P. L.
AU  - Messina, V. P.
AU  - Speroff, T.
DA  - 2013
DO  - 10.1097/MLR.0b013e31828d1210.
IS  - 6
KW  - eppi-reviewer4
N1  - 21445270
4826
PY  - 2013
SN  - 1537-1948 (Electronic) 0025-7079 (Linking)
SP  - 509-16
ST  - Exploring the frontier of electronic health record surveillance: the case of postoperative complications
T2  - Med Care
TI  - Exploring the frontier of electronic health record surveillance: the case of postoperative complications
UR  - http://ovidsp.tx.ovid.com/ovftpdfs/FPDDNCIBFGEADD00/fs046/ovft/live/gv023/00005650/00005650-201306000-00006.pdf
VL  - 51
ID  - 191
ER  - 


TY  - JOUR
AB  - MOTIVATION: Natural language processing (NLP) techniques are increasingly being used in biology to automate the capture of new biological discoveries in text, which are being reported at a rapid rate. Yet, information represented in NLP data structures is classically very different from information organized with ontologies as found in model organisms or genetic databases. To facilitate the computational reuse and integration of information buried in unstructured text with that of genetic databases, we propose and evaluate a translational schema that represents a comprehensive set of phenotypic and genetic entities, as well as their closely related biomedical entities and relations as expressed in natural language. In addition, the schema connects different scales of biological information, and provides mappings from the textual information to existing ontologies, which are essential in biology for integration, organization, dissemination and knowledge management of heterogeneous phenotypic information. A common comprehensive representation for otherwise heterogeneous phenotypic and genetic datasets, such as the one proposed, is critical for advancing systems biology because it enables acquisition and reuse of unprecedented volumes of diverse types of knowledge and information from text. RESULTS: A novel representational schema, PGschema, was developed that enables translation of phenotypic, genetic and their closely related information found in textual narratives to a well-defined data structure comprising phenotypic and genetic concepts from established ontologies along with modifiers and relationships. Evaluation for coverage of a selected set of entities showed that 90% of the information could be represented (95% confidence interval: 86-93%; n = 268). Moreover, PGschema can be expressed automatically in an XML format using natural language techniques to process the text. To our knowledge, we are providing the first evaluation of a translational schema for NLP that contains declarative knowledge about genes and their associated biomedical data (e.g. phenotypes). AVAILABILITY: http://zellig.cpmc.columbia.edu/PGschema
AU  - Friedman, C.
AU  - Borlawsky, T.
AU  - Shagina, L.
AU  - Xing, H. R.
AU  - Lussier, Y. A.
DA  - 2006
IS  - 19
KW  - eppi-reviewer4
N1  - 21443874
6658
Friedman, Carol Borlawsky, Tara Shagina, Lyudmila Xing, H Rosie Lussier, Yves A 1K22 LM008308-01/LM/NLM NIH HHS/United States 1U54CA121852-01A1/CA/NCI NIH HHS/United States K22 LM008308/LM/NLM NIH HHS/United States K22 LM008308-03/LM/NLM NIH HHS/United States R01 LM007659/LM/NLM NIH HHS/United States R01 LM007659-01/LM/NLM NIH HHS/United States R01 LM008635/LM/NLM NIH HHS/United States R01 LM008635-01/LM/NLM NIH HHS/United States R01 LM07659/LM/NLM NIH HHS/United States R01 LM08635/LM/NLM NIH HHS/United States Research Support, N.I.H., Extramural England Bioinformatics. 2006 Oct 1;22(19):2421-9. Epub 2006 Jul 26.
PY  - 2006
SN  - 1367-4811 (Electronic) 1367-4803 (Linking)
SP  - 2421-9
ST  - Bio-Ontology and text: bridging the modeling gap
T2  - Bioinformatics
TI  - Bio-Ontology and text: bridging the modeling gap
UR  - http://bioinformatics.oxfordjournals.org/content/22/19/2421.full.pdf
VL  - 22
ID  - 54
ER  - 


TY  - JOUR
AB  - PURPOSE: It is useful to convert free-text diagnostic reports into structured diagnostic reports by semantic analysis for the secondary investigation of their contents. In this study, we propose a system in which description units are automatically extracted to create structured text reports and we evaluated its usefulness. METHODS: We defined the rules to create description units and developed the system that can automatically extract these description units from free-text diagnostic reports. We applied this system to reports of cerebral perfusion scintigrams and obtained 5 dictionaries of description units, increasing the number of scintigrams from 100 to 500 in increments of 100. Each dictionary was used to analyze another 100 scintigrams. The results obtained using each dictionary were compared with the results of physicians' interpretation. RESULTS: The recall rate of this system to the physicians' interpretation increased when correlated with the number of scintigrams but with 300 cases was almost saturated at 85%. CONCLUSION: We propose a semantic analysis system and show its usefulness in the semantic evaluation of the reports of cerebral perfusion scintigrams.
AU  - Fujii, H.
AU  - Yamagishi, H.
AU  - Ando, Y.
AU  - Tsukamoto, N.
AU  - Kawaguchi, O.
AU  - Kasamatsu, T.
AU  - Kurosaki, K.
AU  - Osada, M.
AU  - Kaneko, H.
AU  - Kubo, A.
DA  - 2007
IS  - Pt 1
KW  - eppi-reviewer4
N1  - 21448232
6431
Fujii, Hirofumi Yamagishi, Hiromasa Ando, Yutaka Tsukamoto, Nobuhiro Kawaguchi, Osamu Kasamatsu, Tomotaka Kurosaki, Kaoru Osada, Masakazu Kaneko, Hiroshi Kubo, Atsushi Evaluation Studies Netherlands Stud Health Technol Inform. 2007;129(Pt 1):669-73.
PY  - 2007
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 669-73
ST  - Structuring of free-text diagnostic report
T2  - Stud Health Technol Inform
TI  - Structuring of free-text diagnostic report
VL  - 129
ID  - 69
ER  - 


TY  - JOUR
AB  - OBJECTIVES: Bringing together structured and text-based sources is an exciting challenge for biomedical informaticians, since most relevant biomedical sources belong to one of these categories. In this paper we evaluate the feasibility of integrating relational and text-based biomedical sources using: i) an original logical schema acquisition method for textual databases developed by the authors, and ii) OntoFusion, a system originally designed by the authors for the integration of relational sources. METHODS: We conducted an integration experiment involving a test set of seven differently structured sources covering the domain of genetic diseases. We used our logical schema acquisition method to generate schemas for all textual sources. The sources were integrated using the methods and tools provided by OntoFusion. The integration was validated using a test set of 500 queries. RESULTS: A panel of experts answered a questionnaire to evaluate i) the quality of the extracted schemas, ii) the query processing performance of the integrated set of sources, and iii) the relevance of the retrieved results. The results of the survey show that our method extracts coherent and representative logical schemas. Experts' feedback on the performance of the integrated system and the relevance of the retrieved results was also positive. Regarding the validation of the integration, the system successfully provided correct results for all queries in the test set. CONCLUSIONS: The results of the experiment suggest that text-based sources including a logical schema can be regarded as equivalent to structured databases. Using our method, previous research and existing tools designed for the integration of structured databases can be reused - possibly subject to minor modifications - to integrate differently structured sources.
AU  - Garcia-Remesal, M.
AU  - Maojo, V.
AU  - Billhardt, H.
AU  - Crespo, J.
DA  - 2010
DO  - 10.3414/ME0614. Epub 2009 Nov 20.
IS  - 4
KW  - eppi-reviewer4
N1  - 21446133
5910
PY  - 2010
SN  - 0026-1270 (Print) 0026-1270 (Linking)
SP  - 337-48
ST  - Integration of relational and textual biomedical sources. A pilot experiment using a semi-automated method for logical schema acquisition
T2  - Methods Inf Med
TI  - Integration of relational and textual biomedical sources. A pilot experiment using a semi-automated method for logical schema acquisition
UR  - http://methods.schattauer.de/contents/archivestandard/issue/1102/manuscript/12379/download.html
VL  - 49
ID  - 91
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To determine whether quality measures based on computer-extracted EHR data can reproduce findings based on data manually extracted by reviewers. DATA SOURCES: We studied 12 measures of care indicated for adolescent well-care visits for 597 patients in three pediatric health systems. STUDY DESIGN: Observational study. DATA COLLECTION/EXTRACTION METHODS: Manual reviewers collected quality data from the EHR. Site personnel programmed their EHR systems to extract the same data from structured fields in the EHR according to national health IT standards. PRINCIPAL FINDINGS: Overall performance measured via computer-extracted data was 21.9 percent, compared with 53.2 percent for manual data. Agreement measures were high for immunizations. Otherwise, agreement between computer extraction and manual review was modest (Kappa = 0.36) because computer-extracted data frequently missed care events (sensitivity = 39.5 percent). Measure validity varied by health care domain and setting. A limitation of our findings is that we studied only three domains and three sites. CONCLUSIONS: The accuracy of computer-extracted EHR quality reporting depends on the use of structured data fields, with the highest agreement found for measures and in the setting that had the greatest concentration of structured fields. We need to improve documentation of care, data extraction, and adaptation of EHR systems to practice workflow.
AU  - Gardner, W.
AU  - Morton, S.
AU  - Byron, S. C.
AU  - Tinoco, A.
AU  - Canan, B. D.
AU  - Leonhart, K.
AU  - Kong, V.
AU  - Scholle, S. H.
DA  - 2014
DO  - 10.1111/1475-6773.12159. Epub 2014 Jan 29.
IS  - 4
KW  - eppi-reviewer4
N1  - 21448661
4558
PY  - 2014
SN  - 1475-6773 (Electronic) 0017-9124 (Linking)
SP  - 1226-48
ST  - Using computer-extracted data from electronic health records to measure the quality of adolescent well-care
T2  - Health Serv Res
TI  - Using computer-extracted data from electronic health records to measure the quality of adolescent well-care
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4239847/pdf/hesr0049-1226.pdf
VL  - 49
ID  - 81
ER  - 


TY  - JOUR
AB  - BACKGROUND: The identification of patients who pose an epidemic hazard when they are admitted to a health facility plays a role in preventing the risk of hospital acquired infection. An automated clinical decision support system to detect suspected cases, based on the principle of syndromic surveillance, is being developed at the University of Lyon's Hopital de la Croix-Rousse. This tool will analyse structured data and narrative reports from computerized emergency department (ED) medical records. The first step consists of developing an application (UrgIndex) which automatically extracts and encodes information found in narrative reports. The purpose of the present article is to describe and evaluate this natural language processing system. METHODS: Narrative reports have to be pre-processed before utilizing the French-language medical multi-terminology indexer (ECMT) for standardized encoding. UrgIndex identifies and excludes syntagmas containing a negation and replaces non-standard terms (abbreviations, acronyms, spelling errors...). Then, the phrases are sent to the ECMT through an Internet connection. The indexer's reply, based on Extensible Markup Language, returns codes and literals corresponding to the concepts found in phrases. UrgIndex filters codes corresponding to suspected infections. Recall is defined as the number of relevant processed medical concepts divided by the number of concepts evaluated (coded manually by the medical epidemiologist). Precision is defined as the number of relevant processed concepts divided by the number of concepts proposed by UrgIndex. Recall and precision were assessed for respiratory and cutaneous syndromes. RESULTS: Evaluation of 1,674 processed medical concepts contained in 100 ED medical records (50 for respiratory syndromes and 50 for cutaneous syndromes) showed an overall recall of 85.8% (95% CI: 84.1-87.3). Recall varied from 84.5% for respiratory syndromes to 87.0% for cutaneous syndromes. The most frequent cause of lack of processing was non-recognition of the term by UrgIndex (9.7%). Overall precision was 79.1% (95% CI: 77.3-80.8). It varied from 81.4% for respiratory syndromes to 77.0% for cutaneous syndromes. CONCLUSIONS: This study demonstrates the feasibility of and interest in developing an automated method for extracting and encoding medical concepts from ED narrative reports, the first step required for the detection of potentially infectious patients at epidemic risk.
AU  - Gerbier, S.
AU  - Yarovaya, O.
AU  - Gicquel, Q.
AU  - Millet, A. L.
AU  - Smaldore, V.
AU  - Pagliaroli, V.
AU  - Darmoni, S.
AU  - Metzger, M. H.
DA  - 2011
DO  - 10.1186/1472-6947-11-50.
KW  - eppi-reviewer4
N1  - 21445169
5427
PY  - 2011
SN  - 1472-6947 (Electronic) 1472-6947 (Linking)
SP  - 50
ST  - Evaluation of natural language processing from emergency department computerized medical records for intra-hospital syndromic surveillance
T2  - BMC Med Inform Decis Mak
TI  - Evaluation of natural language processing from emergency department computerized medical records for intra-hospital syndromic surveillance
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3158541/pdf/1472-6947-11-50.pdf
VL  - 11
ID  - 96
ER  - 


TY  - JOUR
AB  - We present a method that extracts medication information from discharge summaries. The program relies on parsing rules written as a set of regular expressions and on a user-configurable drug lexicon. Our evaluation shows a precision of 94% and recall of 83% in the extraction of medication information. We use a broader definition of medication information than previous studies, including drug names appearing with and without dosage information, misspelled drug names, and contextual information.
AU  - Gold, S.
AU  - Elhadad, N.
AU  - Zhu, X.
AU  - Cimino, J. J.
AU  - Hripcsak, G.
DA  - 2008
KW  - eppi-reviewer4
N1  - 21445307
6169
Gold, Sigfried Elhadad, Noemie Zhu, Xinxin Cimino, James J Hripcsak, George R01 LM006910/LM/NLM NIH HHS/United States R01 LM007659/LM/NLM NIH HHS/United States R01 LM008635/LM/NLM NIH HHS/United States Intramural NIH HHS/United States Research Support, N.I.H., Extramural Research Support, N.I.H., Intramural United States AMIA Annu Symp Proc. 2008 Nov 6:237-41.
PY  - 2008
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 237-41
ST  - Extracting structured medication event information from discharge summaries
T2  - AMIA Annu Symp Proc
TI  - Extracting structured medication event information from discharge summaries
ID  - 144
ER  - 


TY  - JOUR
AB  - The identification of high-risk patients is a critical component in improving patient outcomes and managing costs. This paper describes the Intelligent Care Delivery Analytics platform (ICDA), a system which enables risk assessment analytics that process large collections of dynamic electronic medical data to identify at-risk patients. ICDA works by ingesting large volumes of data into a common data model, then orchestrating a collection of analytics that identify at-risk patients. It also provides an interactive environment through which users can access and review the analytics results. In addition, ICDA provides APIs via which analytics results can be retrieved to surface in external applications. A detailed review of ICDA's architecture is provided. Descriptions of four use cases are included to illustrate ICDA's application within two different data environments. These use cases showcase the system's flexibility and exemplify the types of analytics it enables.
AU  - Gotz, D.
AU  - Stavropoulos, H.
AU  - Sun, J.
AU  - Wang, F.
DA  - 2012
KW  - eppi-reviewer4
N1  - 21445828
4930
Gotz, David Stavropoulos, Harry Sun, Jimeng Wang, Fei United States AMIA Annu Symp Proc. 2012;2012:264-73. Epub 2012 Nov 3.
PY  - 2012
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 264-73
ST  - ICDA: a platform for Intelligent Care Delivery Analytics
T2  - AMIA Annu Symp Proc
TI  - ICDA: a platform for Intelligent Care Delivery Analytics
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540495/pdf/amia_2012_symp_0264.pdf
VL  - 2012
ID  - 55
ER  - 


TY  - JOUR
AB  - OBJECTIVES: To develop an electronic registry of patients with chronic kidney disease (CKD) treated in a nephrology practice in order to provide clinically meaningful measurement and population management to improve rates of blood pressure (BP) control. METHODS: We combined data from multiple electronic sources: the billing system, structured fields in the electronic health record (EHR), and free text physician notes using natural language processing (NLP). We also used point-of-care worksheets to capture clinical rationale. RESULTS: Nephrologist billing accurately identified patients with CKD. Using an algorithm that incorporated multiple BP readings increased the measured rate of control (130/80 mm Hg) from 37.1% to 42.3%. With the addition of NLP to capture BP readings from free text notes, the rate was 52.6%. Data from point-of-care worksheets indicated that in 52% of visits in which patients were identified as not having controlled BP, patients were actually at goal based on BP readings taken at home or on that day in the office. CONCLUSIONS: Building a method for clinically meaningful continuous performance measurement of BP control is possible, but will require data from multiple sources. Electronic measurement systems need to grow to be able to capture and process performance data from patients as well as in real-time from physicians.
AU  - Greenberg, J. O.
AU  - Vakharia, N.
AU  - Szent-Gyorgyi, L. E.
AU  - Desai, S. P.
AU  - Turchin, A.
AU  - Forman, J.
AU  - Bonventre, J. V.
AU  - Kachalia, A.
DA  - 2013
DO  - 10.1136/amiajnl-2012-001308. Epub 2013 Jan 23.
IS  - e1
KW  - eppi-reviewer4
N1  - 21446519
4917
PY  - 2013
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - e97-e101
ST  - Meaningful measurement: developing a measurement system to improve blood pressure control in patients with chronic kidney disease
T2  - J Am Med Inform Assoc
TI  - Meaningful measurement: developing a measurement system to improve blood pressure control in patients with chronic kidney disease
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3715343/pdf/amiajnl-2012-001308.pdf
VL  - 20
ID  - 97
ER  - 


TY  - JOUR
AB  - The Human Phenotype Ontology (HPO) is widely used in the rare disease community for differential diagnostics, phenotype-driven analysis of next-generation sequence-variation data, and translational research, but a comparable resource has not been available for common disease. Here, we have developed a concept-recognition procedure that analyzes the frequencies of HPO disease annotations as identified in over five million PubMed abstracts by employing an iterative procedure to optimize precision and recall of the identified terms. We derived disease models for 3,145 common human diseases comprising a total of 132,006 HPO annotations. The HPO now comprises over 250,000 phenotypic annotations for over 10,000 rare and common diseases and can be used for examining the phenotypic overlap among common diseases that share risk alleles, as well as between Mendelian diseases and common diseases linked by genomic location. The annotations, as well as the HPO itself, are freely available.
AU  - Groza, T.
AU  - Kohler, S.
AU  - Moldenhauer, D.
AU  - Vasilevsky, N.
AU  - Baynam, G.
AU  - Zemojtel, T.
AU  - Schriml, L. M.
AU  - Kibbe, W. A.
AU  - Schofield, P. N.
AU  - Beck, T.
AU  - Vasant, D.
AU  - Brookes, A. J.
AU  - Zankl, A.
AU  - Washington, N. L.
AU  - Mungall, C. J.
AU  - Lewis, S. E.
AU  - Haendel, M. A.
AU  - Parkinson, H.
AU  - Robinson, P. N.
DA  - 2015
DO  - 10.1016/j.ajhg.2015.05.020. Epub 2015 Jun 25.
IS  - 1
KW  - eppi-reviewer4
N1  - 21445804
3981
PY  - 2015
SN  - 1537-6605 (Electronic) 0002-9297 (Linking)
SP  - 111-24
ST  - The Human Phenotype Ontology: Semantic Unification of Common and Rare Disease
T2  - Am J Hum Genet
TI  - The Human Phenotype Ontology: Semantic Unification of Common and Rare Disease
UR  - http://ac.els-cdn.com/S0002929715002347/1-s2.0-S0002929715002347-main.pdf?_tid=ec3d9584-5416-11e6-aef6-00000aacb362&acdnat=1469636991_22a1f83ac586376da9f8ea51c6ae39a4
VL  - 97
ID  - 10
ER  - 


TY  - JOUR
AB  - Objectives: To develop an adaptive approach to mine frequent semantic tags (FSTs) from heterogeneous clinical research texts. Methods: We develop a "plug-n-play" framework that integrates replaceable unsupervised kernel algorithms with formatting, functional, and utility wrappers for FST mining. Temporal information identification and semantic equivalence detection were; two example functional wrappers. We first compared this approach's recall and efficiency for mining FSTs from ClinicalTrials.gov to that of a recently published tag-mining algorithm. Then we assessed this approach's adaptability to two other types of clinical research texts: clinical data requests and clinical trial protocols, by comparing the prevalence trends of FSTs across three texts. Results: Our approach increased the average recall and speed by 12.8% and 47.02% respectively upon the baseline when mining FSTs from ClinicalTrials.gov, and maintained an overlap in relevant FSTs with the baseline ranging between 76.9% and 100% for varying FST frequency thresholds. The FSTs saturated when the data size reached 200 documents. Consistent trends in the prevalence of FST were observed across the three texts as the data size or frequency threshold changed. Conclusions: This paper contributes an adaptive tag-mining framework that is scalable and adaptable without sacrificing its recall. This component-based architectural design can be potentially generalizable to improve the adaptability of other clinical text mining methods.
AU  - Hao, T.
AU  - Weng, C.
DA  - 2015
IS  - 2
KW  - eppi-reviewer4
N1  - 21443467
8075
Hao, T. Weng, C.
PY  - 2015
SN  - 0026-1270
SP  - 164-170
ST  - Adaptive Semantic Tag Mining from Heterogeneous Clinical Research Texts
T2  - Methods of Information in Medicine
TI  - Adaptive Semantic Tag Mining from Heterogeneous Clinical Research Texts
UR  - <Go to ISI>://WOS:000351804000008
http://methods.schattauer.de/contents/archivestandard/issue/2195/manuscript/23608/download.html
VL  - 54
ID  - 170
ER  - 


TY  - JOUR
AB  - OBJECTIVES: Radiology reports are typically made in narrative form; this is a barrier to the implementation of advanced applications for data analysis or a decision support. We developed a system that generates structured reports for chest x-ray radiography. METHODS: Based on analyzing existing reports, we determined the fundamental sentence structure of findings as compositions of procedure, region, finding, and diagnosis. We categorized the observation objects into lung, mediastinum, bone, soft tissue, and pleura and chest wall. The terms of region, finding, and diagnosis were associated with each other. We expressed the terms and the relations between the terms using a resource description framework (RDF) and developed a reporting system based on it. The system shows a list of terms in each category, and modifiers can be entered using templates that are linked to each term. This system guides users to select terms by highlighting associated terms. Fifty chest x-rays with abnormal findings were interpreted by five radiologists and reports were made either by the system or by the free-text method. RESULTS: The system decreased the time needed to make a report by 12.5% compared with the free-text method, and the sentences generated by the system were well concordant with those made by free-text method (F-measure = 90%). The results of the questionnaire showed that our system is applicable to radiology reports of chest x-rays in daily clinical practice. CONCLUSIONS: The method of generating structured reports for chest x-rays was feasible, because it generated almost concordant reports in shorter time compared with the free-text method.
AU  - Hasegawa, Y.
AU  - Matsumura, Y.
AU  - Mihara, N.
AU  - Kawakami, Y.
AU  - Sasai, K.
AU  - Takeda, H.
AU  - Nakamura, H.
DA  - 2010
DO  - 10.3414/ME09-01-0014. Epub 2010 Jul 6.
IS  - 4
KW  - eppi-reviewer4
N1  - 21444715
5735
PY  - 2010
SN  - 0026-1270 (Print) 0026-1270 (Linking)
SP  - 360-70
ST  - Development of a system that generates structured reports for chest x-ray radiography
T2  - Methods Inf Med
TI  - Development of a system that generates structured reports for chest x-ray radiography
UR  - http://methods.schattauer.de/contents/archivestandard/issue/1102/manuscript/13283/download.html
VL  - 49
ID  - 98
ER  - 


TY  - JOUR
AB  - OBJECTIVES: The radiology report is the most important source of clinical imaging information. It documents critical information about the patient's health and the radiologist's interpretation of medical findings. It also communicates information to the referring physicians and records that information for future clinical and research use. Although efforts to structure some radiology report information through predefined templates are beginning to bear fruit, a large portion of radiology report information is entered in free text. The free text format is a major obstacle for rapid extraction and subsequent use of information by clinicians, researchers, and healthcare information systems. This difficulty is due to the ambiguity and subtlety of natural language, complexity of described images, and variations among different radiologists and healthcare organizations. As a result, radiology reports are used only once by the clinician who ordered the study and rarely are used again for research and data mining. In this work, machine learning techniques and a large multi-institutional radiology report repository are used to extract the semantics of the radiology report and overcome the barriers to the re-use of radiology report information in clinical research and other healthcare applications. MATERIAL AND METHODS: We describe a machine learning system to annotate radiology reports and extract report contents according to an information model. This information model covers the majority of clinically significant contents in radiology reports and is applicable to a wide variety of radiology study types. Our automated approach uses discriminative sequence classifiers for named-entity recognition to extract and organize clinically significant terms and phrases consistent with the information model. We evaluated our information extraction system on 150 radiology reports from three major healthcare organizations and compared its results to a commonly used non-machine learning information extraction method. We also evaluated the generalizability of our approach across different organizations by training and testing our system on data from different organizations. RESULTS: Our results show the efficacy of our machine learning approach in extracting the information model's elements (10-fold cross-validation average performance: precision: 87%, recall: 84%, F1 score: 85%) and its superiority and generalizability compared to the common non-machine learning approach (p-value<0.05). CONCLUSIONS: Our machine learning information extraction approach provides an effective automatic method to annotate and extract clinically significant information from a large collection of free text radiology reports. This information extraction system can help clinicians better understand the radiology reports and prioritize their review process. In addition, the extracted information can be used by researchers to link radiology reports to information from other data sources such as electronic health records and the patient's genome. Extracted information also can facilitate disease surveillance, real-time clinical decision support for the radiologist, and content-based image retrieval.
AU  - Hassanpour, S.
AU  - Langlotz, C. P.
DA  - 2016
DO  - 10.1016/j.artmed.2015.09.007. Epub 2015 Oct 3.
KW  - eppi-reviewer4
N1  - 21446064
3880
PY  - 2016
SN  - 1873-2860 (Electronic) 0933-3657 (Linking)
SP  - 29-39
ST  - Information extraction from multi-institutional radiology reports
T2  - Artif Intell Med
TI  - Information extraction from multi-institutional radiology reports
UR  - http://ac.els-cdn.com/S0933365715001244/1-s2.0-S0933365715001244-main.pdf?_tid=f8d37b2e-5416-11e6-9990-00000aacb361&acdnat=1469637012_89e66226172cfc1a549ba6fdd2016e87
VL  - 66
ID  - 155
ER  - 


TY  - JOUR
AB  - Much evidence has shown that people's physical and mental health can be predicted by the words they use. However, such verbal information is seldom used in the screening and diagnosis process probably because the procedure to handle these words is rather difficult with traditional quantitative methods. The first challenge would be to extract robust information from diversified expression patterns, the second to transform unstructured text into a structuralized dataset. The present study developed a new textual assessment method to screen the posttraumatic stress disorder (PTSD) patients using lexical features in the self narratives with text mining techniques. Using 300 self narratives collected online, we extracted highly discriminative keywords with the Chi-square algorithm and constructed a textual assessment model to classify individuals with the presence or absence of PTSD. This resulted in a high agreement between computer and psychiatrists' diagnoses for PTSD and revealed some expressive characteristics in the writings of PTSD patients. Although the results of text analysis are not completely analogous to the results of structured interviews in PTSD diagnosis, the application of text mining is a promising addition to assessing PTSD in clinical and research settings.
AU  - He, Q.
AU  - Veldkamp, B. P.
AU  - de, Vries
DA  - 2012
DO  - 10.1016/j.psychres.2012.01.032. Epub 2012 Mar 29.
IS  - 3
KW  - eppi-reviewer4
N1  - 21447945
5216
PY  - 2012
SN  - 1872-7123 (Electronic) 0165-1781 (Linking)
SP  - 441-7
ST  - Screening for posttraumatic stress disorder using verbal features in self narratives: a text mining approach
T2  - Psychiatry Res
TI  - Screening for posttraumatic stress disorder using verbal features in self narratives: a text mining approach
UR  - http://ac.els-cdn.com/S0165178112000625/1-s2.0-S0165178112000625-main.pdf?_tid=f42ee5fe-5416-11e6-8d4d-00000aacb360&acdnat=1469637004_04baa1657ce9b2a34169ae865a5453c0
VL  - 198
ID  - 99
ER  - 


TY  - JOUR
AB  - ClinicalTrials.gov presents great opportunities for analyzing commonalities in clinical trial target populations to facilitate knowledge reuse when designing eligibility criteria of future trials or to reveal potential systematic biases in selecting population subgroups for clinical research. Towards this goal, this paper presents a novel data resource for enabling such analyses. Our method includes two parts: (1) parsing and indexing eligibility criteria text; and (2) mining common eligibility features and attributes of common numeric features (e.g., A1c). We designed and built a database called "Commonalities in Target Populations of Clinical Trials" (COMPACT), which stores structured eligibility criteria and trial metadata in a readily computable format. We illustrate its use in an example analytic module called CONECT using COMPACT as the backend. Type 2 diabetes is used as an example to analyze commonalities in the target populations of 4,493 clinical trials on this disease.
AU  - He, Z.
AU  - Carini, S.
AU  - Hao, T.
AU  - Sim, I.
AU  - Weng, C.
DA  - 2014
KW  - eppi-reviewer4
N1  - 21446567
4595
He, Zhe Carini, Simona Hao, Tianyong Sim, Ida Weng, Chunhua R01 LM009886/LM/NLM NIH HHS/United States R01LM009886/LM/NLM NIH HHS/United States UL1 TR000040/TR/NCATS NIH HHS/United States Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2014 Nov 14;2014:1777-86. eCollection 2014.
PY  - 2014
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1777-86
ST  - A method for analyzing commonalities in clinical trial target populations
T2  - AMIA Annu Symp Proc
TI  - A method for analyzing commonalities in clinical trial target populations
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4419878/pdf/1969382.pdf
VL  - 2014
ID  - 56
ER  - 


TY  - JOUR
AB  - BACKGROUND: Translational research typically requires data abstracted from medical records as well as data collected specifically for research. Unfortunately, many data within electronic health records are represented as text that is not amenable to aggregation for analyses. We present a scalable open source SQL Server Integration Services package, called Regextractor, for including regular expression parsers into a classic extract, transform, and load workflow. We have used Regextractor to abstract discrete data from textual reports from a number of 'machine generated' sources. To validate this package, we created a pulmonary function test data mart and analyzed the quality of the data mart versus manual chart review. METHODS: Eleven variables from pulmonary function tests performed closest to the initial clinical evaluation date were studied for 100 randomly selected subjects with scleroderma. One research assistant manually reviewed, abstracted, and entered relevant data into a database. Correlation with data obtained from the automated pulmonary function test data mart within the Northwestern Medical Enterprise Data Warehouse was determined. RESULTS: There was a near perfect (99.5%) agreement between results generated from the Regextractor package and those obtained via manual chart abstraction. The pulmonary function test data mart has been used subsequently to monitor disease progression of patients in the Northwestern Scleroderma Registry. In addition to the pulmonary function test example presented in this manuscript, the Regextractor package has been used to create cardiac catheterization and echocardiography data marts. The Regextractor package was released as open source software in October 2009 and has been downloaded 552 times as of 6/1/2012. CONCLUSIONS: Collaboration between clinical researchers and biomedical informatics experts enabled the development and validation of a tool (Regextractor) to parse, abstract and assemble structured data from text data contained in the electronic health record. Regextractor has been successfully used to create additional data marts in other medical domains and is available to the public.
AU  - Hinchcliff, M.
AU  - Just, E.
AU  - Podlusky, S.
AU  - Varga, J.
AU  - Chang, R. W.
AU  - Kibbe, W. A.
DA  - 2012
DO  - 10.1186/1472-6947-12-106.
KW  - eppi-reviewer4
N1  - 21448379
5062
PY  - 2012
SN  - 1472-6947 (Electronic) 1472-6947 (Linking)
SP  - 106
ST  - Text data extraction for a prospective, research-focused data mart: implementation and validation
T2  - BMC Med Inform Decis Mak
TI  - Text data extraction for a prospective, research-focused data mart: implementation and validation
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3537747/pdf/1472-6947-12-106.pdf
VL  - 12
ID  - 23
ER  - 


TY  - JOUR
AB  - Phenotypes are the observable characteristics of an organism arising from its response to the environment. Phenotypes associated with engineered and natural genetic variation are widely recorded using phenotype ontologies in model organisms, as are signs and symptoms of human Mendelian diseases in databases such as OMIM and Orphanet. Exploiting these resources, several computational methods have been developed for integration and analysis of phenotype data to identify the genetic etiology of diseases or suggest plausible interventions. A similar resource would be highly useful not only for rare and Mendelian diseases, but also for common, complex and infectious diseases. We apply a semantic text-mining approach to identify the phenotypes (signs and symptoms) associated with over 6,000 diseases. We evaluate our text-mined phenotypes by demonstrating that they can correctly identify known disease-associated genes in mice and humans with high accuracy. Using a phenotypic similarity measure, we generate a human disease network in which diseases that have similar signs and symptoms cluster together, and we use this network to identify closely related diseases based on common etiological, anatomical as well as physiological underpinnings.
AU  - Hoehndorf, R.
AU  - Schofield, P. N.
AU  - Gkoutos, G. V.
DA  - 2015
DO  - 10.1038/srep10888.
KW  - eppi-reviewer4
N1  - 21443552
4003
PY  - 2015
SN  - 2045-2322 (Electronic) 2045-2322 (Linking)
SP  - 10888
ST  - Analysis of the human diseasome using phenotype similarity between common, genetic, and infectious diseases
T2  - Sci Rep
TI  - Analysis of the human diseasome using phenotype similarity between common, genetic, and infectious diseases
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4458913/pdf/srep10888.pdf
VL  - 5
ID  - 82
ER  - 


TY  - JOUR
AB  - In many healthcare organizations, comparative effectiveness research and quality improvement (QI) investigations are hampered by a lack of access to data created as a byproduct of patient care. Data collection often hinges upon either manual chart review or ad hoc requests to technical experts who support legacy clinical systems. In order to facilitate this needed capacity for data exploration at our institution (Duke University Health System), we have designed and deployed a robust Web application for cohort identification and data extraction--the Duke Enterprise Data Unified Content Explorer (DEDUCE). DEDUCE is envisioned as a simple, web-based environment that allows investigators access to administrative, financial, and clinical information generated during patient care. By using business intelligence tools to create a view into Duke Medicine's enterprise data warehouse, DEDUCE provides a Guided Query functionality using a wizard-like interface that lets users filter through millions of clinical records, explore aggregate reports, and, export extracts. Researchers and QI specialists can obtain detailed patient- and observation-level extracts without needing to understand structured query language or the underlying database model. Developers designing such tools must devote sufficient training and develop application safeguards to ensure that patient-centered clinical researchers understand when observation-level extracts should be used. This may mitigate the risk of data being misunderstood and consequently used in an improper fashion.
AU  - Horvath, M. M.
AU  - Winfield, S.
AU  - Evans, S.
AU  - Slopek, S.
AU  - Shang, H.
AU  - Ferranti, J.
DA  - 2011
DO  - 10.1016/j.jbi.2010.11.008. Epub 2010 Dec 2.
IS  - 2
KW  - eppi-reviewer4
N1  - 21444598
5605
PY  - 2011
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 266-76
ST  - The DEDUCE Guided Query tool: providing simplified access to clinical data for research and quality improvement
T2  - J Biomed Inform
TI  - The DEDUCE Guided Query tool: providing simplified access to clinical data for research and quality improvement
UR  - http://ac.els-cdn.com/S1532046410001747/1-s2.0-S1532046410001747-main.pdf?_tid=fbc2fcba-5416-11e6-b630-00000aab0f27&acdnat=1469637017_d41bd0ec7c24c918c6188a7f656fb972
VL  - 44
ID  - 24
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To measure the uncertainty of temporal assertions like "3 weeks ago" in clinical texts. DESIGN: Temporal assertions extracted from narrative clinical reports were compared to facts extracted from a structured clinical database for the same patients. MEASUREMENTS: The authors correlated the assertions and the facts to determine the dependence of the uncertainty of the assertions on the semantic and lexical properties of the assertions. RESULTS: The observed deviation between the stated duration and actual duration averaged about 20% of the stated deviation. Linear regression revealed that assertions about events further in the past tend to be more uncertain, smaller numeric values tend to be more uncertain (1 mo v. 30 d), and round numbers tend to be more uncertain (10 versus 11 yrs). CONCLUSIONS: The authors empirically derived semantics behind statements of duration using "ago," and verified intuitions about how numbers are used.
AU  - Hripcsak, G.
AU  - Elhadad, N.
AU  - Chen, Y. H.
AU  - Zhou, L.
AU  - Morrison, F. P.
DA  - 2009
DO  - 10.1197/jamia.M3007. Epub 2008 Dec 11.
IS  - 2
KW  - eppi-reviewer4
N1  - 21448675
6143
PY  - 2009
SN  - 1067-5027 (Print) 1067-5027 (Linking)
SP  - 220-7
ST  - Using empiric semantic correlation to interpret temporal assertions in clinical texts
T2  - J Am Med Inform Assoc
TI  - Using empiric semantic correlation to interpret temporal assertions in clinical texts
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2649319/pdf/220.S1067502708002430.main.pdf
VL  - 16
ID  - 70
ER  - 


TY  - JOUR
AB  - Data mining in electronic medical records may facilitate clinical research, but much of the structured data may be miscoded, incomplete, or non-specific. The exploitation of narrative data using natural language processing may help, although nesting, varying granularity, and repetition remain challenges. In a study of community-acquired pneumonia using electronic records, these issues led to poor classification. Limiting queries to accurate, complete records led to vastly reduced, possibly biased samples. We exploited knowledge latent in the electronic records to improve classification. A similarity metric was used to cluster cases. We defined discordance as the degree to which cases within a cluster give different answers for some query that addresses a classification task of interest. Cases with higher discordance are more likely to be incorrectly classified, and can be reviewed manually to adjust the classification, improve the query, or estimate the likely accuracy of the query. In a study of pneumonia--in which the ICD9-CM coding was found to be very poor--the discordance measure was statistically significantly correlated with classification correctness (.45; 95% CI .15-.62).
AU  - Hripcsak, G.
AU  - Knirsch, C.
AU  - Zhou, L.
AU  - Wilcox, A.
AU  - Melton, G. B.
DA  - 2007
IS  - 3
KW  - eppi-reviewer4
N1  - 21448669
6699
Hripcsak, George Knirsch, Charles Zhou, Li Wilcox, Adam Melton, Genevieve B R01 LM006910/LM/NLM NIH HHS/United States R01 LM06910/LM/NLM NIH HHS/United States Research Support, N.I.H., Extramural Research Support, Non-U.S. Gov't United States Comput Biol Med. 2007 Mar;37(3):296-304. Epub 2006 Apr 18.
PY  - 2007
SN  - 0010-4825 (Print) 0010-4825 (Linking)
SP  - 296-304
ST  - Using discordance to improve classification in narrative clinical databases: an application to community-acquired pneumonia
T2  - Comput Biol Med
TI  - Using discordance to improve classification in narrative clinical databases: an application to community-acquired pneumonia
UR  - http://ac.els-cdn.com/S0010482506000345/1-s2.0-S0010482506000345-main.pdf?_tid=fe982370-5416-11e6-8178-00000aacb360&acdnat=1469637022_f6b89de2811c69739711ab10fe30b2cc
VL  - 37
ID  - 25
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To assess the performance of electronic health record data for syndromic surveillance and to assess the feasibility of broadly distributed surveillance. DESIGN: Two systems were developed to identify influenza-like illness and gastrointestinal infectious disease in ambulatory electronic health record data from a network of community health centers. The first system used queries on structured data and was designed for this specific electronic health record. The second used natural language processing of narrative data, but its queries were developed independently from this health record. Both were compared to influenza isolates and to a verified emergency department chief complaint surveillance system. MEASUREMENTS: Lagged cross-correlation and graphs of the three time series. RESULTS: For influenza-like illness, both the structured and narrative data correlated well with the influenza isolates and with the emergency department data, achieving cross-correlations of 0.89 (structured) and 0.84 (narrative) for isolates and 0.93 and 0.89 for emergency department data, and having similar peaks during influenza season. For gastrointestinal infectious disease, the structured data correlated fairly well with the emergency department data (0.81) with a similar peak, but the narrative data correlated less well (0.47). CONCLUSIONS: It is feasible to use electronic health records for syndromic surveillance. The structured data performed best but required knowledge engineering to match the health record data to the queries. The narrative data illustrated the potential performance of a broadly disseminated system and achieved mixed results.
AU  - Hripcsak, G.
AU  - Soulakis, N. D.
AU  - Li, L.
AU  - Morrison, F. P.
AU  - Lai, A. M.
AU  - Friedman, C.
AU  - Calman, N. S.
AU  - Mostashari, F.
DA  - 2009
DO  - 10.1197/jamia.M2922. Epub 2009 Mar 4.
IS  - 3
KW  - eppi-reviewer4
N1  - 21448301
6084
PY  - 2009
SN  - 1067-5027 (Print) 1067-5027 (Linking)
SP  - 354-61
ST  - Syndromic surveillance using ambulatory electronic health records
T2  - J Am Med Inform Assoc
TI  - Syndromic surveillance using ambulatory electronic health records
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2732227/pdf/354.S1067502709000280.main.pdf
VL  - 16
ID  - 192
ER  - 


TY  - BOOK
A2  - Yang, M. Q.
A2  - Zhu, M. M.
A2  - Zhang, Y.
A2  - Arabnia, H. R.
A2  - Deng, Y.
A2  - Bourbakis, N.
AB  - Despite an influx of molecular data in the form of sequences, structure, transcription profiles etc., most of the protein interaction information relevant to cell biology research still exists strictly in the scientific literature which is written in a natural language that computers cannot easily manipulate. Automatically mining and extracting information from biomedical text holds the promise of easily consolidating large amounts of biological knowledge in computer-accessible form. In this talk, we present a novel approach Bio-IEDM (Biomedical Information Extraction and Data Mining) to integrate text mining and predictive modeling to analyze biomolecular network from biomedical literature databases. Our method consists of two phases. In phase 1, we discuss a semi-supervised efficient learning approach to automatically extract biological relationships such as protein-protein interaction, protein-gene interaction from the biomedical literature databases to construct the biomolecular network. In phase 2, we present a novel clustering algorithm to analyze the biomolecular network graph to identify biologically meaningful subnetworks (communities). The clustering algorithm considers the characteristics of the scale-free network graphs and is based on the local density of the vertex and its neighborhood functions that can be used to find more meaningful clusters with different density level. The experimental results indicate our approach is very effective in extracting biological knowledge from a huge collection of biomedical literatures. The integration of data mining and information extraction provides a promising direction for analyzing the biomolecular network.
AU  - Hu, X. H.
AU  - Yang, J. Y.
DA  - 2007
KW  - eppi-reviewer4
N1  - 21443872
10095
Hu, Xiaohua 7th IEEE International Conference on Bioinformatics and Bioengineering OCT 14-17, 2007 Boston, MA IEEE, IEEE Comp Soc, IEEE Engn Med Biol, NSF, Int Soc Intelligent Biol Med, Syst, Man, Cybermet Soc
PY  - 2007
SN  - 978-1-4244-1509-0
SP  - 1446-1446
ST  - Biomedical literature mining
T2  - Proceedings of the 7th Ieee International Symposium on Bioinformatics and Bioengineering, Vols I and Ii
TI  - Biomedical literature mining
UR  - <Go to ISI>://WOS:000252958200237
ID  - 157
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Depression is a prevalent disorder difficult to diagnose and treat. In particular, depressed patients exhibit largely unpredictable responses to treatment. Toward the goal of personalizing treatment for depression, we develop and evaluate computational models that use electronic health record (EHR) data for predicting the diagnosis and severity of depression, and response to treatment. MATERIALS AND METHODS: We develop regression-based models for predicting depression, its severity, and response to treatment from EHR data, using structured diagnosis and medication codes as well as free-text clinical reports. We used two datasets: 35,000 patients (5000 depressed) from the Palo Alto Medical Foundation and 5651 patients treated for depression from the Group Health Research Institute. RESULTS: Our models are able to predict a future diagnosis of depression up to 12 months in advance (area under the receiver operating characteristic curve (AUC) 0.70-0.80). We can differentiate patients with severe baseline depression from those with minimal or mild baseline depression (AUC 0.72). Baseline depression severity was the strongest predictor of treatment response for medication and psychotherapy. CONCLUSIONS: It is possible to use EHR data to predict a diagnosis of depression up to 12 months in advance and to differentiate between extreme baseline levels of depression. The models use commonly available data on diagnosis, medication, and clinical progress notes, making them easily portable. The ability to automatically determine severity can facilitate assembly of large patient cohorts with similar severity from multiple sites, which may enable elucidation of the moderators of treatment response in the future.
AU  - Huang, S. H.
AU  - LePendu, P.
AU  - Iyer, S. V.
AU  - Tai-Seale, M.
AU  - Carrell, D.
AU  - Shah, N. H.
DA  - 2014
DO  - 10.1136/amiajnl-2014-002733. Epub 2014 Jul 2.
IS  - 6
KW  - eppi-reviewer4
N1  - 21448454
4393
PY  - 2014
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 1069-75
ST  - Toward personalizing treatment for depression: predicting diagnosis and severity
T2  - J Am Med Inform Assoc
TI  - Toward personalizing treatment for depression: predicting diagnosis and severity
UR  - http://jamia.oxfordjournals.org/content/jaminfo/21/6/1069.full.pdf
VL  - 21
ID  - 83
ER  - 


TY  - JOUR
AB  - Post-marketing drug surveillance for adverse drug events (ADEs) has typically relied on spontaneous reporting. Recently, regulatory agencies have turned their attention to more preemptive approaches that use existing data for surveillance. We conducted an environmental scan to identify active surveillance systems worldwide that use existing data for the detection of ADEs. We extracted data about the systems' structures, data, and functions. We synthesized the information across systems to identify common features of these systems. We identified nine active surveillance systems. Two systems are US based-the FDA Sentinel Initiative (including both the Mini-Sentinel Initiative and the Federal Partner Collaboration) and the Vaccine Safety Datalink (VSD); two are Canadian-the Canadian Network for Observational Drug Effect Studies (CNODES) and the Vaccine and Immunization Surveillance in Ontario (VISION); and two are European-the Exploring and Understanding Adverse Drug Reactions by Integrative Mining of Clinical Records and Biomedical Knowledge (EU-ADR) Alliance and the Vaccine Adverse Event Surveillance and Communication (VAESCO). Additionally, there is the Asian Pharmacoepidemiology Network (AsPEN) and the Shanghai Drug Monitoring and Evaluative System (SDMES). We identified two systems in the UK-the Vigilance and Risk Management of Medicines (VRMM) Division and the Drug Safety Research Unit (DSRU), an independent academic unit. These surveillance systems mostly use administrative claims or electronic medical records; most conduct pharmacovigilance on behalf of a regulatory agency. Either a common data model or a centralized model is used to access existing data. The systems have been built using national data alone or via partnership with other countries. However, active surveillance systems using existing data remain rare. North America and Europe have the most population coverage; with Asian countries making good advances.
AU  - Huang, Y. L.
AU  - Moon, J.
AU  - Segal, J. B.
DA  - 2014
DO  - 10.1007/s40264-014-0194-3.
IS  - 8
KW  - eppi-reviewer4
N1  - 21444232
4380
PY  - 2014
SN  - 0114-5916 (Print) 0114-5916 (Linking)
SP  - 581-96
ST  - A comparison of active adverse event surveillance systems worldwide
T2  - Drug Saf
TI  - A comparison of active adverse event surveillance systems worldwide
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4134479/pdf/40264_2014_Article_194.pdf
VL  - 37
ID  - 71
ER  - 


TY  - JOUR
AB  - Postoperative Acute Respiratory Failure (ARF) is a serious complication in critical care affecting patient morbidity and mortality. In this paper we investigate a novel approach to predicting ARF in critically ill patients. We study the use of two disparate sources of information - semi-structured text contained in nursing notes and investigative reports that are regularly recorded and the respiration rate, a physiological signal that is continuously monitored during a patient's ICU stay. Unlike previous works that retrospectively analyze complications, we exclude discharge summaries from our analysis envisaging a real time system that predicts ARF during the ICU stay. Our experiments, on more than 800 patient records from the MIMIC II database, demonstrate that text sources within the ICU contain strong signals for distinguishing between patients who are at risk for ARF from those who are not at risk. These results suggest that large scale systems using both structured and unstructured data recorded in critical care can be effectively used to predict complications, which in turn can lead to preemptive care with potentially improved outcomes, mortality rates and decreased length of stay and cost.
AU  - Huddar, V.
AU  - Rajan, V.
AU  - Bhattacharya, S.
AU  - Roy, S.
DA  - 2014
DO  - 10.1109/EMBC.2014.6944180.
KW  - eppi-reviewer4
N1  - 21447438
4152
PY  - 2014
SN  - 1557-170X (Print) 1557-170X (Linking)
SP  - 2702-5
ST  - Predicting postoperative acute respiratory failure in critical care using nursing notes and physiological signals
T2  - Conf Proc IEEE Eng Med Biol Soc
TI  - Predicting postoperative acute respiratory failure in critical care using nursing notes and physiological signals
UR  - http://ieeexplore.ieee.org/ielx7/6923026/6943513/06944180.pdf?tp=&arnumber=6944180&isnumber=6943513
VL  - 2014
ID  - 198
ER  - 


TY  - JOUR
AB  - BACKGROUND: The Center for Disease Control and Prevention (CDC) indicates that one of the largest problems threatening healthcare includes antibiotic resistance. Tetracycline, an effective antibiotic that has been in use for many years, is becoming less successful in treating certain pathogens. To better understand the temporal patterns in the growth of antibiotic resistance, patient diagnostic test records can be analyzed. METHODS: Data mining methods including frequent item set mining and association rules via the Apriori algorithm were used to analyze results from 80,241 Target Enriched Multiplex-PCR (TEM-PCR) reference laboratory tests. From the data mining results, five common respiratory pathogens and their co-detection rates with tetracycline resistance genes (TRG) were further analyzed and organized according to year, patient age, and geography. RESULTS: From 2010, all five pathogens were associated with at least a 24% rise in co-detection rate for TRGs. Patients from 0-2 years old exhibited the lowest rate of TRG co-detection, while patients between 13-50 years old displayed the highest frequency of TRG co-detection. The Northeastern region of the United States recorded the highest rate of patients co-detected with a TRG and a respiratory pathogen. Along the East-west gradient, the relative frequency of co-detection between TRGs and respiratory pathogens decreased dramatically. CONCLUSIONS: Significant trends were uncovered regarding the co-detection frequencies of TRGs and respiratory pathogens over time. It is valuable for the field of public health to monitor trends regarding the spread of resistant infectious disease, especially since tetracycline continues to be utilized a treatment for various microbial infections. Analyzing large datasets containing TEM-PCR results for co-detections provides valuable insights into trends of antibiotic resistance gene expression so that the effectiveness of first-line treatments can be continuously monitored.
AU  - Huff, M. D.
AU  - Weisman, D.
AU  - Adams, J.
AU  - Li, S.
AU  - Green, J.
AU  - Malone, L. L.
AU  - Clemmons, S.
DA  - 2014
DO  - 10.1186/1471-2334-14-460.
KW  - eppi-reviewer4
N1  - 21445457
4323
PY  - 2014
SN  - 1471-2334 (Electronic) 1471-2334 (Linking)
SP  - 460
ST  - The frequency of tetracycline resistance genes co-detected with respiratory pathogens: a database mining study uncovering descriptive trends throughout the United States
T2  - BMC Infect Dis
TI  - The frequency of tetracycline resistance genes co-detected with respiratory pathogens: a database mining study uncovering descriptive trends throughout the United States
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4156627/pdf/12879_2014_Article_3763.pdf
VL  - 14
ID  - 11
ER  - 


TY  - JOUR
AB  - Natural Language Processing (NLP) offers an approach for capturing data from narratives and creating structured reports for further computer processing. We explored the ability of a NLP system, Medical Language Extraction and Encoding (MedLEE), on nursing narratives MedLEE extracted 490 concepts from narrative text in a sample of 553 oncology nursing process notes. The most frequently monitored and recorded signs and symptoms were related to chemotherapy care, such as adverse reactions shortness of breath, nausea, pain and bleeding. In terms of nursing interventions, chemotherapy blood culture, medication, and blood transfusion were commonly recorded in free text NLP may provide a feasible approach to extract data related to patient safety/quality measures and nursing outcomes by capturing nursing concepts that are not recorded through structured data entry. For better NLP performance in the domain of nursing additional nursing terms and abbreviations must be added to MedLEE lexicon.
AU  - Hyun, S.
AU  - Johnson, S. B.
AU  - Bakken, S.
DA  - 2009
IS  - 4
KW  - eppi-reviewer4
N1  - 21445268
9022
Hyun, Sookyung Johnson, Stephen B. Bakken, Suzanne Hyun, Sookyung/G-3509-2012
PY  - 2009
SN  - 1538-2931
SP  - 215-223
ST  - Exploring the Ability of Natural Language Processing to Extract Data From Nursing Narratives
T2  - Cin-Computers Informatics Nursing
TI  - Exploring the Ability of Natural Language Processing to Extract Data From Nursing Narratives
UR  - <Go to ISI>://WOS:000267722400002
http://ovidsp.tx.ovid.com/ovftpdfs/FPDDNCIBEDODMH00/fs047/ovft/live/gv024/00024665/00024665-200907000-00005.pdf
VL  - 27
ID  - 207
ER  - 


TY  - RPRT
AB  - Brain and Health Informatics (BHI) aims to develop and disseminate understandings of novel intelligent computing formalisms, techniques, and technologies in the special application contexts of brain and health/well-being related studies and services. It is devoted to interdisciplinary studies on BHI, covering computational, logical, cognitive, neurophysiological, biological, physical, ecological, and social perspectives of BHI, The conference was held in Maebashi, Japan, The proceedings contain 53 papers that were presented at BHI2013. Topics cover Thinking and perception-centric investigations of human information processing systems. Information technologies for curating, mining, managing and using big brain/health data, Technologies for healthcare. Data analysis, data mining and machine learning. Applications, Mental healthcare with ICT, Granular knowledge discovery in biomedical and active media environment, Human centered computing, Neuro-robotics, intelligent healthcare data analysis
AU  - Imamura, Kazuyuki
AU  - Usui, Shiro
AU  - Shirao, Tomoaki
AU  - Kasamatsu, Takuji
AU  - Schwabe, Lars
AU  - Zhong, Ning
DA  - 2013
KW  - eppi-reviewer4
N1  - 21508230
11490
PB  - MAEBASHI INST OF TECH (JAPAN)
PY  - 2013
RP  - 21508230
11490
SN  - ADA624430, AOARD, XC, CSP-131017, AOARD, FA2386-13-1-1017
SP  - 557
ST  - Brain and Health Informatics International Conference (BHI 2013) Held in Maebashi, Japan on October 29-31, 2013
TI  - Brain and Health Informatics International Conference (BHI 2013) Held in Maebashi, Japan on October 29-31, 2013
ID  - 40
ER  - 


TY  - JOUR
AB  - BioC is a recently created XML format to share text data and annotations, and an accompanying input/output library to promote interoperability of data and tools for natural language processing of biomedical text. This article reports the use of BioC to address a common challenge in processing biomedical text information-that of frequent entity name abbreviation. We selected three different abbreviation definition identification modules, and used the publicly available BioC code to convert these independent modules into BioC-compatible components that interact seamlessly with BioC-formatted data, and other BioC-compatible modules. In addition, we consider four manually annotated corpora of abbreviations in biomedical text: the Ab3P corpus of 1250 PubMed abstracts, the BIOADI corpus of 1201 PubMed abstracts, the old MEDSTRACT corpus of 199 PubMed((R)) citations and the Schwartz and Hearst corpus of 1000 PubMed abstracts. Annotations in these corpora have been re-evaluated by four annotators and their consistency and quality levels have been improved. We converted them to BioC-format and described the representation of the annotations. These corpora are used to measure the three abbreviation-finding algorithms and the results are given. The BioC-compatible modules, when compared with their original form, have no difference in their efficiency, running time or any other comparable aspects. They can be conveniently used as a common pre-processing step for larger multi-layered text-mining endeavors. Database URL: Code and data are available for download at the BioC site: http://bioc.sourceforge.net.
AU  - Islamaj, Dogan
AU  - Comeau, D. C.
AU  - Yeganova, L.
AU  - Wilbur, W. J.
DA  - 2014
DO  - 10.1093/database/bau044. Print 2014.
KW  - eppi-reviewer4
N1  - 21445380
4420
PY  - 2014
SN  - 1758-0463 (Electronic) 1758-0463 (Linking)
ST  - Finding abbreviations in biomedical literature: three BioC-compatible modules and four BioC-formatted corpora
T2  - Database (Oxford)
TI  - Finding abbreviations in biomedical literature: three BioC-compatible modules and four BioC-formatted corpora
VL  - 2014
ID  - 193
ER  - 


TY  - JOUR
AB  - The vast collection of biomedical literature and its continued expansion has presented a number of challenges to researchers who require structured findings to stay abreast of and analyze molecular mechanisms relevant to their domain of interest. By structuring literature content into topic-specific machine-readable databases, the aggregate data from multiple articles can be used to infer trends that can be compared and contrasted with similar findings from topic-independent resources. Our study presents a generalized procedure for semi-automatically creating a custom topic-specific molecular interaction database through the use of text mining to assist manual curation. We apply the procedure to capture molecular events that underlie 'pain', a complex phenomenon with a large societal burden and unmet medical need. We describe how existing text mining solutions are used to build a pain-specific corpus, extract molecular events from it, add context to the extracted events and assess their relevance. The pain-specific corpus contains 765 692 documents from Medline and PubMed Central, from which we extracted 356 499 unique normalized molecular events, with 261 438 single protein events and 93 271 molecular interactions supplied by BioContext. Event chains are annotated with negation, speculation, anatomy, Gene Ontology terms, mutations, pain and disease relevance, which collectively provide detailed insight into how that event chain is associated with pain. The extracted relations are visualized in a wiki platform (wiki-pain.org) that enables efficient manual curation and exploration of the molecular mechanisms that underlie pain. Curation of 1500 grouped event chains ranked by pain relevance revealed 613 accurately extracted unique molecular interactions that in the future can be used to study the underlying mechanisms involved in pain. Our approach demonstrates that combining existing text mining tools with domain-specific terms and wiki-based visualization can facilitate rapid curation of molecular interactions to create a custom database. Database URL: ***
AU  - Jamieson, D. G.
AU  - Roberts, P. M.
AU  - Robertson, D. L.
AU  - Sidders, B.
AU  - Nenadic, G.
DA  - 2013
DO  - 10.1093/database/bat033. Print 2013.
KW  - eppi-reviewer4
N1  - 21443969
4808
PY  - 2013
SN  - 1758-0463 (Electronic) 1758-0463 (Linking)
SP  - bat033
ST  - Cataloging the biomedical world of pain through semi-automated curation of molecular interactions
T2  - Database (Oxford)
TI  - Cataloging the biomedical world of pain through semi-automated curation of molecular interactions
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3662864/pdf/bat033.pdf
VL  - 2013
ID  - 124
ER  - 


TY  - JOUR
AB  - BACKGROUND: Comparative analysis of expression microarray studies is difficult due to the large influence of technical factors on experimental outcome. Still, the identified differentially expressed genes may hint at the same biological processes. However, manually curated assignment of genes to biological processes, such as pursued by the Gene Ontology (GO) consortium, is incomplete and limited. We hypothesised that automatic association of genes with biological processes through thesaurus-controlled mining of Medline abstracts would be more effective. Therefore, we developed a novel algorithm (LAMA: Literature-Aided Meta-Analysis) to quantify the similarity between transcriptomics studies. We evaluated our algorithm on a large compendium of 102 microarray studies published in the field of muscle development and disease, and compared it to similarity measures based on gene overlap and over-representation of biological processes assigned by GO. RESULTS: While the overlap in both genes and overrepresented GO-terms was poor, LAMA retrieved many more biologically meaningful links between studies, with substantially lower influence of technical factors. LAMA correctly grouped muscular dystrophy, regeneration and myositis studies, and linked patient and corresponding mouse model studies. LAMA also retrieves the connecting biological concepts. Among other new discoveries, we associated cullin proteins, a class of ubiquitinylation proteins, with genes down-regulated during muscle regeneration, whereas ubiquitinylation was previously reported to be activated during the inverse process: muscle atrophy. CONCLUSION: Our literature-based association analysis is capable of finding hidden common biological denominators in microarray studies, and circumvents the need for raw data analysis or curated gene annotation databases.
AU  - Jelier, R.
AU  - t, Hoen
AU  - P, A.
AU  - Sterrenburg, E.
AU  - den, Dunnen
AU  - J, T.
AU  - van, Ommen
AU  - G, J.
AU  - Kors, J. A.
AU  - Mons, B.
DA  - 2008
DO  - 10.1186/1471-2105-9-291.
KW  - eppi-reviewer4
N1  - 21446401
6258
PY  - 2008
SN  - 1471-2105 (Electronic) 1471-2105 (Linking)
SP  - 291
ST  - Literature-aided meta-analysis of microarray data: a compendium study on muscle development and disease
T2  - BMC Bioinformatics
TI  - Literature-aided meta-analysis of microarray data: a compendium study on muscle development and disease
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2459190/pdf/1471-2105-9-291.pdf
VL  - 9
ID  - 133
ER  - 


TY  - JOUR
AB  - BACKGROUND: Prioritizing genetic variants is a challenge because disease susceptibility loci are often located in genes of unknown function or the relationship with the corresponding phenotype is unclear. A global data-mining exercise on the biomedical literature can establish the phenotypic profile of genes with respect to their connection to disease phenotypes. The importance of protein-protein interaction networks in the genetic heterogeneity of common diseases or complex traits is becoming increasingly recognized. Thus, the development of a network-based approach combined with phenotypic profiling would be useful for disease gene prioritization. RESULTS: We developed a random-set scoring model and implemented it to quantify phenotype relevance in a network-based disease gene-prioritization approach. We validated our approach based on different gene phenotypic profiles, which were generated from PubMed abstracts, OMIM, and GeneRIF records. We also investigated the validity of several vocabulary filters and different likelihood thresholds for predicted protein-protein interactions in terms of their effect on the network-based gene-prioritization approach, which relies on text-mining of the phenotype data. Our method demonstrated good precision and sensitivity compared with those of two alternative complex-based prioritization approaches. We then conducted a global ranking of all human genes according to their relevance to a range of human diseases. The resulting accurate ranking of known causal genes supported the reliability of our approach. Moreover, these data suggest many promising novel candidate genes for human disorders that have a complex mode of inheritance. CONCLUSION: We have implemented and validated a network-based approach to prioritize genes for human diseases based on their phenotypic profile. We have devised a powerful and transparent tool to identify and rank candidate genes. Our global gene prioritization provides a unique resource for the biological interpretation of data from genome-wide association studies, and will help in the understanding of how the associated genetic variants influence disease or quantitative phenotypes.
AU  - Jiang, L.
AU  - Edwards, S. M.
AU  - Thomsen, B.
AU  - Workman, C. T.
AU  - Guldbrandtsen, B.
AU  - Sorensen, P.
DA  - 2014
DO  - 10.1186/1471-2105-15-315.
KW  - eppi-reviewer4
N1  - 21447661
4273
PY  - 2014
SN  - 1471-2105 (Electronic) 1471-2105 (Linking)
SP  - 315
ST  - A random set scoring model for prioritization of disease candidate genes using protein complexes and data-mining of GeneRIF, OMIM and PubMed records
T2  - BMC Bioinformatics
TI  - A random set scoring model for prioritization of disease candidate genes using protein complexes and data-mining of GeneRIF, OMIM and PubMed records
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4181406/pdf/12859_2013_Article_6589.pdf
VL  - 15
ID  - 84
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To develop an electronic health record that facilitates rapid capture of detailed narrative observations from clinicians, with partial structuring of narrative information for integration and reuse. DESIGN: We propose a design in which unstructured text and coded data are fused into a single model called structured narrative. Each major clinical event (e.g., encounter or procedure) is represented as a document that is marked up to identify gross structure (sections, fields, paragraphs, lists) as well as fine structure within sentences (concepts, modifiers, relationships). Marked up items are associated with standardized codes that enable linkage to other events, as well as efficient reuse of information, which can speed up data entry by clinicians. Natural language processing is used to identify fine structure, which can reduce the need for form-based entry. VALIDATION: The model is validated through an example of use by a clinician, with discussion of relevant aspects of the user interface, data structures and processing rules. DISCUSSION: The proposed model represents all patient information as documents with standardized gross structure (templates). Clinicians enter their data as free text, which is coded by natural language processing in real time making it immediately usable for other computation, such as alerts or critiques. In addition, the narrative data annotates and augments structured data with temporal relations, severity and degree modifiers, causal connections, clinical explanations and rationale. CONCLUSION: Structured narrative has potential to facilitate capture of data directly from clinicians by allowing freedom of expression, giving immediate feedback, supporting reuse of clinical information and structuring data for subsequent processing, such as quality assurance and clinical research.
AU  - Johnson, S. B.
AU  - Bakken, S.
AU  - Dine, D.
AU  - Hyun, S.
AU  - Mendonca, E.
AU  - Morrison, F.
AU  - Bright, T.
AU  - Van, Vleck
AU  - Wrenn, J.
AU  - Stetson, P.
DA  - 2008
IS  - 1
KW  - eppi-reviewer4
N1  - 21445006
6416
Johnson, Stephen B Bakken, Suzanne Dine, Daniel Hyun, Sookyung Mendonca, Eneida Morrison, Frances Bright, Tiffani Van Vleck, Tielman Wrenn, Jesse Stetson, Peter 1K22 LM008805/LM/NLM NIH HHS/United States 5R01 LM007268/LM/NLM NIH HHS/United States Research Support, N.I.H., Extramural United States J Am Med Inform Assoc. 2008 Jan-Feb;15(1):54-64. Epub 2007 Oct 18.
PY  - 2008
SN  - 1067-5027 (Print) 1067-5027 (Linking)
SP  - 54-64
ST  - An electronic health record based on structured narrative
T2  - J Am Med Inform Assoc
TI  - An electronic health record based on structured narrative
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2274868/pdf/54.S1067502707002605.main.pdf
VL  - 15
ID  - 180
ER  - 


TY  - JOUR
AB  - BACKGROUND AND AIMS: Biliary dyskinesia is a common diagnosis that frequently results in cholecystectomy. In adults, most clinicians use a cutoff value for the gallbladder ejection fraction (GBEF) of <35% to define the disease. This disorder is not well characterized in children. Our aim was to determine the relationship between GBEF and gallbladder pathology using a large state-wide medical record repository. METHODS: We obtained records from all patients 21 years old and younger who underwent HIDA testing within the Indiana Network for Patient Care (INPC) from 2004 to 2013. GBEF results were obtained from radiology reports using data mining techniques. Age, gender, race, and insurance status were obtained for each patient. Any gallbladder pathology obtained subsequent to a hepatic iminodiacetic acid (HIDA) scan were also obtained and parsed for mention of cholecystitis, cholelithiasis, or cholesterolosis. We performed mixed effects logistic regression analysis to determine the influence of age, gender, race, insurance status, pathologist, and GBEF on the presence of these histologic findings. RESULTS: 2,841 HIDA scans on 2,558 patients were found. Of these, 310 patients had a full text gallbladder pathology report paired with the HIDA scan. GBEF did not correlate with the presence of gallbladder pathology (cholecystitis, cholelithiasis, or cholesterolosis) when controlling for age, gender, race, insurance status, and pathologist using a mixed effects model. CONCLUSIONS: Hypokinetic gallbladders are no more likely to have gallbladder pathology than normal or hyperkinetic gallbladders in the setting of a patient with both a HIDA scan and a cholecystectomy. Care should be used when interpreting the results of HIDA scans in children and adolescents.
AU  - Jones, P. M.
AU  - Rosenman, M. B.
AU  - Pfefferkorn, M. D.
AU  - Rescorla, F. J.
AU  - Bennett, W. E.
AU  - Jr
DA  - 2015
KW  - eppi-reviewer4
N1  - 21445515
3854
Jones, Patrick M Rosenman, Marc B Pfefferkorn, Marian D Rescorla, Frederick J Bennett, William E Jr J Pediatr Gastroenterol Nutr. 2015 Dec 14.
PY  - 2015
SN  - 1536-4801 (Electronic) 0277-2116 (Linking)
ST  - Gallbladder Ejection Fraction is Unrelated to Gallbladder Pathology in Children and Adolescents
T2  - J Pediatr Gastroenterol Nutr
TI  - Gallbladder Ejection Fraction is Unrelated to Gallbladder Pathology in Children and Adolescents
ID  - 181
ER  - 


TY  - JOUR
AB  - BACKGROUND: Antipsychotic prescription information is commonly derived from structured fields in clinical health records. However, utilising diverse and comprehensive sources of information is especially important when investigating less frequent patterns of medication prescribing such as antipsychotic polypharmacy (APP). This study describes and evaluates a novel method of extracting APP data from both structured and free-text fields in electronic health records (EHRs), and its use for research purposes. METHODS: Using anonymised EHRs, we identified a cohort of patients with serious mental illness (SMI) who were treated in South London and Maudsley NHS Foundation Trust mental health care services between 1 January and 30 June 2012. Information about antipsychotic co-prescribing was extracted using a combination of natural language processing and a bespoke algorithm. The validity of the data derived through this process was assessed against a manually coded gold standard to establish precision and recall. Lastly, we estimated the prevalence and patterns of antipsychotic polypharmacy. RESULTS: Individual instances of antipsychotic prescribing were detected with high precision (0.94 to 0.97) and moderate recall (0.57-0.77). We detected baseline APP (two or more antipsychotics prescribed in any 6-week window) with 0.92 precision and 0.74 recall and long-term APP (antipsychotic co-prescribing for 6 months) with 0.94 precision and 0.60 recall. Of the 7,201 SMI patients receiving active care during the observation period, 338 (4.7 %; 95 % CI 4.2-5.2) were identified as receiving long-term APP. Two second generation antipsychotics (64.8 %); and first -second generation antipsychotics were most commonly co-prescribed (32.5 %). CONCLUSIONS: These results suggest that this is a potentially practical tool for identifying polypharmacy from mental health EHRs on a large scale. Furthermore, extracted data can be used to allow researchers to characterize patterns of polypharmacy over time including different drug combinations, trends in polypharmacy prescribing, predictors of polypharmacy prescribing and the impact of polypharmacy on patient outcomes.
AU  - Kadra, G.
AU  - Stewart, R.
AU  - Shetty, H.
AU  - Jackson, R. G.
AU  - Greenwood, M. A.
AU  - Roberts, A.
AU  - Chang, C. K.
AU  - MacCabe, J. H.
AU  - Hayes, R. D.
DA  - 2015
DO  - 10.1186/s12888-015-0557-z.
KW  - eppi-reviewer4
N1  - 21445291
3954
PY  - 2015
SN  - 1471-244X (Electronic) 1471-244X (Linking)
SP  - 166
ST  - Extracting antipsychotic polypharmacy data from electronic health records: developing and evaluating a novel process
T2  - BMC Psychiatry
TI  - Extracting antipsychotic polypharmacy data from electronic health records: developing and evaluating a novel process
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4511263/pdf/12888_2015_Article_557.pdf
VL  - 15
ID  - 145
ER  - 


TY  - JOUR
AB  - BACKGROUND: Free-text medication prescriptions contain detailed instruction information that is key when preparing drug data for analysis. The objective of this study was to develop a novel model and automated text-mining method to extract detailed structured medication information from free-text prescriptions and explore their variability (e.g. optional dosages) in primary care research databases. METHODS: We introduce a prescription model that provides minimum and maximum values for dose number, frequency and interval, allowing modelling variability and flexibility within a drug prescription. We developed a text mining system that relies on rules to extract such structured information from prescription free-text dosage instructions. The system was applied to medication prescriptions from an anonymised primary care electronic record database (Clinical Practice Research Datalink, CPRD). RESULTS: We have evaluated our approach on a test set of 220 CPRD prescription free-text directions. The system achieved an overall accuracy of 91 % at the prescription level, with 97 % accuracy across the attribute levels. We then further analysed over 56,000 most common free text prescriptions from CPRD records and found that 1 in 4 has inherent variability, i.e. a choice in taking medication specified by different minimum and maximum doses, duration or frequency. CONCLUSIONS: Our approach provides an accurate, automated way of coding prescription free text information, including information about flexibility and variability within a prescription. The method allows the researcher to decide how best to prepare the prescription data for drug efficacy and safety analyses in any given setting, and test various scenarios and their impact.
AU  - Karystianis, G.
AU  - Sheppard, T.
AU  - Dixon, W. G.
AU  - Nenadic, G.
DA  - 2016
DO  - 10.1186/s12911-016-0255-x.
KW  - eppi-reviewer4
N1  - 21446797
3838
PY  - 2016
SN  - 1472-6947 (Electronic) 1472-6947 (Linking)
SP  - 18
ST  - Modelling and extraction of variability in free-text medication prescriptions from an anonymised primary care electronic medical record research database
T2  - BMC Med Inform Decis Mak
TI  - Modelling and extraction of variability in free-text medication prescriptions from an anonymised primary care electronic medical record research database
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4748480/pdf/12911_2016_Article_255.pdf
VL  - 16
ID  - 208
ER  - 


TY  - JOUR
AB  - The application of semantic technologies to the integration of biological data and the interoperability of bioinformatics analysis and visualization tools has been the common theme of a series of annual BioHackathons hosted in Japan for the past five years. Here we provide a review of the activities and outcomes from the BioHackathons held in 2011 in Kyoto and 2012 in Toyama. In order to efficiently implement semantic technologies in the life sciences, participants formed various sub-groups and worked on the following topics: Resource Description Framework (RDF) models for specific domains, text mining of the literature, ontology development, essential metadata for biological databases, platforms to enable efficient Semantic Web technology development and interoperability, and the development of applications for Semantic Web data. In this review, we briefly introduce the themes covered by these sub-groups. The observations made, conclusions drawn, and software development projects that emerged from these activities are discussed.
AU  - Katayama, T.
AU  - Wilkinson, M. D.
AU  - Aoki-Kinoshita, K. F.
AU  - Kawashima, S.
AU  - Yamamoto, Y.
AU  - Yamaguchi, A.
AU  - Okamoto, S.
AU  - Kawano, S.
AU  - Kim, J. D.
AU  - Wang, Y.
AU  - Wu, H.
AU  - Kano, Y.
AU  - Ono, H.
AU  - Bono, H.
AU  - Kocbek, S.
AU  - Aerts, J.
AU  - Akune, Y.
AU  - Antezana, E.
AU  - Arakawa, K.
AU  - Aranda, B.
AU  - Baran, J.
AU  - Bolleman, J.
AU  - Bonnal, R. J.
AU  - Buttigieg, P. L.
AU  - Campbell, M. P.
AU  - Chen, Y. A.
AU  - Chiba, H.
AU  - Cock, P. J.
AU  - Cohen, K. B.
AU  - Constantin, A.
AU  - Duck, G.
AU  - Dumontier, M.
AU  - Fujisawa, T.
AU  - Fujiwara, T.
AU  - Goto, N.
AU  - Hoehndorf, R.
AU  - Igarashi, Y.
AU  - Itaya, H.
AU  - Ito, M.
AU  - Iwasaki, W.
AU  - Kalas, M.
AU  - Katoda, T.
AU  - Kim, T.
AU  - Kokubu, A.
AU  - Komiyama, Y.
AU  - Kotera, M.
AU  - Laibe, C.
AU  - Lapp, H.
AU  - Lutteke, T.
AU  - Marshall, M. S.
AU  - Mori, T.
AU  - Mori, H.
AU  - Morita, M.
AU  - Murakami, K.
AU  - Nakao, M.
AU  - Narimatsu, H.
AU  - Nishide, H.
AU  - Nishimura, Y.
AU  - Nystrom-Persson, J.
AU  - Ogishima, S.
AU  - Okamura, Y.
AU  - Okuda, S.
AU  - Oshita, K.
AU  - Packer, N. H.
AU  - Prins, P.
AU  - Ranzinger, R.
AU  - Rocca-Serra, P.
AU  - Sansone, S.
AU  - Sawaki, H.
AU  - Shin, S. H.
AU  - Splendiani, A.
AU  - Strozzi, F.
AU  - Tadaka, S.
AU  - Toukach, P.
AU  - Uchiyama, I.
AU  - Umezaki, M.
AU  - Vos, R.
AU  - Whetzel, P. L.
AU  - Yamada, I.
AU  - Yamasaki, C.
AU  - Yamashita, R.
AU  - York, W. S.
AU  - Zmasek, C. M.
AU  - Kawamoto, S.
AU  - Takagi, T.
DA  - 2014
DO  - 10.1186/2041-1480-5-5.
IS  - 1
KW  - eppi-reviewer4
N1  - 21443857
4549
PY  - 2014
SN  - 2041-1480 (Electronic)
SP  - 5
ST  - BioHackathon series in 2011 and 2012: penetration of ontology and linked data in life science domains
T2  - J Biomed Semantics
TI  - BioHackathon series in 2011 and 2012: penetration of ontology and linked data in life science domains
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3978116/pdf/2041-1480-5-5.pdf
VL  - 5
ID  - 12
ER  - 


TY  - JOUR
AB  - Data contained in the electronic health record (EHR) present a tremendous opportunity to improve quality-of-care and enhance research capabilities. However, the EHR is not structured to provide data for such purposes: most clinical information is entered as free text and content varies substantially between providers. Discrete information on patients' functional status is typically not collected. Data extraction tools are often unavailable. We have developed the Knowledge Program (KP), a comprehensive initiative to improve the collection of discrete clinical information into the EHR and the retrievability of data for use in research, quality, and patient care. A distinct feature of the KP is the systematic collection of patient-reported outcomes, which is captured discretely, allowing more refined analyses of care outcomes. The KP capitalizes on features of the Epic EHR and utilizes an external IT infrastructure distinct from Epic for enhanced functionality. Here, we describe the development and implementation of the KP.
AU  - Katzan, I.
AU  - Speck, M.
AU  - Dopler, C.
AU  - Urchek, J.
AU  - Bielawski, K.
AU  - Dunphy, C.
AU  - Jehi, L.
AU  - Bae, C.
AU  - Parchman, A.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21446281
5302
Katzan, Irene Speck, Micheal Dopler, Chris Urchek, John Bielawski, Kay Dunphy, Cheryl Jehi, Lara Bae, Charles Parchman, Alandra United States AMIA Annu Symp Proc. 2011;2011:683-92. Epub 2011 Oct 22.
PY  - 2011
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 683-92
ST  - The Knowledge Program: an innovative, comprehensive electronic data capture system and warehouse
T2  - AMIA Annu Symp Proc
TI  - The Knowledge Program: an innovative, comprehensive electronic data capture system and warehouse
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3243190/pdf/0683_amia_2011_proc.pdf
VL  - 2011
ID  - 114
ER  - 


TY  - JOUR
AB  - Clinical data in electronic medical records (EMRs) are a potential source of longitudinal clinical data for research. The Electronic Medical Records and Genomics Network (eMERGE) investigates whether data captured through routine clinical care using EMRs can identify disease phenotypes with sufficient positive and negative predictive values for use in genome-wide association studies (GWAS). Using data from five different sets of EMRs, we have identified five disease phenotypes with positive predictive values of 73 to 98% and negative predictive values of 98 to 100%. Most EMRs captured key information (diagnoses, medications, laboratory tests) used to define phenotypes in a structured format. We identified natural language processing as an important tool to improve case identification rates. Efforts and incentives to increase the implementation of interoperable EMRs will markedly improve the availability of clinical data for genomics research.
AU  - Kho, A. N.
AU  - Pacheco, J. A.
AU  - Peissig, P. L.
AU  - Rasmussen, L.
AU  - Newton, K. M.
AU  - Weston, N.
AU  - Crane, P. K.
AU  - Pathak, J.
AU  - Chute, C. G.
AU  - Bielinski, S. J.
AU  - Kullo, I. J.
AU  - Li, R.
AU  - Manolio, T. A.
AU  - Chisholm, R. L.
AU  - Denny, J. C.
DA  - 2011
DO  - 10.1126/scitranslmed.3001807.
IS  - 79
KW  - eppi-reviewer4
N1  - 21445008
5505
PY  - 2011
SN  - 1946-6242 (Electronic) 1946-6234 (Linking)
SP  - 79re1
ST  - Electronic medical records for genetic research: results of the eMERGE consortium
T2  - Sci Transl Med
TI  - Electronic medical records for genetic research: results of the eMERGE consortium
UR  - http://stm.sciencemag.org/content/scitransmed/3/79/79re1.full.pdf
VL  - 3
ID  - 76
ER  - 


TY  - JOUR
AB  - Electronically stored clinical documents may contain both structured data and unstructured data. The use of structured clinical data varies by facility, but clinicians are familiar with coded data such as International Classification of Diseases, Ninth Revision, Systematized Nomenclature of Medicine-Clinical Terms codes, and commonly other data including patient chief complaints or laboratory results. Most electronic health records have much more clinical information stored as unstructured data, for example, clinical narrative such as history of present illness, procedure notes, and clinical decision making are stored as unstructured data. Despite the importance of this information, electronic capture or retrieval of unstructured clinical data has been challenging. The field of natural language processing (NLP) is undergoing rapid development, and existing tools can be successfully used for quality improvement, research, healthcare coding, and even billing compliance. In this brief review, we provide examples of successful uses of NLP using emergency medicine physician visit notes for various projects and the challenges of retrieving specific data and finally present practical methods that can run on a standard personal computer as well as high-end state-of-the-art funded processes run by leading NLP informatics researchers.
AU  - Kimia, A. A.
AU  - Savova, G.
AU  - Landschaft, A.
AU  - Harper, M. B.
DA  - 2015
DO  - 10.1097/PEC.0000000000000484.
IS  - 7
KW  - eppi-reviewer4
N1  - 21446197
3974
PY  - 2015
SN  - 1535-1815 (Electronic) 0749-5161 (Linking)
SP  - 536-41
ST  - An Introduction to Natural Language Processing: How You Can Get More From Those Electronic Notes You Are Generating
T2  - Pediatr Emerg Care
TI  - An Introduction to Natural Language Processing: How You Can Get More From Those Electronic Notes You Are Generating
UR  - http://ovidsp.tx.ovid.com/ovftpdfs/FPDDNCIBEDODMH00/fs046/ovft/live/gv025/00006565/00006565-201507000-00018.pdf
VL  - 31
ID  - 209
ER  - 


TY  - JOUR
AB  - BACKGROUND: Capturing accurate and machine-interpretable primary data from clinical encounters is a challenging task, yet critical to the integrity of the practice of medicine. We explore the intriguing possibility that technology can help accurately capture structured data from the clinical encounter using a combination of automated speech recognition (ASR) systems and tools for extraction of clinical meaning from narrative medical text. Our goal is to produce a displayed evolving encounter note, visible and editable (using speech) during the encounter. RESULTS: This is very ambitious, and so far we have taken only the most preliminary steps. We report a simple proof-of-concept system and the design of the more comprehensive one we are building, discussing both the engineering design and challenges encountered. Without a formal evaluation, we were encouraged by our initial results. The proof-of-concept, despite a few false positives, correctly recognized the proper category of single-and multi-word phrases in uncorrected ASR output. The more comprehensive system captures and transcribes speech and stores alternative phrase interpretations in an XML-based format used by a text-engineering framework. It does not yet use the framework to perform the language processing present in the proof-of-concept. CONCLUSION: The work here encouraged us that the goal is reachable, so we conclude with proposed next steps.Some challenging steps include acquiring a corpus of doctor-patient conversations, exploring a workable microphone setup, performing user interface research, and developing a multi-speaker version of our tools.
AU  - Klann, J. G.
AU  - Szolovits, P.
DA  - 2009
DO  - 10.1186/1472-6947-9-S1-S3.
KW  - eppi-reviewer4
N1  - 21446145
5915
PY  - 2009
SN  - 1472-6947 (Electronic) 1472-6947 (Linking)
SP  - S3
ST  - An intelligent listening framework for capturing encounter notes from a doctor-patient dialog
T2  - BMC Med Inform Decis Mak
TI  - An intelligent listening framework for capturing encounter notes from a doctor-patient dialog
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2773918/pdf/1472-6947-9-S1-S3.pdf
VL  - 9 Suppl 1
ID  - 199
ER  - 


TY  - JOUR
AB  - Large repositories of life science data in the form of domain-specific literature, textual databases and other large specialised textual collections (corpora) in electronic form increase on a daily basis to a level beyond the human mind can grasp and interpret. As the volume of data continues to increase, substantial support from new information technologies and computational techniques grounded in the form of the ever increasing applications of the mining paradigm is becoming apparent. These emerging technologies play an increasingly critical role in aiding research productivity, and they provide the means for reducing the workload for information access and decision support and for speeding up and enhancing the knowledge discovery process. In order to accomplish these higher level goals and support the mining approach however, a fundamental and unavoidable starting point is the identification and mapping of terminology from the textual, unstructured data onto biomedical knowledge sources and concept hierarchies. In this paper, we provide a description of the work regarding terminology recognition using the Swedish MeSH (R) thesaurus and its corresponding English original source. We explain the various transformation and refinement steps applied to the original database tables into a fully-fledged processing oriented annotating resource. Particular attention has been given to a number of these steps in order to automatically map the extensive variability of lexical terms to structured MeSH (R) nodes. Issues on annotation and coverage are also discussed.
AU  - Kokkinakis, D.
AU  - European Language, Resources
AU  - Association
DA  - 2008
KW  - eppi-reviewer4
N1  - 21446546
9435
Kokkinakis, Dimitrios 6th International Conference on Language Resources and Evaluation (LREC) MAY 28-30, 2008 Marrakech, MOROCCO European Language Resources Assoc (ELRA), Evaluat & Language Resources Distribut Agcy (ELDA), Ist Linguistica Computazionale (ILC), Nuance, Inst Nederlandse Lexicologie (INL), Microsoft, European Media Lab GmBH (EML), Linguatec, Connexor, Orange, Telisma, AAMT, AMTA, ACL, AFNLP, ALTA, COCOSDA, Oriental COCOSDA, EACL, EAMT, ELSNET, EURALEX, GWA, IAMT, ISCA, KnowledgeWeb, LDC, NEMLAR Network, SIGLEX, TEI, Technolangue French Program, WRITE, Informat Soc & Media, European Commiss, Unit E 2 Content & Knowledge
PY  - 2008
SP  - 489-495
ST  - MeSH (R) - From a Controlled Vocabulary to a Processable Resource
T2  - Sixth International Conference on Language Resources and Evaluation, Lrec 2008
TI  - MeSH (R) - From a Controlled Vocabulary to a Processable Resource
UR  - <Go to ISI>://WOS:000324028900089
ID  - 26
ER  - 


TY  - JOUR
AB  - One of the requirements for a federated information system is interoperability, the ability of one computer system to access and use the resources of another system. This feature is particularly important in biomedical research systems, which need to coordinate a variety of disparate types of data. In order to meet this need, the National Cancer Institute Center for Bioinformatics (NCICB) has created the cancer Common Ontologic Representation Environment (caCORE), an interoperability infrastructure based on Model Driven Architecture. The caCORE infrastructure provides a mechanism to create interoperable biomedical information systems. Systems built using the caCORE paradigm address both aspects of interoperability: the ability to access data (syntactic interoperability) and understand the data once retrieved (semantic interoperability). This infrastructure consists of an integrated set of three major components: a controlled terminology service (Enterprise Vocabulary Services), a standards-based metadata repository (the cancer Data Standards Repository) and an information system with an Application Programming Interface (API) based on Domain Model Driven Architecture. This infrastructure is being leveraged to create a Semantic Service-Oriented Architecture (SSOA) for cancer research by the National Cancer Institute's cancer Biomedical Informatics Grid (caBIG).
AU  - Komatsoulis, G. A.
AU  - Warzel, D. B.
AU  - Hartel, F. W.
AU  - Shanbhag, K.
AU  - Chilukuri, R.
AU  - Fragoso, G.
AU  - Coronado, Sd
AU  - Reeves, D. M.
AU  - Hadfield, J. B.
AU  - Ludet, C.
AU  - Covitz, P. A.
DA  - 2008
IS  - 1
KW  - eppi-reviewer4
N1  - 21443933
6499
Komatsoulis, George A Warzel, Denise B Hartel, Francis W Shanbhag, Krishnakant Chilukuri, Ram Fragoso, Gilberto Coronado, Sherri de Reeves, Dianne M Hadfield, Jillaine B Ludet, Christophe Covitz, Peter A Z99 CA999999/Intramural NIH HHS/United States Research Support, N.I.H., Intramural United States J Biomed Inform. 2008 Feb;41(1):106-23. Epub 2007 Apr 2.
PY  - 2008
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 106-23
ST  - caCORE version 3: Implementation of a model driven, service-oriented architecture for semantic interoperability
T2  - J Biomed Inform
TI  - caCORE version 3: Implementation of a model driven, service-oriented architecture for semantic interoperability
UR  - http://ac.els-cdn.com/S1532046407000299/1-s2.0-S1532046407000299-main.pdf?_tid=22ef4762-5417-11e6-86f2-00000aab0f26&acdnat=1469637082_c7fba10a63145615c8f62da6f0efd7f1
VL  - 41
ID  - 134
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Death certificates provide an invaluable source for cancer mortality statistics; however, this value can only be realised if accurate, quantitative data can be extracted from certificates--an aim hampered by both the volume and variable nature of certificates written in natural language. This paper proposes an automatic classification system for identifying cancer related causes of death from death certificates. METHODS: Detailed features, including terms, n-grams and SNOMED CT concepts were extracted from a collection of 447,336 death certificates. These features were used to train Support Vector Machine classifiers (one classifier for each cancer type). The classifiers were deployed in a cascaded architecture: the first level identified the presence of cancer (i.e., binary cancer/nocancer) and the second level identified the type of cancer (according to the ICD-10 classification system). A held-out test set was used to evaluate the effectiveness of the classifiers according to precision, recall and F-measure. In addition, detailed feature analysis was performed to reveal the characteristics of a successful cancer classification model. RESULTS: The system was highly effective at identifying cancer as the underlying cause of death (F-measure 0.94). The system was also effective at determining the type of cancer for common cancers (F-measure 0.7). Rare cancers, for which there was little training data, were difficult to classify accurately (F-measure 0.12). Factors influencing performance were the amount of training data and certain ambiguous cancers (e.g., those in the stomach region). The feature analysis revealed a combination of features were important for cancer type classification, with SNOMED CT concept and oncology specific morphology features proving the most valuable. CONCLUSION: The system proposed in this study provides automatic identification and characterisation of cancers from large collections of free-text death certificates. This allows organisations such as Cancer Registries to monitor and report on cancer mortality in a timely and accurate manner. In addition, the methods and findings are generally applicable beyond cancer classification and to other sources of medical text besides death certificates.
AU  - Koopman, B.
AU  - Zuccon, G.
AU  - Nguyen, A.
AU  - Bergheim, A.
AU  - Grayson, N.
DA  - 2015
DO  - 10.1016/j.ijmedinf.2015.08.004. Epub 2015 Aug 13.
IS  - 11
KW  - eppi-reviewer4
N1  - 21443762
3908
PY  - 2015
SN  - 1872-8243 (Electronic) 1386-5056 (Linking)
SP  - 956-65
ST  - Automatic ICD-10 classification of cancers from free-text death certificates
T2  - Int J Med Inform
TI  - Automatic ICD-10 classification of cancers from free-text death certificates
UR  - http://ac.els-cdn.com/S1386505615300289/1-s2.0-S1386505615300289-main.pdf?_tid=25478e48-5417-11e6-ac3d-00000aacb35e&acdnat=1469637086_38b47d26b16909c5e1ef86165f2f77a0
VL  - 84
ID  - 156
ER  - 


TY  - CHAP
A2  - Holmes, J. H.
A2  - Bellazzi, R.
A2  - Sacchi, L.
A2  - Peek, N.
AB  - Electronic Medical Records (EMRs) provide a wealth of data that can be used to generate predictive models for diseases. Quite some studies have been performed that use EMRs to generate such models for specific diseases, but most of them are based on more traditional techniques used in medical domain, such as logistic regression. This paper studies the benefit of using advanced data mining techniques for Colorectal Cancer (CRC). CRC is the second most common cancer in the EU and is known to be a disease with very a-specific predictors, making it difficult to generate good predictive models. In addition, the EMR data itself has its own challenges, including the sparsity, the differences in which physicians code the data, the temporal nature of the data, and the imbalance in the data. Results show that state-of-the-art data mining techniques, including temporal data mining, are able to generate better predictive models than currently available in the literature.
AU  - Kop, R.
AU  - Hoogendoorn, M.
AU  - Moons, L. M. G.
AU  - Numans, M. E.
AU  - ten, Teije
KW  - eppi-reviewer4
N1  - 21447131
8020
Kop, Reinier Hoogendoorn, Mark Moons, Leon M. G. Numans, Mattijs E. ten Teije, Annette AIME 2015 15th Conference on Artificial Intelligence in Medicine (AIME) JUN 17-20, 2015 Univ Pavia, Pavia, ITALY
PY  - 2015
SN  - 0302-9743 978-3-319-19551-3; 978-3-319-19550-6
SP  - 133-142
ST  - On the Advantage of Using Dedicated Data Mining Techniques to Predict Colorectal Cancer
T2  - Artificial Intelligence in Medicine
TI  - On the Advantage of Using Dedicated Data Mining Techniques to Predict Colorectal Cancer
UR  - <Go to ISI>://WOS:000364534300016
VL  - 9105
ID  - 57
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Drug named entity recognition (NER) is a critical step for complex biomedical NLP tasks such as the extraction of pharmacogenomic, pharmacodynamic and pharmacokinetic parameters. Large quantities of high quality training data are almost always a prerequisite for employing supervised machine-learning techniques to achieve high classification performance. However, the human labour needed to produce and maintain such resources is a significant limitation. In this study, we improve the performance of drug NER without relying exclusively on manual annotations. METHODS: We perform drug NER using either a small gold-standard corpus (120 abstracts) or no corpus at all. In our approach, we develop a voting system to combine a number of heterogeneous models, based on dictionary knowledge, gold-standard corpora and silver annotations, to enhance performance. To improve recall, we employed genetic programming to evolve 11 regular-expression patterns that capture common drug suffixes and used them as an extra means for recognition. MATERIALS: Our approach uses a dictionary of drug names, i.e. DrugBank, a small manually annotated corpus, i.e. the pharmacokinetic corpus, and a part of the UKPMC database, as raw biomedical text. Gold-standard and silver annotated data are used to train maximum entropy and multinomial logistic regression classifiers. RESULTS: Aggregating drug NER methods, based on gold-standard annotations, dictionary knowledge and patterns, improved the performance on models trained on gold-standard annotations, only, achieving a maximum F-score of 95%. In addition, combining models trained on silver annotations, dictionary knowledge and patterns are shown to achieve comparable performance to models trained exclusively on gold-standard data. The main reason appears to be the morphological similarities shared among drug names. CONCLUSION: We conclude that gold-standard data are not a hard requirement for drug NER. Combining heterogeneous models build on dictionary knowledge can achieve similar or comparable classification performance with that of the best performing model trained on gold-standard annotations.
AU  - Korkontzelos, I.
AU  - Piliouras, D.
AU  - Dowsey, A. W.
AU  - Ananiadou, S.
DA  - 2015
DO  - 10.1016/j.artmed.2015.05.007. Epub 2015 Jun 17.
IS  - 2
KW  - eppi-reviewer4
N1  - 21443893
3983
PY  - 2015
SN  - 1873-2860 (Electronic) 0933-3657 (Linking)
SP  - 145-53
ST  - Boosting drug named entity recognition using an aggregate classifier
T2  - Artif Intell Med
TI  - Boosting drug named entity recognition using an aggregate classifier
UR  - http://ac.els-cdn.com/S0933365715000780/1-s2.0-S0933365715000780-main.pdf?_tid=28ebaa16-5417-11e6-b1da-00000aab0f01&acdnat=1469637092_69c902d50fcc5be4153869e26c96c18d
VL  - 65
ID  - 115
ER  - 


TY  - RPRT
AB  - Text mining of the biomedical literature provides patterns of relationships among concepts, people, and institutions, offering enhanced medical/technical intelligence unobtainable by other means. This report describes myriad text mining capabilities. Section 1 covers biomedical knowledge management, the role of text mining in knowledge management, and describes the cultural changes and global agreements required to allow the full power and capabilities of text mining to be utilized. The next two sections address information retrieval issues. Section 2 describes the extraction of useful information from the published biomedical literature. Section 3 describes the information content in different record fields in a major medical database. The next four sections address computational linguistics issues, especially related to identifying patterns and relationships in text. Section 4 outlines a family of methods for generating radical biomedical discovery from the literature. Section 5 shows how increasing specialization within the biomedical community creates roadblocks for the acceleration of radical discovery, and recommends ways to eliminate these roadblocks. Section 6 describes the detection of unexpected asymmetries from the biomedical literature, with a specific example on bilateral organ cancer incidence asymmetry detection. Section 7 describes a unique approach for removing words/phrases of low technical content and improving the quality of the resulting technical taxonomies. Section 8 describes the use and misuse of citation analysis in biomedical text mining. Section 9 describes citation mining. Section 10 describes the use of citation analysis to evaluate the quality of research performers. Section 11 shows a systematic approach for defining the seminal literature of any biomedical topic. Sections 12 and 13 describe the differences between highly and poorly cited biomedical articles, with specific case studies from leading medical journals.
AU  - Kostoff Ronald, N.
DA  - 2007
KW  - eppi-reviewer4
N1  - 21509558
10652
PB  - OFFICE OF NAVAL RESEARCH ARLINGTON VA
PY  - 2007
RP  - 21509558
10652
SN  - ADA473638, XB, ONR
SP  - 381
ST  - Text Mining the Biomedical Literature
TI  - Text Mining the Biomedical Literature
ID  - 58
ER  - 


TY  - JOUR
AB  - Controlled clinical trials are usually supported with an in-front data aggregation system, which supports the storage of relevant information according to the trial context within a highly structured environment. In contrast to the documentation of clinical trials, daily routine documentation has many characteristics that influence data quality. One such characteristic is the use of non-standardized text, which is an indispensable part of information representation in clinical information systems. Based on a cohort study we highlight challenges for mining electronic health records targeting free text entry fields within semi-structured data sources. Our prototypical information extraction system achieved an F-measure of 0.91 (precision=0.90, recall=0.93) for the training set and an F-measure of 0.90 (precision=0.89, recall=0.92) for the test set. We analyze the obtained results in detail and highlight challenges and future directions for the secondary use of routine data in general.
AU  - Kreuzthaler, M.
AU  - Schulz, S.
AU  - Berghold, A.
DA  - 2015
DO  - 10.1016/j.jbi.2014.10.010. Epub 2014 Nov 21.
KW  - eppi-reviewer4
N1  - 21447956
4198
PY  - 2015
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 188-95
ST  - Secondary use of electronic health records for building cohort studies through top-down information extraction
T2  - J Biomed Inform
TI  - Secondary use of electronic health records for building cohort studies through top-down information extraction
UR  - http://ac.els-cdn.com/S1532046414002329/1-s2.0-S1532046414002329-main.pdf?_tid=2c228182-5417-11e6-abfb-00000aacb360&acdnat=1469637098_a7f12e89940b35877b6c1536419d9ef5
VL  - 53
ID  - 27
ER  - 


TY  - JOUR
AB  - The use of clinical data from electronic medical records (EMRs) for clinical research and for evaluation of quality of care requires an extraction process. Many efforts have failed because the extracted data seemed to be unstructured, incomplete and ridden by errors. We have developed and tested a concept of extracting semi-structured EMRs (Journal III, Profdoc) data from 776 diabetes patients in a general practice clinic over a 5 year period. We used standard database management techniques commonly applied in clinical research in the pharmaceutical industry to clean up the data and make the data available for statistical analysis. The key problem was difficulties locating the data, as no standard way to enter the data in the EMR system was reinforced. Furthermore, no built-in edit checks to facilitate data entry were available. Laboratory, drug information and diagnostic data could be used directly while other data such as vital signs required much work to locate and become useful.
AU  - Kristianson, K. J.
AU  - Ljunggren, H.
AU  - Gustafsson, L. L.
DA  - 2009
DO  - 10.1177/1460458209345889.
IS  - 4
KW  - eppi-reviewer4
N1  - 21444496
5896
PY  - 2009
SN  - 1741-2811 (Electronic) 1460-4582 (Linking)
SP  - 305-19
ST  - Data extraction from a semi-structured electronic medical record system for outpatients: a model to facilitate the access and use of data for quality control and research
T2  - Health Informatics J
TI  - Data extraction from a semi-structured electronic medical record system for outpatients: a model to facilitate the access and use of data for quality control and research
VL  - 15
ID  - 85
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Research on publication trends in journal articles on sleep disorders (SDs) and the associated methodologies by using text mining has been limited. The present study involved text mining for terms to determine the publication trends in sleep-related journal articles published during 2000-2013 and to identify associations between SD and methodology terms as well as conducting statistical analyses of the text mining findings. METHODS: SD and methodology terms were extracted from 3,720 sleep-related journal articles in the PubMed database by using MetaMap. The extracted data set was analyzed using hierarchical cluster analyses and adjusted logistic regression models to investigate publication trends and associations between SD and methodology terms. RESULTS: MetaMap had a text mining precision, recall, and false positive rate of 0.70, 0.77, and 11.51%, respectively. The most common SD term was breathing-related sleep disorder, whereas narcolepsy was the least common. Cluster analyses showed similar methodology clusters for each SD term, except narcolepsy. The logistic regression models showed an increasing prevalence of insomnia, parasomnia, and other sleep disorders but a decreasing prevalence of breathing-related sleep disorder during 2000-2013. Different SD terms were positively associated with different methodology terms regarding research design terms, measure terms, and analysis terms. CONCLUSION: Insomnia-, parasomnia-, and other sleep disorder-related articles showed an increasing publication trend, whereas those related to breathing-related sleep disorder showed a decreasing trend. Furthermore, experimental studies more commonly focused on hypersomnia and other SDs and less commonly on insomnia, breathing-related sleep disorder, narcolepsy, and parasomnia. Thus, text mining may facilitate the exploration of the publication trends in SDs and the associated methodologies.
AU  - Lam, C.
AU  - Lai, F. C.
AU  - Wang, C. H.
AU  - Lai, M. H.
AU  - Hsu, N.
AU  - Chung, M. H.
DA  - 2016
DO  - 10.1371/journal.pone.0156031. eCollection 2016.
IS  - 5
KW  - eppi-reviewer4
N1  - 21448381
3825
PY  - 2016
SN  - 1932-6203 (Electronic) 1932-6203 (Linking)
SP  - e0156031
ST  - Text Mining of Journal Articles for Sleep Disorder Terminologies
T2  - PLoS One
TI  - Text Mining of Journal Articles for Sleep Disorder Terminologies
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4874549/pdf/pone.0156031.pdf
VL  - 11
ID  - 200
ER  - 


TY  - JOUR
AB  - The prevalence of electronic medical record (EMR) systems has made mass-screening for clinical trials viable through secondary uses of clinical data, which often exist in both structured and free text formats. The tradeoffs of using information in either data format for clinical trials screening are understudied. This paper compares the results of clinical trial eligibility queries over ICD9-encoded diagnoses and NLP-processed textual discharge summaries. The strengths and weaknesses of both data sources are summarized along the following dimensions: information completeness, expressiveness, code granularity, and accuracy of temporal information. We conclude that NLP-processed patient reports supplement important information for eligibility screening and should be used in combination with structured data.
AU  - Li, L.
AU  - Chase, H. S.
AU  - Patel, C. O.
AU  - Friedman, C.
AU  - Weng, C.
DA  - 2008
KW  - eppi-reviewer4
N1  - 21444217
6168
Li, Li Chase, Herbert S Patel, Chintan O Friedman, Carol Weng, Chunhua LM007659/LM/NLM NIH HHS/United States LM00865/LM/NLM NIH HHS/United States LM06910/LM/NLM NIH HHS/United States UL1 RR024156/RR/NCRR NIH HHS/United States Comparative Study Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2008 Nov 6:404-8.
PY  - 2008
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 404-8
ST  - Comparing ICD9-encoded diagnoses and NLP-processed discharge summaries for clinical trials pre-screening: a case study
T2  - AMIA Annu Symp Proc
TI  - Comparing ICD9-encoded diagnoses and NLP-processed discharge summaries for clinical trials pre-screening: a case study
ID  - 86
ER  - 


TY  - JOUR
AB  - Background: In this study we implemented and developed state-of-the-art machine learning (ML) and natural language processing (NLP) technologies and built a computerized algorithm for medication reconciliation. Our specific aims are: (1) to develop a computerized algorithm for medication discrepancy detection between patients' discharge prescriptions (structured data) and medications documented in free-text clinical notes (unstructured data); and (2) to assess the performance of the algorithm on real-world medication reconciliation data. Methods: We collected clinical notes and discharge prescription lists for all 271 patients enrolled in the Complex Care Medical Home Program at Cincinnati Children's Hospital Medical Center between 1/1/2010 and 12/31/2013. A double-annotated, gold-standard set of medication reconciliation data was created for this collection. We then developed a hybrid algorithm consisting of three processes: (1) a ML algorithm to identify medication entities from clinical notes, (2) a rule-based method to link medication names with their attributes, and (3) a NLP-based, hybrid approach to match medications with structured prescriptions in order to detect medication discrepancies. The performance was validated on the gold-standard medication reconciliation data, where precision (P), recall (R), F-value (F) and workload were assessed. Results: The hybrid algorithm achieved 95.0%/91.6%/93.3% of P/R/F on medication entity detection and 98.7%/99.4%/99.1% of P/R/F on attribute linkage. The medication matching achieved 92.4%/90.7%/91.5% (P/R/F) on identifying matched medications in the gold-standard and 88.6%/82.5%/85.5% (P/R/F) on discrepant medications. By combining all processes, the algorithm achieved 92.4%/90.7%/91.5% (P/R/F) and 71.5%/65.2%/68.2% (P/R/F) on identifying the matched and the discrepant medications, respectively. The error analysis on algorithm outputs identified challenges to be addressed in order to improve medication discrepancy detection. Conclusion: By leveraging ML and NLP technologies, an end-to-end, computerized algorithm achieves promising outcome in reconciling medications between clinical notes and discharge prescriptions.
AU  - Li, Q.
AU  - Spooner, S. A.
AU  - Kaiser, M.
AU  - Lingren, N.
AU  - Robbins, J.
AU  - Lingren, T.
AU  - Tang, H. X.
AU  - Solti, I.
AU  - Ni, Y. Z.
DA  - 2015
KW  - eppi-reviewer4
N1  - 21445051
7923
Li, Qi Spooner, Stephen Andrew Kaiser, Megan Lingren, Nataline Robbins, Jessica Lingren, Todd Tang, Huaxiu Solti, Imre Ni, Yizhao
PY  - 2015
SN  - 1472-6947
ST  - An end-to-end hybrid algorithm for automated medication discrepancy detection
T2  - Bmc Medical Informatics and Decision Making
TI  - An end-to-end hybrid algorithm for automated medication discrepancy detection
UR  - <Go to ISI>://WOS:000354245800001
VL  - 15
ID  - 100
ER  - 


TY  - JOUR
AB  - BACKGROUND: Typically, algorithms to classify phenotypes using electronic medical record (EMR) data were developed to perform well in a specific patient population. There is increasing interest in analyses which can allow study of a specific outcome across different diseases. Such a study in the EMR would require an algorithm that can be applied across different patient populations. Our objectives were: (1) to develop an algorithm that would enable the study of coronary artery disease (CAD) across diverse patient populations; (2) to study the impact of adding narrative data extracted using natural language processing (NLP) in the algorithm. Additionally, we demonstrate how to implement CAD algorithm to compare risk across 3 chronic diseases in a preliminary study. METHODS AND RESULTS: We studied 3 established EMR based patient cohorts: diabetes mellitus (DM, n = 65,099), inflammatory bowel disease (IBD, n = 10,974), and rheumatoid arthritis (RA, n = 4,453) from two large academic centers. We developed a CAD algorithm using NLP in addition to structured data (e.g. ICD9 codes) in the RA cohort and validated it in the DM and IBD cohorts. The CAD algorithm using NLP in addition to structured data achieved specificity >95% with a positive predictive value (PPV) 90% in the training (RA) and validation sets (IBD and DM). The addition of NLP data improved the sensitivity for all cohorts, classifying an additional 17% of CAD subjects in IBD and 10% in DM while maintaining PPV of 90%. The algorithm classified 16,488 DM (26.1%), 457 IBD (4.2%), and 245 RA (5.0%) with CAD. In a cross-sectional analysis, CAD risk was 63% lower in RA and 68% lower in IBD compared to DM (p<0.0001) after adjusting for traditional cardiovascular risk factors. CONCLUSIONS: We developed and validated a CAD algorithm that performed well across diverse patient populations. The addition of NLP into the CAD algorithm improved the sensitivity of the algorithm, particularly in cohorts where the prevalence of CAD was low. Preliminary data suggest that CAD risk was significantly lower in RA and IBD compared to DM.
AU  - Liao, K. P.
AU  - Ananthakrishnan, A. N.
AU  - Kumar, V.
AU  - Xia, Z.
AU  - Cagan, A.
AU  - Gainer, V. S.
AU  - Goryachev, S.
AU  - Chen, P.
AU  - Savova, G. K.
AU  - Agniel, D.
AU  - Churchill, S.
AU  - Lee, J.
AU  - Murphy, S. N.
AU  - Plenge, R. M.
AU  - Szolovits, P.
AU  - Kohane, I.
AU  - Shaw, S. Y.
AU  - Karlson, E. W.
AU  - Cai, T.
DA  - 2015
DO  - 10.1371/journal.pone.0136651. eCollection 2015.
IS  - 8
KW  - eppi-reviewer4
N1  - 21446596
3912
PY  - 2015
SN  - 1932-6203 (Electronic) 1932-6203 (Linking)
SP  - e0136651
ST  - Methods to Develop an Electronic Medical Record Phenotype Algorithm to Compare the Risk of Coronary Artery Disease across 3 Chronic Disease Cohorts
T2  - PLoS One
TI  - Methods to Develop an Electronic Medical Record Phenotype Algorithm to Compare the Risk of Coronary Artery Disease across 3 Chronic Disease Cohorts
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547801/pdf/pone.0136651.pdf
VL  - 10
ID  - 101
ER  - 


TY  - JOUR
AB  - OBJECTIVES: To improve the accuracy of mining structured and unstructured components of the electronic medical record (EMR) by adding temporal features to automatically identify patients with rheumatoid arthritis (RA) with methotrexate-induced liver transaminase abnormalities. MATERIALS AND METHODS: Codified information and a string-matching algorithm were applied to a RA cohort of 5903 patients from Partners HealthCare to select 1130 patients with potential liver toxicity. Supervised machine learning was applied as our key method. For features, Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) was used to extract standard vocabulary from relevant sections of the unstructured clinical narrative. Temporal features were further extracted to assess the temporal relevance of event mentions with regard to the date of transaminase abnormality. All features were encapsulated in a 3-month-long episode for classification. Results were summarized at patient level in a training set (N=480 patients) and evaluated against a test set (N=120 patients). RESULTS: The system achieved positive predictive value (PPV) 0.756, sensitivity 0.919, F1 score 0.829 on the test set, which was significantly better than the best baseline system (PPV 0.590, sensitivity 0.703, F1 score 0.642). Our innovations, which included framing the phenotype problem as an episode-level classification task, and adding temporal information, all proved highly effective. CONCLUSIONS: Automated methotrexate-induced liver toxicity phenotype discovery for patients with RA based on structured and unstructured information in the EMR shows accurate results. Our work demonstrates that adding temporal features significantly improved classification results.
AU  - Lin, C.
AU  - Karlson, E. W.
AU  - Dligach, D.
AU  - Ramirez, M. P.
AU  - Miller, T. A.
AU  - Mo, H.
AU  - Braggs, N. S.
AU  - Cagan, A.
AU  - Gainer, V.
AU  - Denny, J. C.
AU  - Savova, G. K.
DA  - 2015
DO  - 10.1136/amiajnl-2014-002642. Epub 2014 Oct 25.
IS  - e1
KW  - eppi-reviewer4
N1  - 21443763
4232
PY  - 2015
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - e151-61
ST  - Automatic identification of methotrexate-induced liver toxicity in patients with rheumatoid arthritis from the electronic medical record
T2  - J Am Med Inform Assoc
TI  - Automatic identification of methotrexate-induced liver toxicity in patients with rheumatoid arthritis from the electronic medical record
UR  - http://jamia.oxfordjournals.org/content/jaminfo/22/e1/e151.full.pdf
VL  - 22
ID  - 182
ER  - 


TY  - JOUR
AB  - BACKGROUND AND OBJECTIVE: The importance of data standards when integrating clinical research data has been recognized. The common data element (CDE) is a consensus-based data element for data harmonization and sharing between clinical researchers, it can support data standards adoption and mapping. However, the lack of a suitable methodology has become a barrier to data standard adoption. Our aim was to demonstrate an approach that allowed clinical researchers to design electronic case report forms (eCRFs) that complied with the data standard. METHODS: We used a multi-technique approach, including information retrieval, natural language processing and an ontology-based knowledgebase to facilitate data standard adoption using the eCRF design. The approach took research questions as query texts with the aim of retrieving and associating relevant CDEs with the research questions. RESULTS: The approach was implemented using a CDE-based eCRF builder, which was evaluated using CDE- related questions from CRFs used in the Parkinson Disease Biomarker Program, as well as CDE-unrelated questions from a technique support website. Our approach had a precision of 0.84, a recall of 0.80, a F-measure of 0.82 and an error of 0.31. Using the 303 testing CDE-related questions, our approach responded and provided suggested CDEs for 88.8% (269/303) of the study questions with a 90.3% accuracy (243/269). The reason for any missed and failed responses was also analyzed. CONCLUSION: This study demonstrates an approach that helps to cross the barrier that inhibits data standard adoption in eCRF building and our evaluation reveals the approach has satisfactory performance. Our CDE-based form builder provides an alternative perspective regarding data standard compliant eCRF design.
AU  - Lin, C. H.
AU  - Wu, N. Y.
AU  - Liou, D. M.
DA  - 2015
DO  - 10.1016/j.jbi.2014.08.013. Epub 2014 Sep 6.
KW  - eppi-reviewer4
N1  - 21446903
4296
PY  - 2015
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 49-57
ST  - A multi-technique approach to bridge electronic case report form design and data standard adoption
T2  - J Biomed Inform
TI  - A multi-technique approach to bridge electronic case report form design and data standard adoption
UR  - http://ac.els-cdn.com/S1532046414001944/1-s2.0-S1532046414001944-main.pdf?_tid=2f814b24-5417-11e6-9990-00000aacb361&acdnat=1469637104_8faaa4e6cd379ceaba6f4c5a60344b7b
VL  - 53
ID  - 87
ER  - 


TY  - JOUR
AB  - A semantic lexicon which associates words and phrases in text to concepts is critical for extracting and encoding clinical information in free text and therefore achieving semantic interoperability between structured and unstructured data in Electronic Health Records (EHRs). Directly using existing standard terminologies may have limited coverage with respect to concepts and their corresponding mentions in text. In this paper, we analyze how tokens and phrases in a large corpus distribute and how well the UMLS captures the semantics. A corpus-driven semantic lexicon, MedLex, has been constructed where the semantics is based on the UMLS assisted with variants mined and usage information gathered from clinical text. The detailed corpus analysis of tokens, chunks, and concept mentions shows the UMLS is an invaluable source for natural language processing. Increasing the semantic coverage of tokens provides a good foundation in capturing clinical information comprehensively. The study also yields some insights in developing practical NLP systems.
AU  - Liu, H.
AU  - Wu, S. T.
AU  - Li, D.
AU  - Jonnalagadda, S.
AU  - Sohn, S.
AU  - Wagholikar, K.
AU  - Haug, P. J.
AU  - Huff, S. M.
AU  - Chute, C. G.
DA  - 2012
KW  - eppi-reviewer4
N1  - 21448468
4928
Liu, Hongfang Wu, Stephen T Li, Dingcheng Jonnalagadda, Siddhartha Sohn, Sunghwan Wagholikar, Kavishwar Haug, Peter J Huff, Stanley M Chute, Christopher G R01 GM102282/GM/NIGMS NIH HHS/United States R01 LM009959/LM/NLM NIH HHS/United States R01LM009959A1/LM/NLM NIH HHS/United States Research Support, N.I.H., Extramural Research Support, U.S. Gov't, Non-P.H.S. United States AMIA Annu Symp Proc. 2012;2012:568-76. Epub 2012 Nov 3.
PY  - 2012
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 568-76
ST  - Towards a semantic lexicon for clinical natural language processing
T2  - AMIA Annu Symp Proc
TI  - Towards a semantic lexicon for clinical natural language processing
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540492/pdf/amia_2012_symp_0568.pdf
VL  - 2012
ID  - 125
ER  - 


TY  - JOUR
AB  - The nursing outcomes of hospitalized patients whose plans of care include death anxiety, which is a diagnosis among patients at the end-of-life, are obscure. The authors of the current article applied data mining techniques to nursing plan-of-care data for patients diagnosed with death anxiety, as defined by North American Nursing Diagnosis Association International, from four different hospitals to examine nursing care outcomes and associated factors. Results indicate that <50% of patients met the expected outcome of comfortable death. Gerontology unit patients were more likely to meet the expected outcome than patients from other unit types, although results were not statistically significant. Younger patients (i.e., age <65) had a lower chance of meeting the outcome compared with older patients (i.e., age >/=65) (chi(2)(1) = 9.266, p < 0.004). Longer stays improved the chances of meeting the outcome (chi(2)(2) = 6.47, p < 0.04). Results indicate that death anxiety outcomes are suboptimal and suggest the need to better educate clinicians about diagnosing and treating death anxiety among patients who face the end-of-life transition.
AU  - Lodhi, M. K.
AU  - Cheema, U. I.
AU  - Stifter, J.
AU  - Wilkie, D. J.
AU  - Keenan, G. M.
AU  - Yao, Y.
AU  - Ansari, R.
AU  - Khokhar, A. A.
DA  - 2014
DO  - 10.3928/19404921-20140818-01. Epub 2014 Aug 27.
IS  - 5
KW  - eppi-reviewer4
N1  - 21444584
4318
PY  - 2014
SN  - 1940-4921 (Print) 1938-2464 (Linking)
SP  - 224-34
ST  - Death anxiety in hospitalized end-of-life patients as captured from a structured electronic health record: differences by patient and nurse characteristics
T2  - Res Gerontol Nurs
TI  - Death anxiety in hospitalized end-of-life patients as captured from a structured electronic health record: differences by patient and nurse characteristics
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4697309/pdf/nihms-743252.pdf
VL  - 7
ID  - 44
ER  - 


TY  - JOUR
AB  - BACKGROUND: Technological advances in clinical informatics have made large amounts of data accessible and potentially useful for research. As a result, a burgeoning literature addresses efforts to bridge the fields of health services research and biomedical informatics. The Electronic Data Methods Forum review examines peer-reviewed literature at the intersection of comparative effectiveness research and clinical informatics. The authors are specifically interested in characterizing this literature and identifying cross-cutting themes and gaps in the literature. METHODS: A 3-step systematic literature search was conducted, including a structured search of PubMed, manual reviews of articles from selected publication lists, and manual reviews of research activities based on prospective electronic clinical data. Two thousand four hundred thirty-five citations were identified as potentially relevant. Ultimately, a full-text review was performed for 147 peer-reviewed papers. RESULTS: One hundred thirty-two articles were selected for inclusion in the review. Of these, 88 articles are the focus of the discussion in this paper. Three types of articles were identified, including papers that: (1) provide historical context or frameworks for using clinical informatics for research, (2) describe platforms and projects, and (3) discuss issues, challenges, and applications of natural language processing. In addition, 2 cross-cutting themes emerged: the challenges of conducting research in the absence of standardized ontologies and data collection; and unique data governance concerns related to the transfer, storage, deidentification, and access to electronic clinical data. Finally, the authors identified several current gaps on important topics such as the use of clinical informatics for cohort identification, cloud computing, and single point access to research data.
AU  - Lopez, M. H.
AU  - Holve, E.
AU  - Sarkar, I. N.
AU  - Segal, C.
DA  - 2012
DO  - 10.1097/MLR.0b013e318259becd.
KW  - eppi-reviewer4
N1  - 21443923
5141
PY  - 2012
SN  - 1537-1948 (Electronic) 0025-7079 (Linking)
SP  - S38-48
ST  - Building the informatics infrastructure for comparative effectiveness research (CER): a review of the literature
T2  - Med Care
TI  - Building the informatics infrastructure for comparative effectiveness research (CER): a review of the literature
UR  - http://ovidsp.tx.ovid.com/ovftpdfs/FPDDNCGCKCAOFF00/fs047/ovft/live/gv024/00005650/00005650-201207001-00011.pdf
VL  - 50 Suppl
ID  - 92
ER  - 


TY  - JOUR
AB  - To address the problem of extracting structured information from pathology reports for research purposes in the STRIDE Clinical Data Warehouse, we adapted the ChartIndex Medical Language Processing system to automatically identify and map anatomic and diagnostic noun phrases found in full-text pathology reports to SNOMED CT concept descriptors. An evaluation of the system's performance showed a positive predictive value for anatomic concepts of 92.3% and positive predictive value for diagnostic concepts of 84.4%. The experiment also suggested strategies for improving ChartIndex's performance coding pathology reports.
AU  - Lowe, H. J.
AU  - Huang, Y.
AU  - Regula, D. P.
DA  - 2009
KW  - eppi-reviewer4
N1  - 21448655
6134
Lowe, Henry J Huang, Yang Regula, Donald P United States AMIA Annu Symp Proc. 2009 Nov 14;2009:386-90.
PY  - 2009
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 386-90
ST  - Using a statistical natural language Parser augmented with the UMLS specialist lexicon to assign SNOMED CT codes to anatomic sites and pathologic diagnoses in full text pathology reports
T2  - AMIA Annu Symp Proc
TI  - Using a statistical natural language Parser augmented with the UMLS specialist lexicon to assign SNOMED CT codes to anatomic sites and pathologic diagnoses in full text pathology reports
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815395/pdf/amia-f2009-386.pdf
VL  - 2009
ID  - 102
ER  - 


TY  - JOUR
AB  - Manual curation of data from the biomedical literature is a rate-limiting factor for many expert curated databases. Despite the continuing advances in biomedical text mining and the pressing needs of biocurators for better tools, few existing text-mining tools have been successfully integrated into production literature curation systems such as those used by the expert curated databases. To close this gap and better understand all aspects of literature curation, we invited submissions of written descriptions of curation workflows from expert curated databases for the BioCreative 2012 Workshop Track II. We received seven qualified contributions, primarily from model organism databases. Based on these descriptions, we identified commonalities and differences across the workflows, the common ontologies and controlled vocabularies used and the current and desired uses of text mining for biocuration. Compared to a survey done in 2009, our 2012 results show that many more databases are now using text mining in parts of their curation workflows. In addition, the workshop participants identified text-mining aids for finding gene names and symbols (gene indexing), prioritization of documents for curation (document triage) and ontology concept assignment as those most desired by the biocurators. DATABASE URL: http://www.biocreative.org/tasks/bc-workshop-2012/workflow/.
AU  - Lu, Z.
AU  - Hirschman, L.
DA  - 2012
DO  - 10.1093/database/bas043. Print 2012.
KW  - eppi-reviewer4
N1  - 21443853
5005
PY  - 2012
SN  - 1758-0463 (Electronic) 1758-0463 (Linking)
SP  - bas043
ST  - Biocuration workflows and text mining: overview of the BioCreative 2012 Workshop Track II
T2  - Database (Oxford)
TI  - Biocuration workflows and text mining: overview of the BioCreative 2012 Workshop Track II
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3500522/pdf/bas043.pdf
VL  - 2012
ID  - 103
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To identify Common Data Elements (CDEs) in eligibility criteria of multiple clinical trials studying the same disease using a human-computer collaborative approach. DESIGN: A set of free-text eligibility criteria from clinical trials on two representative diseases, breast cancer and cardiovascular diseases, was sampled to identify disease-specific eligibility criteria CDEs. In this proposed approach, a semantic annotator is used to recognize Unified Medical Language Systems (UMLSs) terms within the eligibility criteria text. The Apriori algorithm is applied to mine frequent disease-specific UMLS terms, which are then filtered by a list of preferred UMLS semantic types, grouped by similarity based on the Dice coefficient, and, finally, manually reviewed. MEASUREMENTS: Standard precision, recall, and F-score of the CDEs recommended by the proposed approach were measured with respect to manually identified CDEs. RESULTS: Average precision and recall of the recommended CDEs for the two diseases were 0.823 and 0.797, respectively, leading to an average F-score of 0.810. In addition, the machine-powered CDEs covered 80% of the cardiovascular CDEs published by The American Heart Association and assigned by human experts. CONCLUSION: It is feasible and effort saving to use a human-computer collaborative approach to augment domain experts for identifying disease-specific CDEs from free-text clinical trial eligibility criteria.
AU  - Luo, Z.
AU  - Miotto, R.
AU  - Weng, C.
DA  - 2013
DO  - 10.1016/j.jbi.2012.07.006. Epub 2012 Jul 27.
IS  - 1
KW  - eppi-reviewer4
N1  - 21445805
5104
PY  - 2013
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 33-9
ST  - A human-computer collaborative approach to identifying common data elements in clinical trial eligibility criteria
T2  - J Biomed Inform
TI  - A human-computer collaborative approach to identifying common data elements in clinical trial eligibility criteria
UR  - http://ac.els-cdn.com/S1532046412001037/1-s2.0-S1532046412001037-main.pdf?_tid=38259cee-5417-11e6-8164-00000aab0f6c&acdnat=1469637118_eb57ced7aa63bb019e48b2b336108aa1
VL  - 46
ID  - 116
ER  - 


TY  - CHAP
A2  - Ardil, C.
AB  - This paper presents a system for discovering association rules from collections of unstructured documents called EART (Extract Association Rules from Text). The EART system treats texts only not images or figures. EART discovers association rules amongst keywords labeling the collection of textual documents. The main characteristic of EART is that the system integrates XML technology (to transform unstructured documents into structured documents) with Information Retrieval scheme (TF-IDF) and Data Mining technique for association rules extraction. EART depends on word feature to extract association rules. It consists of four phases: structure phase, index phase, text mining phase and visualization phase. Our work depends on the analysis of the keywords in the extracted association rules through the co-occurrence of the keywords in one sentence in the original text and the existing of the keywords in one sentence without co-occurrence. Experiments applied on a collection of scientific documents selected from MEDLINE that are related to the outbreak of H5N1 avian influenza virus.
AU  - Mahgoub, H.
KW  - eppi-reviewer4
N1  - 21446625
10286
Mahgoub, Hany Conference of the World-Academy-of-Science-Engineering-and-Technology AUG 25-27, 2006 Prague, CZECH REPUBLIC World Acad Sci Engn & Technol
PY  - 2006
SN  - 1307-6884
SP  - 167-172
ST  - Mining Association Rules from Unstructured Documents
T2  - Proceedings of World Academy of Science, Engineering and Technology, Vol 14
TI  - Mining Association Rules from Unstructured Documents
UR  - <Go to ISI>://WOS:000259632400033
VL  - 14
ID  - 117
ER  - 


TY  - JOUR
AB  - BACKGROUND: To enhance our understanding of complex biological systems like diseases we need to put all of the available data into context and use this to detect relations, pattern and rules which allow predictive hypotheses to be defined. Life science has become a data rich science with information about the behaviour of millions of entities like genes, chemical compounds, diseases, cell types and organs, which are organised in many different databases and/or spread throughout the literature. Existing knowledge such as genotype-phenotype relations or signal transduction pathways must be semantically integrated and dynamically organised into structured networks that are connected with clinical and experimental data. Different approaches to this challenge exist but so far none has proven entirely satisfactory. RESULTS: To address this challenge we previously developed a generic knowledge management framework, BioXM, which allows the dynamic, graphic generation of domain specific knowledge representation models based on specific objects and their relations supporting annotations and ontologies. Here we demonstrate the utility of BioXM for knowledge management in systems biology as part of the EU FP6 BioBridge project on translational approaches to chronic diseases. From clinical and experimental data, text-mining results and public databases we generate a chronic obstructive pulmonary disease (COPD) knowledge base and demonstrate its use by mining specific molecular networks together with integrated clinical and experimental data. CONCLUSIONS: We generate the first semantically integrated COPD specific public knowledge base and find that for the integration of clinical and experimental data with pre-existing knowledge the configuration based set-up enabled by BioXM reduced implementation time and effort for the knowledge base compared to similar systems implemented as classical software development projects. The knowledgebase enables the retrieval of sub-networks including protein-protein interaction, pathway, gene--disease and gene--compound data which are used for subsequent data analysis, modelling and simulation. Pre-structured queries and reports enhance usability; establishing their use in everyday clinical settings requires further simplification with a browser based interface which is currently under development.
AU  - Maier, D.
AU  - Kalus, W.
AU  - Wolff, M.
AU  - Kalko, S. G.
AU  - Roca, J.
AU  - Marin de, Mas
AU  - Turan, N.
AU  - Cascante, M.
AU  - Falciani, F.
AU  - Hernandez, M.
AU  - Villa-Freixa, J.
AU  - Losko, S.
DA  - 2011
DO  - 10.1186/1752-0509-5-38.
KW  - eppi-reviewer4
N1  - 21446279
5547
PY  - 2011
SN  - 1752-0509 (Electronic) 1752-0509 (Linking)
SP  - 38
ST  - Knowledge management for systems biology a general and visually driven framework applied to translational medicine
T2  - BMC Syst Biol
TI  - Knowledge management for systems biology a general and visually driven framework applied to translational medicine
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3060864/pdf/1752-0509-5-38.pdf
VL  - 5
ID  - 158
ER  - 


TY  - JOUR
AB  - Gene ontology (GO) annotation is a common task among model organism databases (MODs) for capturing gene function data from journal articles. It is a time-consuming and labor-intensive task, and is thus often considered as one of the bottlenecks in literature curation. There is a growing need for semiautomated or fully automated GO curation techniques that will help database curators to rapidly and accurately identify gene function information in full-length articles. Despite multiple attempts in the past, few studies have proven to be useful with regard to assisting real-world GO curation. The shortage of sentence-level training data and opportunities for interaction between text-mining developers and GO curators has limited the advances in algorithm development and corresponding use in practical circumstances. To this end, we organized a text-mining challenge task for literature-based GO annotation in BioCreative IV. More specifically, we developed two subtasks: (i) to automatically locate text passages that contain GO-relevant information (a text retrieval task) and (ii) to automatically identify relevant GO terms for the genes in a given article (a concept-recognition task). With the support from five MODs, we provided teams with >4000 unique text passages that served as the basis for each GO annotation in our task data. Such evidence text information has long been recognized as critical for text-mining algorithm development but was never made available because of the high cost of curation. In total, seven teams participated in the challenge task. From the team results, we conclude that the state of the art in automatically mining GO terms from literature has improved over the past decade while much progress is still needed for computer-assisted GO curation. Future work should focus on addressing remaining technical challenges for improved performance of automatic GO concept recognition and incorporating practical benefits of text-mining tools into real-world GO annotation. DATABASE URL: http://www.biocreative.org/tasks/biocreative-iv/track-4-GO/.
AU  - Mao, Y.
AU  - Van, Auken
AU  - Li, D.
AU  - Arighi, C. N.
AU  - McQuilton, P.
AU  - Hayman, G. T.
AU  - Tweedie, S.
AU  - Schaeffer, M. L.
AU  - Laulederkind, S. J.
AU  - Wang, S. J.
AU  - Gobeill, J.
AU  - Ruch, P.
AU  - Luu, A. T.
AU  - Kim, J. J.
AU  - Chiang, J. H.
AU  - Chen, Y. D.
AU  - Yang, C. J.
AU  - Liu, H.
AU  - Zhu, D.
AU  - Li, Y.
AU  - Yu, H.
AU  - Emadzadeh, E.
AU  - Gonzalez, G.
AU  - Chen, J. M.
AU  - Dai, H. J.
AU  - Lu, Z.
DA  - 2014
DO  - 10.1093/database/bau086. Print 2014.
KW  - eppi-reviewer4
N1  - 21447229
4319
PY  - 2014
SN  - 1758-0463 (Electronic) 1758-0463 (Linking)
ST  - Overview of the gene ontology task at BioCreative IV
T2  - Database (Oxford)
TI  - Overview of the gene ontology task at BioCreative IV
VL  - 2014
ID  - 146
ER  - 


TY  - JOUR
AB  - BACKGROUND: Invasive fungal diseases (IFDs) are associated with considerable health and economic costs. Surveillance of the more diagnostically challenging invasive fungal diseases, specifically of the sino-pulmonary system, is not feasible for many hospitals because case finding is a costly and labour intensive exercise. We developed text classifiers for detecting such IFDs from free-text radiology (CT) reports, using machine-learning techniques. METHOD: We obtained free-text reports of CT scans performed over a specific hospitalisation period (2003-2011), for 264 IFD and 289 control patients from three tertiary hospitals. We analysed IFD evidence at patient, report, and sentence levels. Three infectious disease experts annotated the reports of 73 IFD-positive patients for language suggestive of IFD at sentence level, and graded the sentences as to whether they suggested or excluded the presence of IFD. Reliable agreement between annotators was obtained and this was used as training data for our classifiers. We tested a variety of Machine Learning (ML), rule based, and hybrid systems, with feature types including bags of words, bags of phrases, and bags of concepts, as well as report-level structured features. Evaluation was carried out over a robust framework with separate Development and Held-Out datasets. RESULTS: The best systems (using Support Vector Machines) achieved very high recall at report- and patient-levels over unseen data: 95% and 100% respectively. Precision at report-level over held-out data was 71%; however, most of the associated false-positive reports (53%) belonged to patients who had a previous positive report appropriately flagged by the classifier, reducing negative impact in practice. CONCLUSIONS: Our machine learning application holds the potential for developing systematic IFD surveillance systems for hospital populations.
AU  - Martinez, D.
AU  - Ananda-Rajah, M. R.
AU  - Suominen, H.
AU  - Slavin, M. A.
AU  - Thursky, K. A.
AU  - Cavedon, L.
DA  - 2015
DO  - 10.1016/j.jbi.2014.11.009. Epub 2014 Nov 24.
KW  - eppi-reviewer4
N1  - 21443753
4194
PY  - 2015
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 251-60
ST  - Automatic detection of patients with invasive fungal disease from free-text computed tomography (CT) scans
T2  - J Biomed Inform
TI  - Automatic detection of patients with invasive fungal disease from free-text computed tomography (CT) scans
UR  - http://ac.els-cdn.com/S1532046414002391/1-s2.0-S1532046414002391-main.pdf?_tid=3b723042-5417-11e6-a9db-00000aacb361&acdnat=1469637124_ab49f88c42fdf20f2525088378c3381e
VL  - 53
ID  - 104
ER  - 


TY  - JOUR
AB  - OBJECTIVE: We address the task of extracting information from free-text pathology reports, focusing on staging information encoded by the TNM (tumour-node-metastases) and ACPS (Australian clinico-pathological stage) systems. Staging information is critical for diagnosing the extent of cancer in a patient and for planning individualised treatment. Extracting such information into more structured form saves time, improves reporting, and underpins the potential for automated decision support. METHODS AND MATERIAL: We investigate the portability of a text mining model constructed from records from one health centre, by applying it directly to the extraction task over a set of records from a different health centre, with different reporting narrative characteristics. Other than a simple normalisation step on features associated with target labels, we apply the models from one system directly to the other. RESULTS: The best F-scores for in-hospital experiments are 81%, 85%, and 94% (for staging T, N, and M respectively), while best cross-hospital F-scores reach 84%, 81%, and 91% for the same respective categories. CONCLUSIONS: Our performance results compare favourably to the best levels reported in the literature, and--most relevant to our aim here--the cross-corpus results demonstrate the portability of the models we developed.
AU  - Martinez, D.
AU  - Pitson, G.
AU  - MacKinlay, A.
AU  - Cavedon, L.
DA  - 2014
DO  - 10.1016/j.artmed.2014.06.002. Epub 2014 Jun 21.
IS  - 1
KW  - eppi-reviewer4
N1  - 21444451
4389
PY  - 2014
SN  - 1873-2860 (Electronic) 0933-3657 (Linking)
SP  - 11-21
ST  - Cross-hospital portability of information extraction of cancer staging information
T2  - Artif Intell Med
TI  - Cross-hospital portability of information extraction of cancer staging information
UR  - http://ac.els-cdn.com/S0933365714000669/1-s2.0-S0933365714000669-main.pdf?_tid=3eaf6824-5417-11e6-8d4d-00000aacb360&acdnat=1469637129_0dc18ab360bc9df5edc4a56626df63fa
VL  - 62
ID  - 194
ER  - 


TY  - JOUR
AB  - Accessing both structured and unstructured clinical data is a high priority for research efforts. However, HIPAA requires that data meet or exceed a deidentification standard to assure that protected health information (PHI) is removed. This is a particularly difficult problem in the case of unstructured clinical free text and natural language processing (NLP) systems can be trained to automatically de-identify clinical text. Moreover, manual human annotation of clinical note documents for the purpose of building reference standards to evaluate NLP systems is a costly and time consuming process. Annotation schema must be created that can be used to build reliable and valid reference standards to evaluate NLP systems for the deidentification task. We describe the inductive creation of an annotation schema and subsequent reference standard. We also provide estimates of the accuracy of human annotators for this particular task.
AU  - Mayer, J.
AU  - Shen, S.
AU  - South, B. R.
AU  - Meystre, S.
AU  - Friedlin, F. J.
AU  - Ray, W. R.
AU  - Samore, M.
DA  - 2009
KW  - eppi-reviewer4
N1  - 21446038
6133
Mayer, Jeanmarie Shen, Shuying South, Brett R Meystre, Stephane Friedlin, F Jeff Ray, William R Samore, Matthew United States AMIA Annu Symp Proc. 2009 Nov 14;2009:416-20.
PY  - 2009
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 416-20
ST  - Inductive creation of an annotation schema and a reference standard for de-identification of VA electronic clinical notes
T2  - AMIA Annu Symp Proc
TI  - Inductive creation of an annotation schema and a reference standard for de-identification of VA electronic clinical notes
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815367/pdf/amia-f2009-416.pdf
VL  - 2009
ID  - 105
ER  - 


TY  - JOUR
AB  - BACKGROUND: We explore techniques for performing model combination between the UMass and Stanford biomedical event extraction systems. Both sub-components address event extraction as a structured prediction problem, and use dual decomposition (UMass) and parsing algorithms (Stanford) to find the best scoring event structure. Our primary focus is on stacking where the predictions from the Stanford system are used as features in the UMass system. For comparison, we look at simpler model combination techniques such as intersection and union which require only the outputs from each system and combine them directly. RESULTS: First, we find that stacking substantially improves performance while intersection and union provide no significant benefits. Second, we investigate the graph properties of event structures and their impact on the combination of our systems. Finally, we trace the origins of events proposed by the stacked model to determine the role each system plays in different components of the output. We learn that, while stacking can propose novel event structures not seen in either base model, these events have extremely low precision. Removing these novel events improves our already state-of-the-art F1 to 56.6% on the test set of Genia (Task 1). Overall, the combined system formed via stacking ("FAUST") performed well in the BioNLP 2011 shared task. The FAUST system obtained 1st place in three out of four tasks: 1st place in Genia Task 1 (56.0% F1) and Task 2 (53.9%), 2nd place in the Epigenetics and Post-translational Modifications track (35.0%), and 1st place in the Infectious Diseases track (55.6%). CONCLUSION: We present a state-of-the-art event extraction system that relies on the strengths of structured prediction and model combination through stacking. Akin to results on other tasks, stacking outperforms intersection and union and leads to very strong results. The utility of model combination hinges on complementary views of the data, and we show that our sub-systems capture different graph properties of event structures. Finally, by removing low precision novel events, we show that performance from stacking can be further improved.
AU  - McClosky, D.
AU  - Riedel, S.
AU  - Surdeanu, M.
AU  - McCallum, A.
AU  - Manning, C. D.
DA  - 2012
DO  - 10.1186/1471-2105-13-S11-S9.
KW  - eppi-reviewer4
N1  - 21444142
5123
PY  - 2012
SN  - 1471-2105 (Electronic) 1471-2105 (Linking)
SP  - S9
ST  - Combining joint models for biomedical event extraction
T2  - BMC Bioinformatics
TI  - Combining joint models for biomedical event extraction
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3395172/pdf/1471-2105-13-S11-S9.pdf
VL  - 13 Suppl 11
ID  - 183
ER  - 


TY  - JOUR
AB  - BACKGROUND: Near universal administration of vaccines mandates intense pharmacovigilance for vaccine safety and a stringently low tolerance for adverse events. Reports of autoimmune diseases (AID) following vaccination have been challenging to evaluate given the high rates of vaccination, background incidence of autoimmunity, and low incidence and variable times for onset of AID after vaccinations. In order to identify biologically plausible pathways to adverse autoimmune events of vaccine-related AID, we used a systems biology approach to create a matrix of innate and adaptive immune mechanisms active in specific diseases, responses to vaccine antigens, adjuvants, preservatives and stabilizers, for the most common vaccine-associated AID found in the Vaccine Adverse Event Reporting System. RESULTS: This report focuses on Guillain-Barre Syndrome (GBS), Rheumatoid Arthritis (RA), Systemic Lupus Erythematosus (SLE), and Idiopathic (or immune) Thrombocytopenic Purpura (ITP). Multiple curated databases and automated text mining of PubMed literature identified 667 genes associated with RA, 448 with SLE, 49 with ITP and 73 with GBS. While all data sources provided valuable and unique gene associations, text mining using natural language processing (NLP) algorithms provided the most information but required curation to remove incorrect associations. Six genes were associated with all four AIDs. Thirty-three pathways were shared by the four AIDs. Classification of genes into twelve immune system related categories identified more "Th17 T-cell subtype" genes in RA than the other AIDs, and more "Chemokine plus Receptors" genes associated with RA than SLE. Gene networks were visualized and clustered into interconnected modules with specific gene clusters for each AID, including one in RA with ten C-X-C motif chemokines. The intersection of genes associated with GBS, GBS peptide auto-antigens, influenza A infection, and influenza vaccination created a subnetwork of genes that inferred a possible role for the MAPK signaling pathway in influenza vaccine related GBS. CONCLUSIONS: Results showing unique and common gene sets, pathways, immune system categories and functional clusters of genes in four autoimmune diseases suggest it is possible to develop molecular classifications of autoimmune and inflammatory events. Combining this information with cellular and other disease responses should greatly aid in the assessment of potential immune-mediated adverse events following vaccination.
AU  - McGarvey, P. B.
AU  - Suzek, B. E.
AU  - Baraniuk, J. N.
AU  - Rao, S.
AU  - Conkright, B.
AU  - Lababidi, S.
AU  - Sutherland, A.
AU  - Forshee, R.
AU  - Madhavan, S.
DA  - 2014
DO  - 10.1186/s12865-014-0061-0.
KW  - eppi-reviewer4
N1  - 21445990
4180
PY  - 2014
SN  - 1471-2172 (Electronic) 1471-2172 (Linking)
SP  - 61
ST  - In silico analysis of autoimmune diseases and genetic relationships to vaccination against infectious diseases
T2  - BMC Immunol
TI  - In silico analysis of autoimmune diseases and genetic relationships to vaccination against infectious diseases
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4266212/pdf/12865_2014_Article_61.pdf
VL  - 15
ID  - 159
ER  - 


TY  - JOUR
AB  - The Munich Information Center for Protein Sequences (MIPS at the Helmholtz Center for Environmental Health, Neuherberg, Germany) has many years of experience in providing annotated collections of biological data. Selected data sets of high relevance, such as model genomes, are subjected to careful manual curation, while the bulk of high-throughput data is annotated by automatic means. High-quality reference resources developed in the past and still actively maintained include Saccharomyces cerevisiae, Neurospora crassa and Arabidopsis thaliana genome databases as well as several protein interaction data sets (MPACT, MPPI and CORUM). More recent projects are PhenomiR, the database on microRNA-related phenotypes, and MIPS PlantsDB for integrative and comparative plant genome research. The interlinked resources SIMAP and PEDANT provide homology relationships as well as up-to-date and consistent annotation for 38,000,000 protein sequences. PPLIPS and CCancer are versatile tools for proteomics and functional genomics interfacing to a database of compilations from gene lists extracted from literature. A novel literature-mining tool, EXCERBT, gives access to structured information on classified relations between genes, proteins, phenotypes and diseases extracted from Medline abstracts by semantic analysis. All databases described here, as well as the detailed descriptions of our projects can be accessed through the MIPS WWW server (http://mips.helmholtz-muenchen.de).
AU  - Mewes, H. W.
AU  - Ruepp, A.
AU  - Theis, F.
AU  - Rattei, T.
AU  - Walter, M.
AU  - Frishman, D.
AU  - Suhre, K.
AU  - Spannagl, M.
AU  - Mayer, K. F.
AU  - Stumpflen, V.
AU  - Antonov, A.
DA  - 2011
DO  - 10.1093/nar/gkq1157. Epub 2010 Nov 24.
IS  - Database issue
KW  - eppi-reviewer4
N1  - 21446729
5612
PY  - 2011
SN  - 1362-4962 (Electronic) 0305-1048 (Linking)
SP  - D220-4
ST  - MIPS: curated databases and comprehensive secondary data resources in 2010
T2  - Nucleic Acids Res
TI  - MIPS: curated databases and comprehensive secondary data resources in 2010
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3013725/pdf/gkq1157.pdf
VL  - 39
ID  - 59
ER  - 


TY  - JOUR
AB  - Decision strategies in dynamic environments do not always succeed in producing desired outcomes, particularly in complex, ill-structured domains. Information systems often capture large amounts of data about such environments. We propose a domain-independent, iterative approach that (a) applies data mining classification techniques to the collected data in order to discover the conditions under which dynamic decision-making strategies produce undesired or suboptimal outcomes and (b) uses this information to improve the decision strategy under these conditions. In this paper, we formally develop this approach and illustrate it by providing detailed examples of its application to a chronic disease care problem in a healthcare management organization, specifically the treatment of patients with type 2 diabetes mellitus. In particular, the proposed iterative approach is used to improve treatment strategies by predicting and eliminating treatment failures, i.e., insufficient or excessive treatment actions, based on information that is available in electronic medical record systems. We also apply the proposed approach to a manufacturing task, resulting in substantial decision strategy improvements, which further demonstrates the generality and flexibility of the proposed approach.
AU  - Meyer, G.
AU  - Adomavicius, G.
AU  - Johnson, P. E.
AU  - Elidrisi, M.
AU  - Rush, W. A.
AU  - Sperl-Hillen, J. M.
AU  - O'Connor, P. J.
DA  - 2014
IS  - 2
KW  - eppi-reviewer4
N1  - 21446434
8200
Meyer, Georg Adomavicius, Gediminas Johnson, Paul E. Elidrisi, Mohamed Rush, William A. Sperl-Hillen, JoAnn M. O'Connor, Patrick J.
PY  - 2014
SN  - 1047-7047
SP  - 239-263
ST  - A Machine Learning Approach to Improving Dynamic Decision Making
T2  - Information Systems Research
TI  - A Machine Learning Approach to Improving Dynamic Decision Making
UR  - <Go to ISI>://WOS:000338701400003
VL  - 25
ID  - 13
ER  - 


TY  - JOUR
AB  - OBJECTIVES: To summarize current excellent research in the field of patient records. METHOD: Synopsis of the papers selected for the IMIA Yearbook 2007. RESULTS: The Electronic Patient Record encompasses a broad field of research and development. Some current research topics were selected for this IMIA Yearbook: EHR representation and communication standards, and secondary uses of clinical data for research and decision support. Four excellent papers representing the research in those fields were selected for the Patient Records section. CONCLUSION: The best papers selected for this section focus on the analysis and comparison of two important clinical documents representation standards, on direct structured data entry, on the use of Natural Language Processing to detect adverse events, and on the development and evaluation of a clinical text corpus annotated for part-of-speech information.
AU  - Meystre, S.
DA  - 2007
KW  - eppi-reviewer4
N1  - 21445010
6385
Meystre, S Germany Yearb Med Inform. 2007:47-9.
PY  - 2007
SN  - 0943-4747 (Print) 0943-4747 (Linking)
SP  - 47-9
ST  - Electronic patient records: some answers to the data representation and reuse challenges. Findings from the section on Patient Records
T2  - Yearb Med Inform
TI  - Electronic patient records: some answers to the data representation and reuse challenges. Findings from the section on Patient Records
ID  - 164
ER  - 


TY  - JOUR
AB  - To evaluate the i2b2 Hive as a tool to query, visualize, and extract clinical data, we selected a use case from the i2b2 airways diseases driving biology project: asthma exacerbations prediction. We analyzed the cohort selection and the extraction of the clinical data used by this asthma exacerbations prediction study. The structured data included the asthma diagnosis, birthdate, age, race, sex, height, weight, and BMI. The smoking status is typically only mentioned in clinical notes, and we evaluated the Natural Language Processing (NLP) application embedded in the i2b2 NLP cell to extract the smoking status from history and physical exam reports.Querying structured data was possible with the i2b2 workbench for about half the clinical data elements. The remaining had to be queried using a commercial database management system client. The automated extraction of the smoking status reached a mean precision of 0.79 and a mean specificity of 0.90.
AU  - Meystre, S. M.
AU  - Deshmukh, V. G.
AU  - Mitchell, J.
DA  - 2009
KW  - eppi-reviewer4
N1  - 21444074
6132
Meystre, Stephane M Deshmukh, Vikrant G Mitchell, Joyce C06-RR11234/RR/NCRR NIH HHS/United States U54 LM008748/LM/NLM NIH HHS/United States UL1-RR025764/RR/NCRR NIH HHS/United States Evaluation Studies Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2009 Nov 14;2009:442-6.
PY  - 2009
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 442-6
ST  - A clinical use case to evaluate the i2b2 Hive: predicting asthma exacerbations
T2  - AMIA Annu Symp Proc
TI  - A clinical use case to evaluate the i2b2 Hive: predicting asthma exacerbations
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815458/pdf/amia-f2009-442.pdf
VL  - 2009
ID  - 160
ER  - 


TY  - JOUR
AB  - Biomedical Natural Language Processing (BioNLP) attempts to capture biomedical phenomena from texts by extracting relations between biomedical entities (i.e. proteins and genes). Traditionally, only binary relations have been extracted from large numbers of published papers. Recently, more complex relations (biomolecular events) have also been extracted. Such events may include several entities or other relations. To evaluate the performance of the text mining systems, several shared task challenges have been arranged for the BioNLP community. With a common and consistent task setting, the BioNLP'09 shared task evaluated complex biomolecular events such as binding and regulation.Finding these events automatically is important in order to improve biomedical event extraction systems. In the present paper, we propose an automatic event extraction system, which contains a model for complex events, by solving a classification problem with rich features. The main contributions of the present paper are: (1) the proposal of an effective bio-event detection method using machine learning, (2) provision of a high-performance event extraction system, and (3) the execution of a quantitative error analysis. The proposed complex (binding and regulation) event detector outperforms the best system from the BioNLP'09 shared task challenge.
AU  - Miwa, M.
AU  - Saetre, R.
AU  - Kim, J. D.
AU  - Tsujii, J.
DA  - 2010
IS  - 1
KW  - eppi-reviewer4
N1  - 21445185
5830
Miwa, Makoto Saetre, Rune Kim, Jin-Dong Tsujii, Jun'ichi Evaluation Studies Research Support, Non-U.S. Gov't England J Bioinform Comput Biol. 2010 Feb;8(1):131-46.
PY  - 2010
SN  - 1757-6334 (Electronic) 0219-7200 (Linking)
SP  - 131-46
ST  - Event extraction with complex event classification using rich features
T2  - J Bioinform Comput Biol
TI  - Event extraction with complex event classification using rich features
VL  - 8
ID  - 210
ER  - 


TY  - JOUR
AB  - BACKGROUND: Electronic health records (EHRs) are increasingly used for clinical and translational research through the creation of phenotype algorithms. Currently, phenotype algorithms are most commonly represented as noncomputable descriptive documents and knowledge artifacts that detail the protocols for querying diagnoses, symptoms, procedures, medications, and/or text-driven medical concepts, and are primarily meant for human comprehension. We present desiderata for developing a computable phenotype representation model (PheRM). METHODS: A team of clinicians and informaticians reviewed common features for multisite phenotype algorithms published in PheKB.org and existing phenotype representation platforms. We also evaluated well-known diagnostic criteria and clinical decision-making guidelines to encompass a broader category of algorithms. RESULTS: We propose 10 desired characteristics for a flexible, computable PheRM: (1) structure clinical data into queryable forms; (2) recommend use of a common data model, but also support customization for the variability and availability of EHR data among sites; (3) support both human-readable and computable representations of phenotype algorithms; (4) implement set operations and relational algebra for modeling phenotype algorithms; (5) represent phenotype criteria with structured rules; (6) support defining temporal relations between events; (7) use standardized terminologies and ontologies, and facilitate reuse of value sets; (8) define representations for text searching and natural language processing; (9) provide interfaces for external software algorithms; and (10) maintain backward compatibility. CONCLUSION: A computable PheRM is needed for true phenotype portability and reliability across different EHR products and healthcare systems. These desiderata are a guide to inform the establishment and evolution of EHR phenotype algorithm authoring platforms and languages.
AU  - Mo, H.
AU  - Thompson, W. K.
AU  - Rasmussen, L. V.
AU  - Pacheco, J. A.
AU  - Jiang, G.
AU  - Kiefer, R.
AU  - Zhu, Q.
AU  - Xu, J.
AU  - Montague, E.
AU  - Carrell, D. S.
AU  - Lingren, T.
AU  - Mentch, F. D.
AU  - Ni, Y.
AU  - Wehbe, F. H.
AU  - Peissig, P. L.
AU  - Tromp, G.
AU  - Larson, E. B.
AU  - Chute, C. G.
AU  - Pathak, J.
AU  - Denny, J. C.
AU  - Speltz, P.
AU  - Kho, A. N.
AU  - Jarvik, G. P.
AU  - Bejan, C. A.
AU  - Williams, M. S.
AU  - Borthwick, K.
AU  - Kitchner, T. E.
AU  - Roden, D. M.
AU  - Harris, P. A.
DA  - 2015
DO  - 10.1093/jamia/ocv112. Epub 2015 Sep 5.
IS  - 6
KW  - eppi-reviewer4
N1  - 21444626
3905
PY  - 2015
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 1220-30
ST  - Desiderata for computable representations of electronic health records-driven phenotype algorithms
T2  - J Am Med Inform Assoc
TI  - Desiderata for computable representations of electronic health records-driven phenotype algorithms
UR  - http://jamia.oxfordjournals.org/content/jaminfo/22/6/1220.full.pdf
VL  - 22
ID  - 184
ER  - 


TY  - JOUR
AB  - Increasing understanding of how to categorize patient symptoms for efficient diagnosis has led to structured patient interviews and diagnostic flowcharts that can provide diagnostic accuracy and save valuable physician time. But the rigidity of predefined questions and controlled vocabulary for answers can leave patients feeling over-constrained, like the doctor (or computer system) is not really listening to them. In addition, not hearing the patient's own words can lead to the physician overlooking subtle details that are diagnostically relevant. How can we reconcile the need for patients to express themselves with the doctor's need to understand the patient's experience in medically appropriate terms? We present I'm Listening, a system for automatically conducting patient pre-visit interviews. It does not replace a human doctor, but can be used before an office visit to elicit complaint details. This information can be used to triage care and prepare patients for visits with educational materials and appropriate tests, making better use of both doctor and patient time. It uses an on-screen avatar and natural language processing to (partially) understand the patient's response. Key is a Commonsense reasoning system that lets patients express themselves in unconstrained natural language, even using metaphor, and that maps the language to medically relevant categories. For example, if a patient describes his or her pain like, "someone sticking in a knife and then turning it", the system could categorize it as sharp, intense, and localized.
AU  - Moore, J.
AU  - Lieberman, H.
DA  - 2009
KW  - eppi-reviewer4
N1  - 21448343
5959
Moore, John Lieberman, Henry Netherlands Stud Health Technol Inform. 2009;149:130-9.
PY  - 2009
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 130-9
ST  - Talking about painful subjects: flexibility and constraints in patient interviews
T2  - Stud Health Technol Inform
TI  - Talking about painful subjects: flexibility and constraints in patient interviews
VL  - 149
ID  - 60
ER  - 


TY  - JOUR
AB  - OBJECTIVE: The authors used the i2b2 Medication Extraction Challenge to evaluate their entity extraction methods, contribute to the generation of a publicly available collection of annotated clinical notes, and start developing methods for ontology-based reasoning using structured information generated from the unstructured clinical narrative. DESIGN: Extraction of salient features of medication orders from the text of de-identified hospital discharge summaries was addressed with a knowledge-based approach using simple rules and lookup lists. The entity recognition tool, MetaMap, was combined with dose, frequency, and duration modules specifically developed for the Challenge as well as a prototype module for reason identification. MEASUREMENTS: Evaluation metrics and corresponding results were provided by the Challenge organizers. RESULTS: The results indicate that robust rule-based tools achieve satisfactory results in extraction of simple elements of medication orders, but more sophisticated methods are needed for identification of reasons for the orders and durations. LIMITATIONS: Owing to the time constraints and nature of the Challenge, some obvious follow-on analysis has not been completed yet. CONCLUSIONS: The authors plan to integrate the new modules with MetaMap to enhance its accuracy. This integration effort will provide guidance in retargeting existing tools for better processing of clinical text.
AU  - Mork, J. G.
AU  - Bodenreider, O.
AU  - Demner-Fushman, D.
AU  - Dogan, R. I.
AU  - Lang, F. M.
AU  - Lu, Z.
AU  - Neveol, A.
AU  - Peters, L.
AU  - Shooshan, S. E.
AU  - Aronson, A. R.
DA  - 2010
DO  - 10.1136/jamia.2010.003970.
IS  - 5
KW  - eppi-reviewer4
N1  - 21445302
5690
PY  - 2010
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 536-9
ST  - Extracting Rx information from clinical narrative
T2  - J Am Med Inform Assoc
TI  - Extracting Rx information from clinical narrative
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2995679/pdf/amiajnl3970.pdf
VL  - 17
ID  - 126
ER  - 


TY  - JOUR
AB  - Electronic clinical documentation can be useful for activities such as public health surveillance, quality improvement, and research, but existing methods of de-identification may not provide sufficient protection of patient data. The general-purpose natural language processor MedLEE retains medical concepts while excluding the remaining text so, in addition to processing text into structured data, it may be able provide a secondary benefit of de-identification. Without modifying the system, the authors tested the ability of MedLEE to remove protected health information (PHI) by comparing 100 outpatient clinical notes with the corresponding XML-tagged output. Of 809 instances of PHI, 26 (3.2%) were detected in output as a result of processing and identification errors. However, PHI in the output was highly transformed, much appearing as normalized terms for medical concepts, potentially making re-identification more difficult. The MedLEE processor may be a good enhancement to other de-identification systems, both removing PHI and providing coded data from clinical text.
AU  - Morrison, F. P.
AU  - Li, L.
AU  - Lai, A. M.
AU  - Hripcsak, G.
DA  - 2009
DO  - 10.1197/jamia.M2862. Epub 2008 Oct 24.
IS  - 1
KW  - eppi-reviewer4
N1  - 21447786
6186
PY  - 2009
SN  - 1067-5027 (Print) 1067-5027 (Linking)
SP  - 37-9
ST  - Repurposing the clinical record: can an existing natural language processing system de-identify clinical notes?
T2  - J Am Med Inform Assoc
TI  - Repurposing the clinical record: can an existing natural language processing system de-identify clinical notes?
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605586/pdf/37.S1067502708001928.main.pdf
VL  - 16
ID  - 161
ER  - 


TY  - JOUR
AB  - BACKGROUND: Bodyweight related measures (weight, height, BMI, abdominal circumference) are extremely important for clinical care, research and quality improvement. These and other vitals signs data are frequently missing from structured tables of electronic health records. However they are often recorded as text within clinical notes. In this project we sought to develop and validate a learning algorithm that would extract bodyweight related measures from clinical notes in the Veterans Administration (VA) Electronic Health Record to complement the structured data used in clinical research. METHODS: We developed the Regular Expression Discovery Extractor (REDEx), a supervised learning algorithm that generates regular expressions from a training set. The regular expressions generated by REDEx were then used to extract the numerical values of interest. To train the algorithm we created a corpus of 268 outpatient primary care notes that were annotated by two annotators. This annotation served to develop the annotation process and identify terms associated with bodyweight related measures for training the supervised learning algorithm. Snippets from an additional 300 outpatient primary care notes were subsequently annotated independently by two reviewers to complete the training set. Inter-annotator agreement was calculated. REDEx was applied to a separate test set of 3561 notes to generate a dataset of weights extracted from text. We estimated the number of unique individuals who would otherwise not have bodyweight related measures recorded in the CDW and the number of additional bodyweight related measures that would be additionally captured. RESULTS: REDEx's performance was: accuracy=98.3%, precision=98.8%, recall=98.3%, F=98.5%. In the dataset of weights from 3561 notes, 7.7% of notes contained bodyweight related measures that were not available as structured data. In addition 2 additional bodyweight related measures were identified per individual per year. CONCLUSION: Bodyweight related measures are frequently stored as text in clinical notes. A supervised learning algorithm can be used to extract this data. Implications for clinical care, epidemiology, and quality improvement efforts are discussed.
AU  - Murtaugh, M. A.
AU  - Gibson, B. S.
AU  - Redd, D.
AU  - Zeng-Treitler, Q.
DA  - 2015
DO  - 10.1016/j.jbi.2015.02.009. Epub 2015 Mar 5.
KW  - eppi-reviewer4
N1  - 21447733
4090
PY  - 2015
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 186-90
ST  - Regular expression-based learning to extract bodyweight values from clinical notes
T2  - J Biomed Inform
TI  - Regular expression-based learning to extract bodyweight values from clinical notes
UR  - http://ac.els-cdn.com/S1532046415000416/1-s2.0-S1532046415000416-main.pdf?_tid=5359b3f6-5417-11e6-bf7f-00000aab0f6c&acdnat=1469637164_f63299de6ad7ffeb07385d8267c20deb
VL  - 54
ID  - 195
ER  - 


TY  - JOUR
AB  - The paper describes a rule-based information extraction (IE) system developed for Polish medical texts. We present two applications designed to select data from medical documentation in Polish: mammography reports and hospital records of diabetic patients. First, we have designed a special ontology that subsequently had its concepts translated into two separate models, represented as typed feature structure (TFS) hierarchies, complying with the format required by the IE platform we adopted. Then, we used dedicated IE grammars to process documents and fill in templates provided by the models. In particular, in the grammars, we addressed such linguistic issues as: ambiguous keywords, negation, coordination or anaphoric expressions. Resolving some of these problems has been deferred to a post-processing phase where the extracted information is further grouped and structured into more complex templates. To this end, we defined special heuristic algorithms on the basis of sample data. The evaluation of the implemented procedures shows their usability for clinical data extraction tasks. For most of the evaluated templates, precision and recall well above 80% were obtained.
AU  - Mykowiecka, A.
AU  - Marciniak, M.
AU  - Kupsc, A.
DA  - 2009
DO  - 10.1016/j.jbi.2009.07.007. Epub 2009 Jul 29.
IS  - 5
KW  - eppi-reviewer4
N1  - 21447913
5987
PY  - 2009
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 923-36
ST  - Rule-based information extraction from patients' clinical data
T2  - J Biomed Inform
TI  - Rule-based information extraction from patients' clinical data
UR  - http://ac.els-cdn.com/S1532046409001002/1-s2.0-S1532046409001002-main.pdf?_tid=5702c948-5417-11e6-993d-00000aab0f27&acdnat=1469637170_b1059df2567a14d6c4dc94259ca33446
VL  - 42
ID  - 147
ER  - 


TY  - JOUR
AB  - Free text entry versus structured data has been proposed as models in data entry in health information systems. A new user interface was developed with the objective of improving data capture. It also implemented a modification of the discharge summary data entry user interface that allowed the selection of already coded terms from a local terminology in the context of an inpatient electronic medical record. This software interacts online with a terminology server to provide feedback on data entry to clinical users in order to automatically code data. To evaluate the impact of this new software, we measured user satisfaction and the impact on autocodification rate. The new system had good acceptance from the users who ranked it high using QUIS (Questionnaire for User Interaction Satisfaction) and the auto codification rate improved from 61.5% to 88.39%.
AU  - Navas, H.
AU  - Osornio, A. L.
AU  - Baum, A.
AU  - Gomez, A.
AU  - Luna, D.
AU  - de, Quiros
AU  - F, G.
DA  - 2007
IS  - Pt 1
KW  - eppi-reviewer4
N1  - 21444439
6432
Navas, Hernan Osornio, Alejandro Lopez Baum, Analia Gomez, Adrian Luna, Daniel de Quiros, Fernan Gonzalez Bernaldo Evaluation Studies Netherlands Stud Health Technol Inform. 2007;129(Pt 1):650-4.
PY  - 2007
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 650-4
ST  - Creation and evaluation of a terminology server for the interactive coding of discharge summaries
T2  - Stud Health Technol Inform
TI  - Creation and evaluation of a terminology server for the interactive coding of discharge summaries
VL  - 129
ID  - 171
ER  - 


TY  - JOUR
AB  - BACKGROUND: The incorporation of sex and gender-specific analysis in medical research is increasing due to pressure from public agencies, funding bodies, and the clinical and research community. However, generations of knowledge and publication trends in this discipline are currently spread over distinct specialties and are difficult to analyze comparatively. METHODS: Using a text-mining approach, we have analysed sex and gender aspects in research within nine clinical subspecialties--Cardiology, Pulmonology, Nephrology, Endocrinology, Gastroenterology, Haematology, Oncology, Rheumatology, Neurology--using six paradigmatic diseases in each one. Articles have been classified into five pre-determined research categories--Epidemiology, Pathophysiology, Clinical research, Management and Outcomes. Additional information has been collected on the type of study (human/animal) and the number of subjects included. Of the 8,836 articles initially retrieved, 3,466 (39%) included sex and gender-specific research and have been further analysed. RESULTS: Literature incorporating sex/gender analysis increased over time and displays a stronger trend if compared to overall publication increase. All disciplines, but cardiology (22%), demonstrated an underrepresentation of research about gender differences in management, which ranges from 3 to 14%. While the use of animal models for identification of sex differences in basic research varies greatly among disciplines, studies involving human subjects are frequently conducted in large cohorts with more than 1,000 patients (24% of all human studies). CONCLUSIONS: Heterogeneity characterizes sex and gender-specific research. Although large cohorts are often analysed, sex and gender differences in clinical management are insufficiently investigated leading to potential inequalities in health provision and outcomes.
AU  - Oertelt-Prigione, S.
AU  - Parol, R.
AU  - Krohn, S.
AU  - Preissner, R.
AU  - Regitz-Zagrosek, V.
DA  - 2010
DO  - 10.1186/1741-7015-8-70.
KW  - eppi-reviewer4
N1  - 21443547
5629
PY  - 2010
SN  - 1741-7015 (Electronic) 1741-7015 (Linking)
SP  - 70
ST  - Analysis of sex and gender-specific research reveals a common increase in publications and marked differences between disciplines
T2  - BMC Med
TI  - Analysis of sex and gender-specific research reveals a common increase in publications and marked differences between disciplines
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2993643/pdf/1741-7015-8-70.pdf
VL  - 8
ID  - 162
ER  - 


TY  - JOUR
AB  - BACKGROUND: As the output of biological assays increase in resolution and volume, the body of specialized biological data, such as functional annotations of gene and protein sequences, enables extraction of higher-level knowledge needed for practical application in bioinformatics. Whereas common types of biological data, such as sequence data, are extensively stored in biological databases, functional annotations, such as immunological epitopes, are found primarily in semi-structured formats or free text embedded in primary scientific literature. RESULTS: We defined and applied a machine learning approach for literature classification to support updating of TANTIGEN, a knowledgebase of tumor T-cell antigens. Abstracts from PubMed were downloaded and classified as either "relevant" or "irrelevant" for database update. Training and five-fold cross-validation of a k-NN classifier on 310 abstracts yielded classification accuracy of 0.95, thus showing significant value in support of data extraction from the literature. CONCLUSION: We here propose a conceptual framework for semi-automated extraction of epitope data embedded in scientific literature using principles from text mining and machine learning. The addition of such data will aid in the transition of biological databases to knowledgebases.
AU  - Olsen, L.
AU  - Johan, Kudahl
AU  - Winther, O.
AU  - Brusic, V.
DA  - 2013
DO  - 10.1186/1471-2164-14-S5-S14. Epub 2013 Oct 16.
KW  - eppi-reviewer4
N1  - 21446397
4521
PY  - 2013
SN  - 1471-2164 (Electronic) 1471-2164 (Linking)
SP  - S14
ST  - Literature classification for semi-automated updating of biological knowledgebases
T2  - BMC Genomics
TI  - Literature classification for semi-automated updating of biological knowledgebases
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3852072/pdf/1471-2164-14-S5-S14.pdf
VL  - 14 Suppl 5
ID  - 118
ER  - 


TY  - BOOK
A2  - Ortuno, F.
A2  - Rojas, I.
AB  - The staging in breast cancer is one of the most important prognostic factors. However, the complex coding TNM criteria, which includes clinical and pathological components, the existence of different versions of TNM classification guides over time, and the variability of the source used to obtain data, makes the manual collection of TNM staging in free text be variable and imprecise. The aim of this project is to develop a tool based on artificial intelligence that allows the collection of tumor size (T) staging data for breast cancer automatically, reducing the variability. Our approach, based on two steps, starts with the detection and extraction of tumor's size characteristics in free text, using a simple natural language processor. Secondly, based on the data extracted, we applied different data mining algorithms for the T classification such as the J48 classifier tree, LADtree and NaiveBayes. Then, structured TNM reports for patients are created.
AU  - Otal, R. G.
AU  - Guerra, J. L. L.
AU  - Calderon, C. L. P.
AU  - Garcia, A. M.
AU  - Gironzini, V. S.
AU  - Serrano, J. P.
AU  - Conde, A. M.
AU  - Gordillo, M. J. O.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21443608
8658
Gonzalez Otal, R. Lopez Guerra, J. L. Parra Calderon, C. L. Martinez Garcia, A. Suarez Gironzini, V. Peinado Serrano, J. Moreno Conde, A. Ortiz Gordillo, M. J. International Work-Conference on Bioinformatics and Biomedical Engineering MAR 18-20, 2013 Univ Grenada, Fac Sci, Granada, SPAIN Univ Grenada, Spanish Chapter IEEE Computat Intelligence Soc, SBV Improver, Illumina, e Hlth Business Dev Bull Espana S A, Univ Grenada, Fac Sci, Univ Grenada, Dept Comp Architecture & Comp Technol, Univ Granada, CITIC UGR Parra Calderon, Carlos Luis/C-9315-2015; Moreno-Conde, Alberto/C-9358-2015 Parra Calderon, Carlos Luis/0000-0003-2609-575X;
PY  - 2013
SN  - 978-84-15814-13-9
SP  - 499-500
ST  - Application of Artificial Intelligence in Tumors Sizing Classification for Breast Cancer
T2  - Proceedings Iwbbio 2013: International Work-Conference on Bioinformatics and Biomedical Engineering
TI  - Application of Artificial Intelligence in Tumors Sizing Classification for Breast Cancer
UR  - <Go to ISI>://WOS:000322416100082
ID  - 172
ER  - 


TY  - JOUR
AB  - To estimate the prevalence of problem opioid use, we used natural language processing (NLP) techniques to identify clinical notes containing text indicating problem opioid use from over 8 million electronic health records (EHRs) of 22,142 adult patients receiving chronic opioid therapy (COT) within Group Health clinics from 2006 to 2012. Computer-assisted manual review of NLP-identified clinical notes was then used to identify patients with problem opioid use (overuse, misuse, or abuse) according to the study criteria. These methods identified 9.4% of patients receiving COT as having problem opioid use documented during the study period. An additional 4.1% of COT patients had an International Classification of Disease, version 9 (ICD-9) diagnosis without NLP-identified problem opioid use. Agreement between the NLP methods and ICD-9 coding was moderate (kappa = 0.61). Over one-third of the NLP-positive patients did not have an ICD-9 diagnostic code for opioid abuse or dependence. We used structured EHR data to identify 14 risk indicators for problem opioid use. Forty-seven percent of the COT patients had 3 or more risk indicators. The prevalence of problem opioid use was 9.6% among patients with 3 to 4 risk indicators, 26.6% among those with 5 to 6 risk indicators, and 55.04% among those with 7 or more risk indicators. Higher rates of problem opioid use were observed among young COT patients, patients who sustained opioid use for more than 4 quarters, and patients who received higher opioid doses. Methods used in this study provide a promising approach to efficiently identify clinically recognized problem opioid use documented in EHRs of large patient populations. Computer-assisted manual review of EHR clinical notes found a rate of problem opioid use of 9.4% among 22,142 COT patients over 7 years.
AU  - Palmer, R. E.
AU  - Carrell, D. S.
AU  - Cronkite, D.
AU  - Saunders, K.
AU  - Gross, D. E.
AU  - Masters, E.
AU  - Donevan, S.
AU  - Hylan, T. R.
AU  - Von, Kroff
DA  - 2015
DO  - 10.1097/j.pain.0000000000000145.
IS  - 7
KW  - eppi-reviewer4
N1  - 21447516
4083
PY  - 2015
SN  - 1872-6623 (Electronic) 0304-3959 (Linking)
SP  - 1208-14
ST  - The prevalence of problem opioid use in patients receiving chronic opioid therapy: computer-assisted review of electronic health record clinical notes
T2  - Pain
TI  - The prevalence of problem opioid use in patients receiving chronic opioid therapy: computer-assisted review of electronic health record clinical notes
UR  - http://ovidsp.tx.ovid.com/ovftpdfs/FPDDNCGCKCAOFF00/fs047/ovft/live/gv024/00006396/00006396-201507000-00009.pdf
VL  - 156
ID  - 45
ER  - 


TY  - JOUR
AB  - OBJECTIVE: The aim of this qualitative study is to better understand the types of error occurring during the management of cardiac arrests that led to a death. METHODS: All patient safety incidents involving management of cardiac arrests and resulting in death which were reported to a national patient safety database over a 17-month period were analysed. Structured data from each report were extracted and these together with the free text, were subjected to content analysis which was inductive, with the coding scheme emerged from continuous reading and re-reading of incidents. RESULTS: There were 30 patient safety incidents involving management of cardiac arrests and resulting in death. The reviewers identified a main shortfall in the management of each cardiac arrest and this resulted in 12 different factors being documented. These were grouped into four themes that highlighted systemic weaknesses: miscommunication involving crash number (4/30, 13%), shortfalls in staff attending the arrest (4/30, 13%), equipment deficits (11/30, 36%), and poor application of knowledge and skills (11/30, 37%). CONCLUSION: The factors identified represent serious shortfalls in the quality of response to cardiac arrests resulting in death in hospital. No firm conclusion can be drawn about how many deaths in the study population would have been averted if the emergency had been managed to a high standard. The effective management of cardiac arrests should be considered as one of the markers of safe care within a healthcare organisation.
AU  - Panesar, S. S.
AU  - Ignatowicz, A. M.
AU  - Donaldson, L. J.
DA  - 2014
IS  - 12
KW  - eppi-reviewer4
N1  - 21445096
4200
Panesar, Sukhmeet S Ignatowicz, Agnieszka M Donaldson, Liam J Observational Study Research Support, Non-U.S. Gov't Ireland Resuscitation. 2014 Dec;85(12):1759-63.
PY  - 2014
SN  - 1873-1570 (Electronic) 0300-9572 (Linking)
SP  - 1759-63
ST  - Errors in the management of cardiac arrests: an observational study of patient safety incidents in England
T2  - Resuscitation
TI  - Errors in the management of cardiac arrests: an observational study of patient safety incidents in England
UR  - http://ac.els-cdn.com/S0300957214007795/1-s2.0-S0300957214007795-main.pdf?_tid=5c2e9a50-5417-11e6-8f0a-00000aacb361&acdnat=1469637179_cf0ad82d5f891a610f0c2ca463df44e7
VL  - 85
ID  - 28
ER  - 


TY  - JOUR
AB  - RESEARCH OBJECTIVE: To develop scalable informatics infrastructure for normalization of both structured and unstructured electronic health record (EHR) data into a unified, concept-based model for high-throughput phenotype extraction. MATERIALS AND METHODS: Software tools and applications were developed to extract information from EHRs. Representative and convenience samples of both structured and unstructured data from two EHR systems-Mayo Clinic and Intermountain Healthcare-were used for development and validation. Extracted information was standardized and normalized to meaningful use (MU) conformant terminology and value set standards using Clinical Element Models (CEMs). These resources were used to demonstrate semi-automatic execution of MU clinical-quality measures modeled using the Quality Data Model (QDM) and an open-source rules engine. RESULTS: Using CEMs and open-source natural language processing and terminology services engines-namely, Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) and Common Terminology Services (CTS2)-we developed a data-normalization platform that ensures data security, end-to-end connectivity, and reliable data flow within and across institutions. We demonstrated the applicability of this platform by executing a QDM-based MU quality measure that determines the percentage of patients between 18 and 75 years with diabetes whose most recent low-density lipoprotein cholesterol test result during the measurement year was <100 mg/dL on a randomly selected cohort of 273 Mayo Clinic patients. The platform identified 21 and 18 patients for the denominator and numerator of the quality measure, respectively. Validation results indicate that all identified patients meet the QDM-based criteria. CONCLUSIONS: End-to-end automated systems for extracting clinical information from diverse EHR systems require extensive use of standardized vocabularies and terminologies, as well as robust information models for storing, discovering, and processing that information. This study demonstrates the application of modular and open-source resources for enabling secondary use of EHR data through normalization into standards-based, comparable, and consistent format for high-throughput phenotyping to identify patient cohorts.
AU  - Pathak, J.
AU  - Bailey, K. R.
AU  - Beebe, C. E.
AU  - Bethard, S.
AU  - Carrell, D. C.
AU  - Chen, P. J.
AU  - Dligach, D.
AU  - Endle, C. M.
AU  - Hart, L. A.
AU  - Haug, P. J.
AU  - Huff, S. M.
AU  - Kaggal, V. C.
AU  - Li, D.
AU  - Liu, H.
AU  - Marchant, K.
AU  - Masanz, J.
AU  - Miller, T.
AU  - Oniki, T. A.
AU  - Palmer, M.
AU  - Peterson, K. J.
AU  - Rea, S.
AU  - Savova, G. K.
AU  - Stancl, C. R.
AU  - Sohn, S.
AU  - Solbrig, H. R.
AU  - Suesse, D. B.
AU  - Tao, C.
AU  - Taylor, D. P.
AU  - Westberg, L.
AU  - Wu, S.
AU  - Zhuo, N.
AU  - Chute, C. G.
DA  - 2013
DO  - 10.1136/amiajnl-2013-001939. Epub 2013 Nov 4.
IS  - e2
KW  - eppi-reviewer4
N1  - 21447058
4645
PY  - 2013
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - e341-8
ST  - Normalization and standardization of electronic health records for high-throughput phenotyping: the SHARPn consortium
T2  - J Am Med Inform Assoc
TI  - Normalization and standardization of electronic health records for high-throughput phenotyping: the SHARPn consortium
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3861933/pdf/amiajnl-2013-001939.pdf
VL  - 20
ID  - 127
ER  - 


TY  - JOUR
AB  - RxNorm and NDF-RT published by the National Library of Medicine (NLM) and Veterans Affairs (VA), respectively, are two publicly available federal medication terminologies. In this study, we evaluate the applicability of RxNorm and National Drug File-Reference Terminology (NDF-RT) for extraction and classification of medication data retrieved using structured querying and natural language processing techniques from electronic health records at two different medical centers within the Rochester Epidemiology Project (REP). Specifically, we explore how mappings between RxNorm concept codes and NDF-RT drug classes can be leveraged for hierarchical organization and grouping of REP medication data, identify gaps and coverage issues, and analyze the recently released NLM's NDF-RT Web service API. Our study concludes that RxNorm and NDF-RT can be applied together for classification of medication extracted from multiple EHR systems, although several issues and challenges remain to be addressed. We further conclude that the Web service APIs developed by the NLM provide useful functionalities for such activities.
AU  - Pathak, J.
AU  - Murphy, S. P.
AU  - Willaert, B. N.
AU  - Kremers, H. M.
AU  - Yawn, B. P.
AU  - Rocca, W. A.
AU  - Chute, C. G.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21448704
5301
Pathak, Jyotishman Murphy, Sean P Willaert, Brian N Kremers, Hilal M Yawn, Barbara P Rocca, Walter A Chute, Christopher G R01 AG034676/AG/NIA NIH HHS/United States Research Support, Non-U.S. Gov't United States AMIA Annu Symp Proc. 2011;2011:1089-98. Epub 2011 Oct 22.
PY  - 2011
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1089-98
ST  - Using RxNorm and NDF-RT to classify medication data extracted from electronic health records: experiences from the Rochester Epidemiology Project
T2  - AMIA Annu Symp Proc
TI  - Using RxNorm and NDF-RT to classify medication data extracted from electronic health records: experiences from the Rochester Epidemiology Project
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3243205/pdf/1089_amia_2011_proc.pdf
VL  - 2011
ID  - 128
ER  - 


TY  - JOUR
AB  - Semantic interoperability requires consistent use of controlled terminologies. However, non-terminology experts (although perhaps experts in a particular domain) are prone to produce variant coding. We examine this problem by investigating SNOMED CT coding variation for other findings reported on case report forms from a clinical research study on urea cycle disorders. The natural language findings from the forms were normalized, and the associated SNOMED CT concept descriptions were compared. The subset of normalized strings associated with two different concept descriptions were further compared to determine the relationship among the associated SNOMED CT concepts. We found 45% of the concept description pairs were associated with two hierarchically related concepts or with the same concept, while 55% were associated with two unrelated concepts. Clearer guidelines for use of SNOMED CT in particular contexts, or structured data entry tools tailored to the needs of non-expert coders, are needed to better manage coding variation.
AU  - Patrick, T. B.
AU  - Richesson, R.
AU  - Andrews, J. E.
AU  - Folk, L. C.
DA  - 2008
KW  - eppi-reviewer4
N1  - 21448090
6176
Patrick, Timothy B Richesson, Rachel Andrews, James E Folk, Lillian C U54 HD061221/HD/NICHD NIH HHS/United States U54RR01945/RR/NCRR NIH HHS/United States Research Support, N.I.H., Extramural Research Support, Non-U.S. Gov't United States AMIA Annu Symp Proc. 2008 Nov 6:11-5.
PY  - 2008
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 11-5
ST  - SNOMED CT coding variation and grouping for "other findings" in a longitudinal study on urea cycle disorders
T2  - AMIA Annu Symp Proc
TI  - SNOMED CT coding variation and grouping for "other findings" in a longitudinal study on urea cycle disorders
ID  - 61
ER  - 


TY  - JOUR
AB  - OBJECTIVE: There is increasing interest in using electronic health records (EHRs) to identify subjects for genomic association studies, due in part to the availability of large amounts of clinical data and the expected cost efficiencies of subject identification. We describe the construction and validation of an EHR-based algorithm to identify subjects with age-related cataracts. MATERIALS AND METHODS: We used a multi-modal strategy consisting of structured database querying, natural language processing on free-text documents, and optical character recognition on scanned clinical images to identify cataract subjects and related cataract attributes. Extensive validation on 3657 subjects compared the multi-modal results to manual chart review. The algorithm was also implemented at participating electronic MEdical Records and GEnomics (eMERGE) institutions. RESULTS: An EHR-based cataract phenotyping algorithm was successfully developed and validated, resulting in positive predictive values (PPVs) >95%. The multi-modal approach increased the identification of cataract subject attributes by a factor of three compared to single-mode approaches while maintaining high PPV. Components of the cataract algorithm were successfully deployed at three other institutions with similar accuracy. DISCUSSION: A multi-modal strategy incorporating optical character recognition and natural language processing may increase the number of cases identified while maintaining similar PPVs. Such algorithms, however, require that the needed information be embedded within clinical documents. CONCLUSION: We have demonstrated that algorithms to identify and characterize cataracts can be developed utilizing data collected via the EHR. These algorithms provide a high level of accuracy even when implemented across multiple EHRs and institutional boundaries.
AU  - Peissig, P. L.
AU  - Rasmussen, L. V.
AU  - Berg, R. L.
AU  - Linneman, J. G.
AU  - McCarty, C. A.
AU  - Waudby, C.
AU  - Chen, L.
AU  - Denny, J. C.
AU  - Wilke, R. A.
AU  - Pathak, J.
AU  - Carrell, D.
AU  - Kho, A. N.
AU  - Starren, J. B.
DA  - 2012
DO  - 10.1136/amiajnl-2011-000456.
IS  - 2
KW  - eppi-reviewer4
N1  - 21445943
5259
PY  - 2012
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 225-34
ST  - Importance of multi-modal approaches to effectively identify cataract cases from electronic health records
T2  - J Am Med Inform Assoc
TI  - Importance of multi-modal approaches to effectively identify cataract cases from electronic health records
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3277618/pdf/amiajnl-2011-000456.pdf
VL  - 19
ID  - 29
ER  - 


TY  - JOUR
AB  - The electronic medical record has evolved from a digital representation of individual patient results and documents to information of large scale and complexity. Big Data refers to new technologies providing management and processing capabilities, targeting massive and disparate data sets. For an individual patient, techniques such as Natural Language Processing allow the integration and analysis of textual reports with structured results. For groups of patients, Big Data offers the promise of large-scale analysis of outcomes, patterns, temporal trends, and correlations. The evolution of Big Data analytics moves us from description and reporting to forecasting, predictive modeling, and decision optimization.
AU  - Peters, S. G.
AU  - Buntrock, J. D.
DA  - 2014
DO  - 10.1097/JAC.0000000000000037.
IS  - 3
KW  - eppi-reviewer4
N1  - 21443838
4437
PY  - 2014
SN  - 1550-3267 (Electronic) 0148-9917 (Linking)
SP  - 206-10
ST  - Big data and the electronic health record
T2  - J Ambul Care Manage
TI  - Big data and the electronic health record
UR  - http://ovidsp.tx.ovid.com/ovftpdfs/FPDDNCGCKCAOFF00/fs046/ovft/live/gv023/00004479/00004479-201407000-00004.pdf
VL  - 37
ID  - 176
ER  - 


TY  - JOUR
AB  - Objective: Performance of computerized adverse drug event (ADE) monitoring of electronic health records through a prospective ADE Monitor and ICD9-coded clinical text review operating independently and simultaneously on the same patient population for a 10-year period are compared. Requirements are compiled for clinical decision support in pharmacy systems to enhance ADE detection. Methods: A large tertiary care facility in Utah, with a history of quality improvement using its advanced hospital information system, was leveraged in this study. ICD9-based review of clinical charts (ICD9 System) was compared quantitatively and qualitatively to computer-assisted pharmacist-verified ADEs (ADE Monitor). The capture-recapture statistical method was applied to the data to determine an estimated prevalence of ADEs. Results: A total estimated ADE prevalence of 5.53% (13,420/242,599) was calculated, with the ICD9 system identifying 2,604 or 19.4%, and the ADE monitor 3,386 or 25.2% of all estimated ADEs. Both methods commonly identified 4.9% of all estimated ADEs and matched 62.0% of the time, each having its strength in detecting a slightly different domain of ADEs. 70% of the ADE documentation in the clinical notes was found in the discharge summaries. Conclusion: Coupled with spontaneous reporting, computerized methods account for approximately half of all ADEs that can currently be detected. To enhance ADE monitoring and patient safety in a hospitalized setting, pharmacy information systems should incorporate prospective structuring and coding of the text in clinical charts and using that data alongside computer-generated alerts of laboratory results and drug orders. Natural language processing can aid computerized detection by automating the coding, in real-time, of physician text from clinical charts so that decision support rules can be created and applied. New detection strategies and enhancements to existing systems should be researched to enhance the detection of ADEs since approximately half are not currently detected.
AU  - Petratos, G. N.
AU  - Kim, Y.
AU  - Evans, R. S.
AU  - Williams, S. D.
AU  - Gardner, R. M.
DA  - 2010
IS  - 3
KW  - eppi-reviewer4
N1  - 21444221
9802
Petratos, G. N. Kim, Y. Evans, R. S. Williams, S. D. Gardner, R. M.
PY  - 2010
SN  - 1869-0327
SP  - 293-303
ST  - Comparing the Effectiveness of Computerized Adverse Drug Event Monitoring Systems to Enhance Clinical Decision Support for Hospitalized Patients
T2  - Applied Clinical Informatics
TI  - Comparing the Effectiveness of Computerized Adverse Drug Event Monitoring Systems to Enhance Clinical Decision Support for Hospitalized Patients
UR  - <Go to ISI>://WOS:000208686300009
https://aci.schattauer.de/contents/archive/issue/1094/manuscript/13557/download.html
VL  - 1
ID  - 30
ER  - 


TY  - JOUR
AB  - To support biomedical experts in their knowledge discovery process, we have developed a literature mining method called RaJoLink for identification of relations between biomedical concepts in disconnected sets of articles. The method implements Swanson's ABC model approach for generating hypotheses in a new way. The main novelty is a semi-automated suggestion of candidates for agents a that might be logically connected with a given phenomenon c under investigation. The choice of candidates for a is based on rare terms identified in the literature on c. As rare terms are not part of the typical range of information, which describe the phenomenon under investigation, such information might be considered as unusual observations about the phenomenon c. If literatures on these rare terms have an interesting term in common, this joint term is declared as a candidate for a. Linking terms b between literature on a and literature on c are then searched for in the closed discovery to provide additional supportive evidence for uncovered connections. We have applied the method to the literature on autism and have used MEDLINE as a source of data. Expert evaluation has confirmed that the discovered relations might contribute to a better understanding of autism.
AU  - Petric, I.
AU  - Urbancic, T.
AU  - Cestnik, B.
AU  - Macedoni-Luksic, M.
DA  - 2009
DO  - 10.1016/j.jbi.2008.08.004. Epub 2008 Aug 19.
IS  - 2
KW  - eppi-reviewer4
N1  - 21446398
6217
PY  - 2009
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 219-27
ST  - Literature mining method RaJoLink for uncovering relations between biomedical concepts
T2  - J Biomed Inform
TI  - Literature mining method RaJoLink for uncovering relations between biomedical concepts
UR  - http://ac.els-cdn.com/S1532046408001044/1-s2.0-S1532046408001044-main.pdf?_tid=680e203e-5417-11e6-8001-00000aab0f27&acdnat=1469637198_ebc0fe537647112e05fce19c962f3e58
VL  - 42
ID  - 46
ER  - 


TY  - JOUR
AB  - It is increasingly common to have medical information in electronic format. This includes scientific articles as well as clinical management reviews, and even records from health institutions with patient data. However, traditional instruments, both individual and institutional, are of little use for selecting the most appropriate information in each case, either in the clinical or research field. So-called text or data <<mining>> enables this huge amount of information to be managed, extracting it from various sources using processing systems (filtration and curation), integrating it and permitting the generation of new knowledge. This review aims to provide an overview of text and data mining, and of the potential usefulness of this bioinformatic technique in the exercise of care in respiratory medicine and in research in the same field.
AU  - Piedra, D.
AU  - Ferrer, A.
AU  - Gea, J.
DA  - 2014
DO  - 10.1016/j.arbres.2013.04.009. Epub 2014 Feb 4.
IS  - 3
KW  - eppi-reviewer4
N1  - 21448380
4544
PY  - 2014
SN  - 1579-2129 (Electronic) 0300-2896 (Linking)
SP  - 113-9
ST  - Text mining and medicine: usefulness in respiratory diseases
T2  - Arch Bronconeumol
TI  - Text mining and medicine: usefulness in respiratory diseases
UR  - http://www.sciencedirect.com/science/article/pii/S0300289613001452
VL  - 50
ID  - 135
ER  - 


TY  - JOUR
AB  - Drug Named Entity Recognition (drug-NER) is a critical step for complex Biomedical Natural Language Processing (BioNLP) tasks such as the extraction of pharmacogenomic, pharmaco-dynamic and pharmaco-kinetic parameters. Large quantities of high quality training data are almost always a prerequisite for employing supervised machine-learning (ML) techniques to achieve high classification performance. However, the human labour needed to produce and maintain such resources is a detrimental limitation. In this study, we attempt to improve the performance of drug NER without relying exclusively on manual annotations. Instead, we use either a small gold-standard corpus (120 abstracts) or no corpus at all. In our approach, we use a voting system to combine a number of heterogeneous models to enhance performance. Moreover, 11 regular-expressions that capture common drug suffixes were evolved via genetic-programming. We evaluate our approach against state-of-the-art recognisers trained on manual annotations, automatic annotations and a mixture of both. Aggregate classifiers are shown to improve performance, achieving a maximum F-score of 95%. In addition, combined models trained on mixed data are shown to achieve comparable performance to models trained exclusively on gold-standard data.
AU  - Piliouras, D.
AU  - Korkontzelos, I.
AU  - Dowsey, A.
AU  - Ananiadou, S.
AU  - Ieee
DA  - 2013
KW  - eppi-reviewer4
N1  - 21444580
8616
Piliouras, Dimitrios Korkontzelos, Ioannis Dowsey, Andrew Ananiadou, Sophia 1st IEEE International Conference on Healthcare Informatics (ICHI) SEP 09-11, 2013 Philadelphia, PA IEEE, IEEE Comp Soc Dowsey, Andrew/0000-0002-7404-9128 978-0-7695-5089-3
PY  - 2013
SP  - 14-21
ST  - Dealing with data sparsity in Drug Named Entity Recognition
T2  - 2013 Ieee International Conference on Healthcare Informatics (Ichi 2013)
TI  - Dealing with data sparsity in Drug Named Entity Recognition
UR  - <Go to ISI>://WOS:000332894400002
ID  - 148
ER  - 


TY  - JOUR
AB  - This paper describes the design, implementation and population of a lexical resource for biology and bioinformatics (the BioLexicon) developed within an ongoing European project. The aim of this project is text-based knowledge harvesting for support to information extraction and text mining in the biomedical domain. The BioLexicon is a large-scale lexical-terminological resource encoding different information types in one single integrated resource. In the design of the resource we follow the ISO/DIS 24613 "Lexical Mark-up Framework" standard, which ensures reusability of the information encoded and easy exchange of both data and architecture. The design of the resource also takes into account the needs of our text mining partners who automatically extract syntactic and semantic information from texts and feed it to the lexicon. The present contribution first describes in detail the model of the BioLexicon along its three main layers: morphology, syntax and semantics; then, it briefly describes the database implementation of the model and the population strategy followed within the project, together with an example. The BioLexicon database in fact comes equipped with automatic uploading procedures based on a common exchange XML format, which guarantees that the lexicon can be properly populated with data coming from different sources.
AU  - Quochi, V.
AU  - Monachini, M.
AU  - Del, Gratta
AU  - Calzolari, N.
AU  - European Language, Resources
AU  - Association
DA  - 2008
KW  - eppi-reviewer4
N1  - 21446365
9436
Quochi, Valeria Monachini, Monica Del Gratta, Riccardo Calzolari, Nicoletta 6th International Conference on Language Resources and Evaluation (LREC) MAY 28-30, 2008 Marrakech, MOROCCO European Language Resources Assoc (ELRA), Evaluat & Language Resources Distribut Agcy (ELDA), Ist Linguistica Computazionale (ILC), Nuance, Inst Nederlandse Lexicologie (INL), Microsoft, European Media Lab GmBH (EML), Linguatec, Connexor, Orange, Telisma, AAMT, AMTA, ACL, AFNLP, ALTA, COCOSDA, Oriental COCOSDA, EACL, EAMT, ELSNET, EURALEX, GWA, IAMT, ISCA, KnowledgeWeb, LDC, NEMLAR Network, SIGLEX, TEI, Technolangue French Program, WRITE, Informat Soc & Media, European Commiss, Unit E 2 Content & Knowledge Monachini, Monica/F-3077-2015; Quochi, Valeria/E-7468-2011 Quochi, Valeria/0000-0002-1321-5444
PY  - 2008
SP  - 2285-2292
ST  - A Lexicon for Biology and Bioinformatics: The BOOTStrep Experience
T2  - Sixth International Conference on Language Resources and Evaluation, Lrec 2008
TI  - A Lexicon for Biology and Bioinformatics: The BOOTStrep Experience
UR  - <Go to ISI>://WOS:000324028902062
ID  - 211
ER  - 


TY  - RPRT
AB  - In this position paper, we describe the design and implementation of the Oak Ridge Bio-surveillance Toolkit (ORBiT): a collection of novel statistical and machine learning tools implemented for (1) integrating heterogeneous traditional (e.g. emergency room visits, and non-traditional (social media such as Twitter and Instagram) data sources, (2) analyzing large-scale visual interface for the end-user to interact and provide feedback. We present examples of how ORBiT can be used to summarize extremely large-scale datasets effectively and how user interactions can translate into the data analytics process for bio-surveillance. We also present a strategy to estimate parameters relevant to disease spread models from near real time data feeds and show how these estimates can be integrated with disease spread models for large-scale populations. We conclude with a perspective on how integrating data and visual analytics could lead to better forecastingand prediction of disease spread as well as improved awareness of disease susceptible regions.
AU  - Ramanathan, Arvind
AU  - Pullum Laura, L.
AU  - Steed Chad, A.
AU  - Parker Tara, L.
AU  - Quinn Shannon, P.
AU  - Chennubhotla Chakra, S.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21509165
11671
PB  - OAK RIDGE NATIONAL LAB TN
PY  - 2013
RP  - 21509165
11671
SN  - HDIAC-2058156
SP  - 6
ST  - Oak Ridge Bio-surveillance Toolkit (orbit): Integrating Big-data Analytics With Visual Analysis For Public Health Dynamics
TI  - Oak Ridge Bio-surveillance Toolkit (orbit): Integrating Big-data Analytics With Visual Analysis For Public Health Dynamics
ID  - 165
ER  - 


TY  - JOUR
AB  - Breast malignancy is the second most common cause of cancer death among women in Western countries. Identifying high-risk patients is vital in order to provide them with specialized treatment. In some situations, such as when access to experienced oncologists is not possible, decision support methods can be helpful in predicting the recurrence of cancer. Three thousand six hundred ninety-nine breast cancer patients admitted in south-east Sweden from 1986 to 1995 were studied. A decision tree was trained with all patients except for 100 cases and tested with those 100 cases. Two domain experts were asked for their opinions about the probability of recurrence of a certain outcome for these 100 patients. ROC curves, area under the ROC curves, and calibration for predictions were computed and compared. After comparing the predictions from a model built by data mining with predictions made by two domain experts, no significant differences were noted. In situations where experienced oncologists are not available, predictive models created with data mining techniques can be used to support physicians in decision making with acceptable accuracy.
AU  - Razavi, A. R.
AU  - Gill, H.
AU  - Ahlfeldt, H.
AU  - Shahsavar, N.
DA  - 2007
IS  - 4
KW  - eppi-reviewer4
N1  - 21447436
6462
Razavi, Amir R Gill, Hans Ahlfeldt, Hans Shahsavar, Nosrat Comparative Study Research Support, Non-U.S. Gov't United States J Med Syst. 2007 Aug;31(4):263-73.
PY  - 2007
SN  - 0148-5598 (Print) 0148-5598 (Linking)
SP  - 263-73
ST  - Predicting metastasis in breast cancer: comparing a decision tree with domain experts
T2  - J Med Syst
TI  - Predicting metastasis in breast cancer: comparing a decision tree with domain experts
VL  - 31
ID  - 62
ER  - 


TY  - JOUR
AB  - The Strategic Health IT Advanced Research Projects (SHARP) Program, established by the Office of the National Coordinator for Health Information Technology in 2010 supports research findings that remove barriers for increased adoption of health IT. The improvements envisioned by the SHARP Area 4 Consortium (SHARPn) will enable the use of the electronic health record (EHR) for secondary purposes, such as care process and outcomes improvement, biomedical research and epidemiologic monitoring of the nation's health. One of the primary informatics problem areas in this endeavor is the standardization of disparate health data from the nation's many health care organizations and providers. The SHARPn team is developing open source services and components to support the ubiquitous exchange, sharing and reuse or 'liquidity' of operational clinical data stored in electronic health records. One year into the design and development of the SHARPn framework, we demonstrated end to end data flow and a prototype SHARPn platform, using thousands of patient electronic records sourced from two large healthcare organizations: Mayo Clinic and Intermountain Healthcare. The platform was deployed to (1) receive source EHR data in several formats, (2) generate structured data from EHR narrative text, and (3) normalize the EHR data using common detailed clinical models and Consolidated Health Informatics standard terminologies, which were (4) accessed by a phenotyping service using normalized data specifications. The architecture of this prototype SHARPn platform is presented. The EHR data throughput demonstration showed success in normalizing native EHR data, both structured and narrative, from two independent organizations and EHR systems. Based on the demonstration, observed challenges for standardization of EHR data for interoperable secondary use are discussed.
AU  - Rea, S.
AU  - Pathak, J.
AU  - Savova, G.
AU  - Oniki, T. A.
AU  - Westberg, L.
AU  - Beebe, C. E.
AU  - Tao, C.
AU  - Parker, C. G.
AU  - Haug, P. J.
AU  - Huff, S. M.
AU  - Chute, C. G.
DA  - 2012
DO  - 10.1016/j.jbi.2012.01.009. Epub 2012 Feb 4.
IS  - 4
KW  - eppi-reviewer4
N1  - 21443916
5257
PY  - 2012
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 763-71
ST  - Building a robust, scalable and standards-driven infrastructure for secondary use of EHR data: the SHARPn project
T2  - J Biomed Inform
TI  - Building a robust, scalable and standards-driven infrastructure for secondary use of EHR data: the SHARPn project
UR  - http://ac.els-cdn.com/S1532046412000202/1-s2.0-S1532046412000202-main.pdf?_tid=6d221418-5417-11e6-8164-00000aab0f6c&acdnat=1469637207_c83e8807977867bd0562abc228ad7754
VL  - 45
ID  - 31
ER  - 


TY  - JOUR
AB  - Objectives: The last decade has seen major advances in data driven healthcare at the research, clinical, and policy levels. The HITECH Act and the Affordable Care Act drive electronic health record infrastructure while reimbursement and cost drivers promote data use. Medical societies are gearing up for data driven healthcare, assuring discrete, structured data capture appropriate to their specialties. Compared to medical fields, procedural fields can be more challenging to study, particularly because medical history, medications, and laboratory values are rarely the most critical information. Use of unstructured narrative data (e.g. H&P, operative, and discharge notes) through natural language processing has been suggested as one process for extracting high quality, granular procedural technique and outcome data. But little discussion on unstructured data use has occurred at the society level. Our goal is to understand whether procedural societies could benefit from less reliance on manually captured registries and real consideration of unstructured data solutions. Methods: To evaluate critical processes underlying data driven healthcare in procedural fields, literature review and interviews were undertaken. Literature review focused on defined pathways to capture and use unstructured, granular data in clinical specialties. Manuscripts were separated based on reference to primary care, hospital-based care, procedural care and overall healthcare. Interviews were undertaken with experts in data mining and data-driven healthcare, as well as leaders in data use in anesthesia and surgical communities. Results: Literature reviews and interviews revealed a strong focus on using discrete data captured through manual processes. Interviewees theorized that early surgical leadership in analytics, via the National Surgical Quality Improvement Program (NSQIP) and other programs, supported registry creation with a focus on discrete data capture through a relatively narrow set of variables. This was the only possible process at the time these systems were developed and encompasses very low levels of procedural detail. Some interviewees expressed concern that capture of limited variables represented a bias within the registry system itself, suggesting that limited data acquisition translates to limited understanding of factors influencing outcomes. Conclusion: As healthcare advances into a more mature phase of meaningful data use, it has become clear that richer codified data are critically needed. While medical fields can effectively study clinical choices and outcomes on discrete structured data captured today by the electronic medical record, this may not be true for procedural fields that include a multitude of options in procedural technique. Narrative notes combined with natural language processing may represent a valuable option for capture of technique and post-procedural outcomes. We believe meaningful discussion within the procedural societies of effective capture and aggregation of narrative procedural and post-procedural content is warranted.
AU  - Riskin, L.
AU  - Azagury, D. E.
AU  - Varban, O.
AU  - Riskin, D. J.
DA  - 2012
KW  - eppi-reviewer4
N1  - 21443660
7669
PY  - 2012
SN  - 0930-2794
SP  - S422
ST  - Are procedural fields prepared for data driven healthcare?
T2  - Surgical Endoscopy and Other Interventional Techniques
TI  - Are procedural fields prepared for data driven healthcare?
UR  - http://www.embase.com/search/results?subaction=viewrecord&from=export&id=L71482756 http://dx.doi.org/10.1007/s00464-012-2203-x http://sfxhosted.exlibrisgroup.com/fda?sid=EMBASE&issn=09302794&id=doi:10.1007%2Fs00464-012-2203-x&atitle=Are+procedural+fields+prepared+for+data+driven+healthcare%3F&stitle=Surg.+Endosc.+Interv.+Tech.&title=Surgical+Endoscopy+and+Other+Interventional+Techniques&volume=26&issue=&spage=S422&epage=&aulast=Riskin&aufirst=Loren&auinit=L.&aufull=Riskin+L.&coden=&isbn=&pages
VL  - 26
ID  - 106
ER  - 


TY  - CHAP
A2  - Japkowicz, N.
A2  - Matwin, S.
AB  - Even if research communities and publishing houses are putting increasing efforts in delivering scientific articles as structured texts, nowadays a considerable part of on-line scientific literature is still available in layout-oriented data formats, like PDF, lacking any explicit structural or semantic information. As a consequence the bootstrap of textual analysis of scientific papers is often a time-consuming activity. We present the first version of the Dr. Inventor Framework, a publicly available collection of scientific text mining components useful to prevent or at least mitigate this problem. Thanks to the integration and the customization of several text mining tools and on-line services, the Dr. Inventor Framework is able to analyze scientific publications both in plain text and PDF format, making explicit and easily accessible core aspects of their structure and semantics. The facilities implemented by the Framework include the extraction of structured textual contents, the discursive characterization of sentences, the identifications of the structural elements of both papers header and bibliographic entries and the generation of graph based representations of text excerpts. The Framework is distributed as a Java library. We describe in detail the scientific mining facilities included in the Framework and present two use cases where the Framework is respectively exploited to boost scientific creativity and to generate RDF graphs from scientific publications.
AU  - Ronzano, F.
AU  - Saggion, H.
KW  - eppi-reviewer4
N1  - 21444873
8004
Ronzano, Francesco Saggion, Horacio 18th International Conference on Discovery Science (DS) OCT 04-06, 2015 Banff, CANADA ISM Canada, IBM Co, Alberta Innovates Technol Futures, Canadian Artificial Intelligence Assoc, Dalhousie Univ, Fac Comp Sci
PY  - 2015
SN  - 0302-9743 978-3-319-24282-8; 978-3-319-24281-1
SP  - 209-220
ST  - Dr. Inventor Framework: Extracting Structured Information from Scientific Publications
T2  - Discovery Science, Ds 2015
TI  - Dr. Inventor Framework: Extracting Structured Information from Scientific Publications
UR  - <Go to ISI>://WOS:000367678000018
VL  - 9356
ID  - 47
ER  - 


TY  - BOOK
A2  - Wani, M. A.
A2  - Kantardzic, M. M.
A2  - Li, T.
A2  - Liu, Y.
A2  - Kurgan, L.
A2  - Ye, J.
A2  - Ogihara, M.
A2  - Sagiroglu, S.
A2  - Chen, X. W.
A2  - Peterson, L.
A2  - Hafeez, K.
AB  - We apply a new active learning formulation to the problem of learning medical concepts from unstructured text. The new formulation is based on maximizing the mutual information that a sample labeling provides about the retrieval/classification model. This methodology is related to and extends the Query-by-Committee approach (QBC) [12] by exploiting unlabeled data in novel ways, beyond their common use only as potential query points. Unlike QBC, this method allows us to employ unlabeled data in addition to labeled data in order to select more appropriate samples for labeling. The samples thus chosen are both informative and also relevant according to a distribution of interest. This flexibility allows us to also tailor the model to arbitrary distributions relevant to the task at hand, in particular to the distribution of the test data. This formulation has implications in scenarios where the training and test distributions are different, or when a general model is adapted to a more specific model. Experiments were conducted to evaluate retrieval performance of natural-language text associated to various concepts of interest in the medical domain. We demonstrate the advantages of our formulation compared with QBC, the state-of-the art active learning approach, and against random sample selection.
AU  - Rosales, R.
AU  - Krishnamurthy, P.
AU  - Rao, R. B.
DA  - 2007
KW  - eppi-reviewer4
N1  - 21447993
10051
Rosales, Romer Krishnamurthy, Praveen Rao, R. Bharat 6th International Conference on Machine Learning and Applications DEC 13-15, 2007 Cincinnati, OH Assoc Machine Learning & Applicat, IEEE
PY  - 2007
SN  - 978-0-7695-3069-7
SP  - 530-536
ST  - Semi-supervised active learning for Modeling medical concepts from free text
T2  - Icmla 2007: Sixth International Conference on Machine Learning and Applications, Proceedings
TI  - Semi-supervised active learning for Modeling medical concepts from free text
UR  - <Go to ISI>://WOS:000252793400086
ID  - 212
ER  - 


TY  - JOUR
AB  - Clinical documentation is central to patient care. The success of electronic health record system adoption may depend on how well such systems support clinical documentation. A major goal of integrating clinical documentation into electronic heath record systems is to generate reusable data. As a result, there has been an emphasis on deploying computer-based documentation systems that prioritize direct structured documentation. Research has demonstrated that healthcare providers value different factors when writing clinical notes, such as narrative expressivity, amenability to the existing workflow, and usability. The authors explore the tension between expressivity and structured clinical documentation, review methods for obtaining reusable data from clinical notes, and recommend that healthcare providers be able to choose how to document patient care based on workflow and note content needs. When reusable data are needed from notes, providers can use structured documentation or rely on post-hoc text processing to produce structured data, as appropriate.
AU  - Rosenbloom, S. T.
AU  - Denny, J. C.
AU  - Xu, H.
AU  - Lorenzi, N.
AU  - Stead, W. W.
AU  - Johnson, K. B.
DA  - 2011
DO  - 10.1136/jamia.2010.007237. Epub 2011 Jan 12.
IS  - 2
KW  - eppi-reviewer4
N1  - 21444498
5581
PY  - 2011
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 181-6
ST  - Data from clinical notes: a perspective on the tension between structure and flexible documentation
T2  - J Am Med Inform Assoc
TI  - Data from clinical notes: a perspective on the tension between structure and flexible documentation
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3116264/pdf/amiajnl7237.pdf
VL  - 18
ID  - 119
ER  - 


TY  - JOUR
AB  - OBJECTIVES: To summarize current excellent research in the field of decision-support systems. METHODS: We provide a synopsis of the articles selected for the IMIA Yearbook 2009, from which we attempt to derive a synthetic overview of the activity and new trends in the field. RESULTS: Five papers from international peer reviewed journals have been selected for the section on decision support. While the state of the research in the field of decision-support systems is illustrated by a set of fairly heterogeneous studies, it is possible to identify trends. Thus, issues related to guidelines processing implementation occupies a central role in today's field with two alternative directions: 1. broad medical applications, which attempts to assist decision-makers to process large patient sets; 2. narrow clinical applications focused on in-depth real-time signal processing for a specific population or medical specialty. CONCLUSIONS: The best paper selection of articles on decision-supports shows examples of excellent research on methods concerning original development as well as quality assurance of reported studies. It is also observed that this year's selection point directly to more original research areas such as temporal signal processing, although more traditionally related areas, such as information retrieval and/or natural language processing, remain fairly active in the field. Altogether these papers support the idea that more elaborated computer tools, likely to combine together textual and highly structured data, including real-time data contents, are needed.
AU  - Ruch, P.
DA  - 2009
KW  - eppi-reviewer4
N1  - 21446537
5934
Ruch, P Germany Yearb Med Inform. 2009:96-8.
PY  - 2009
SN  - 2364-0502 (Electronic) 0943-4747 (Linking)
SP  - 96-8
ST  - A medical informatics perspective on decision support: toward a unified research paradigm combining biological vs. clinical, empirical vs. legacy, and structured vs. unstructured data
T2  - Yearb Med Inform
TI  - A medical informatics perspective on decision support: toward a unified research paradigm combining biological vs. clinical, empirical vs. legacy, and structured vs. unstructured data
ID  - 201
ER  - 


TY  - JOUR
AB  - Objective: Epilepsy encompasses an extensive array of clinical and research subdomains, many of which emphasize multi-modal physiological measurements such as electroencephalography and neuroimaging. The integration of structured, unstructured, and signal data into a coherent structure for patient care as well as clinical research requires an effective informatics infrastructure that is underpinned by a formal domain ontology. Methods: We have developed an epilepsy and seizure ontology (EpSO) using a four-dimensional epilepsy classification system that integrates the latest International League Against Epilepsy terminology recommendations and National Institute of Neurological Disorders and Stroke (NINDS) common data elements. It imports concepts from existing ontologies, including the Neural ElectroMagnetic Ontologies, and uses formal concept analysis to create a taxonomy of epilepsy syndromes based on their seizure semiology and anatomical location. Results: EpSO is used in a suite of informatics tools for (a) patient data entry, (b) epilepsy focused clinical free text processing, and (c) patient cohort identification as part of the multi-center NINDS-funded study on sudden unexpected death in epilepsy. EpSO is available for download at http://prism.case.edu/prism/index.php/ EpilepsyOntology. Discussion: An epilepsy ontology consortium is being created for community-driven extension, review, and adoption of EpSO. We are in the process of submitting EpSO to the BioPortal repository. Conclusions EpSO plays a critical role in informatics tools for epilepsy patient care and multi-center clinical research.
AU  - Sahoo, S. S.
AU  - Lhatoo, S. D.
AU  - Gupta, D. K.
AU  - Cui, L.
AU  - Zhao, M.
AU  - Jayapandian, C.
AU  - Bozorgi, A.
AU  - Zhang, G. Q.
DA  - 2014
IS  - 1
KW  - eppi-reviewer4
N1  - 21445088
7664
PY  - 2014
SN  - 1067-5027 1527-974X
SP  - 82-89
ST  - Epilepsy and seizure ontology: Towards an epilepsy informatics infrastructure for clinical research and patient care
T2  - Journal of the American Medical Informatics Association
TI  - Epilepsy and seizure ontology: Towards an epilepsy informatics infrastructure for clinical research and patient care
UR  - http://www.embase.com/search/results?subaction=viewrecord&from=export&id=L370514332 http://dx.doi.org/10.1136/amiajnl-2013-001696 http://sfxhosted.exlibrisgroup.com/fda?sid=EMBASE&issn=10675027&id=doi:10.1136%2Famiajnl-2013-001696&atitle=Epilepsy+and+seizure+ontology%3A+Towards+an+epilepsy+informatics+infrastructure+for+clinical+research+and+patient+care&stitle=J.+Am.+Med.+Informatics+Assoc.&title=Journal+of+the+American+Medical+Informatics+Association&volume=21&issue=1&spage=82&epage=89&aulas
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3912711/pdf/amiajnl-2013-001696.pdf
VL  - 21
ID  - 149
ER  - 


TY  - JOUR
AB  - Extracting comorbidity information is crucial for phenotypic studies because of the confounding effect of comorbidities. We developed an automated method that accurately determines comorbidities from electronic medical records. Using a modified version of the Charlson comorbidity index (CCI), two physicians created a reference standard of comorbidities by manual review of 100 admission notes. We processed the notes using the MedLEE natural language processing system, and wrote queries to extract comorbidities automatically from its structured output. Interrater agreement for the reference set was very high (97.7%). Our method yielded an F1 score of 0.761 and the summed CCI score was not different from the reference standard (p=0.329, power 80.4%). In comparison, obtaining comorbidities from claims data yielded an F1 score of 0.741, due to lower sensitivity (66.1%). Because CCI has previously been validated as a predictor of mortality and readmission, our method could allow automated prediction of these outcomes.
AU  - Salmasian, H.
AU  - Freedberg, D. E.
AU  - Friedman, C.
DA  - 2013
DO  - 10.1136/amiajnl-2013-001889. Epub 2013 Oct 31.
IS  - e2
KW  - eppi-reviewer4
N1  - 21444623
4649
PY  - 2013
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - e239-42
ST  - Deriving comorbidities from medical records using natural language processing
T2  - J Am Med Inform Assoc
TI  - Deriving comorbidities from medical records using natural language processing
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3861932/pdf/amiajnl-2013-001889.pdf
VL  - 20
ID  - 136
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To develop an algorithm for the discovery of drug treatment patterns for endocrine breast cancer therapy within an electronic medical record and to test the hypothesis that information extracted using it is comparable to the information found by traditional methods. MATERIALS: The electronic medical charts of 1507 patients diagnosed with histologically confirmed primary invasive breast cancer. METHODS: The automatic drug treatment classification tool consisted of components for: (1) extraction of drug treatment-relevant information from clinical narratives using natural language processing (clinical Text Analysis and Knowledge Extraction System); (2) extraction of drug treatment data from an electronic prescribing system; (3) merging information to create a patient treatment timeline; and (4) final classification logic. RESULTS: Agreement between results from the algorithm and from a nurse abstractor is measured for categories: (0) no tamoxifen or aromatase inhibitor (AI) treatment; (1) tamoxifen only; (2) AI only; (3) tamoxifen before AI; (4) AI before tamoxifen; (5) multiple AIs and tamoxifen cycles in no specific order; and (6) no specific treatment dates. Specificity (all categories): 96.14%-100%; sensitivity (categories (0)-(4)): 90.27%-99.83%; sensitivity (categories (5)-(6)): 0-23.53%; positive predictive values: 80%-97.38%; negative predictive values: 96.91%-99.93%. DISCUSSION: Our approach illustrates a secondary use of the electronic medical record. The main challenge is event temporality. CONCLUSION: We present an algorithm for automated treatment classification within an electronic medical record to combine information extracted through natural language processing with that extracted from structured databases. The algorithm has high specificity for all categories, high sensitivity for five categories, and low sensitivity for two categories.
AU  - Savova, G. K.
AU  - Olson, J. E.
AU  - Murphy, S. P.
AU  - Cafourek, V. L.
AU  - Couch, F. J.
AU  - Goetz, M. P.
AU  - Ingle, J. N.
AU  - Suman, V. J.
AU  - Chute, C. G.
AU  - Weinshilboum, R. M.
DA  - 2012
IS  - e1
KW  - eppi-reviewer4
N1  - 21443732
5317
Savova, Guergana K Olson, Janet E Murphy, Sean P Cafourek, Victoria L Couch, Fergus J Goetz, Matthew P Ingle, James N Suman, Vera J Chute, Christopher G Weinshilboum, Richard M CA 116201/CA/NCI NIH HHS/United States P50 CA116201/CA/NCI NIH HHS/United States R01 CA122340/CA/NCI NIH HHS/United States U01 HG 04599/HG/NHGRI NIH HHS/United States UO1 GM61388/GM/NIGMS NIH HHS/United States Research Support, N.I.H., Extramural Research Support, Non-U.S. Gov't United States J Am Med Inform Assoc. 2012 Jun;19(e1):e83-9. Epub 2011 Dec 1.
PY  - 2012
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - e83-9
ST  - Automated discovery of drug treatment patterns for endocrine therapy of breast cancer within an electronic medical record
T2  - J Am Med Inform Assoc
TI  - Automated discovery of drug treatment patterns for endocrine therapy of breast cancer within an electronic medical record
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3392847/pdf/amiajnl-2011-000295.pdf
VL  - 19
ID  - 107
ER  - 


TY  - JOUR
AB  - The BRENDA (BRaunschweig ENzyme Database, http://www.brenda-enzymes.org) enzyme information system is the main collection of enzyme functional and property data for the scientific community. The majority of the data are manually extracted from the primary literature. The content covers information on function, structure, occurrence, preparation and application of enzymes as well as properties of mutants and engineered variants. The number of manually annotated references increased by 30% to more than 100 000, the number of ligand structures by 45% to almost 100 000. New query, analysis and data management tools were implemented to improve data processing, data presentation, data input and data access. BRENDA now provides new viewing options such as the display of the statistics of functional parameters and the 3D view of protein sequence and structure features. Furthermore a ligand summary shows comprehensive information on the BRENDA ligands. The enzymes are linked to their respective pathways and can be viewed in pathway maps. The disease text mining part is strongly enhanced. It is possible to submit new, not yet classified enzymes to BRENDA, which then are reviewed and classified by the International Union of Biochemistry and Molecular Biology. A new SBML output format of BRENDA kinetic data allows the construction of organism-specific metabolic models.
AU  - Scheer, M.
AU  - Grote, A.
AU  - Chang, A.
AU  - Schomburg, I.
AU  - Munaretto, C.
AU  - Rother, M.
AU  - Sohngen, C.
AU  - Stelzer, M.
AU  - Thiele, J.
AU  - Schomburg, D.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21443905
9677
Scheer, Maurice Grote, Andreas Chang, Antje Schomburg, Ida Munaretto, Cornelia Rother, Michael Soehngen, Carola Stelzer, Michael Thiele, Juliane Schomburg, Dietmar 1
PY  - 2011
SN  - 0305-1048
SP  - D670-D676
ST  - BRENDA, the enzyme information system in 2011
T2  - Nucleic Acids Research
TI  - BRENDA, the enzyme information system in 2011
UR  - <Go to ISI>://WOS:000285831700107
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3013686/pdf/gkq1089.pdf
VL  - 39
ID  - 166
ER  - 


TY  - JOUR
AB  - Objective Enormous amounts of healthcare data are becoming increasingly accessible through the large-scale adoption of electronic health records. In this work, structured and unstructured (textual) data are combined to assign clinical diagnostic and procedural codes (specifically ICD-9-CM) to patient stays. We investigate whether integrating these heterogeneous data types improves prediction strength compared to using the data types in isolation. Methods Two separate data integration approaches were evaluated. Early data integration combines features of several sources within a single model, and late data integration learns a separate model per data source and combines these predictions with a meta-learner. This is evaluated on data sources and clinical codes from a broad set of medical specialties. Results When compared with the best individual prediction source, late data integration leads to improvements in predictive power (eg, overall F-measure increased from 30.6% to 38.3% for International Classification of Diseases, Ninth Revision, Clinical Modification (ICD-9-CM) diagnostic codes), while early data integration is less consistent. The predictive strength strongly differs between medical specialties, both for ICD-9-CM diagnostic and procedural codes. Discussion Structured data provides complementary information to unstructured data (and vice versa) for predicting ICD-9-CM codes. This can be captured most effectively by the proposed late data integration approach. Conclusions We demonstrated that models using multiple electronic health record data sources systematically outperform models using data sources in isolation in the task of predicting ICD-9-CM codes over a broad range of medical specialties.
AU  - Scheurwegs, E.
AU  - Luyckx, K.
AU  - Luyten, L.
AU  - Daelemans, W.
AU  - Van den, Bulcke
DA  - 2016
IS  - E1
KW  - eppi-reviewer4
N1  - 21444507
7702
Scheurwegs, Elyne Luyckx, Kim Luyten, Leon Daelemans, Walter Van den Bulcke, Tim
PY  - 2016
SN  - 1067-5027
SP  - E11-E19
ST  - Data integration of structured and unstructured sources for assigning clinical codes to patient stays
T2  - Journal of the American Medical Informatics Association
TI  - Data integration of structured and unstructured sources for assigning clinical codes to patient stays
UR  - <Go to ISI>://WOS:000375292600003
http://jamia.oxfordjournals.org/content/23/e1/e11.long
VL  - 23
ID  - 48
ER  - 


TY  - BOOK
A2  - Fahmy, H. M. A.
A2  - Wahba, A. M.
A2  - ElKharashi, M. W.
A2  - ElDin, A. M. B.
AB  - As research continues to generate vast amounts of data, pertaining to protein interactions, there is a critical need to capture these results in structured formats permitting for computational analysis. Automated the extraction of interactions from unstructured text, would improve the content of databases that store this information and set a method for managing the continued growth of new literature being published. Many algorithms have been reported for extracting biochemical interactions from biomedical text. Natural language processing approaches at various complexity levels have been recorded for extracting biochemical interactions from biomedical text. Some algorithms used simple template matching, others exploit sophisticated parsing techniques. In this paper, we present an automated NLP-based information extraction system, to identify protein interactions in biomedical text. Link grammar parsing can handle many syntactic structures and is computationally relatively efficient. Customizing the parser for the biomedical domain is expected to improve its performance further. Our approach is based on first, tagging biological entities with the help of biomedical and linguistic protein names databases. The system extracts complete interactions by analyzing the matching contents of syntactic roles and their linguistically significant combinations.
AU  - Seoud, R. A. A.
AU  - Youssef, A. B. M.
AU  - Kadah, Y. M.
DA  - 2007
KW  - eppi-reviewer4
N1  - 21445314
9999
Seoud, Rania A. Abul Youssef, Abou-Bakr M. Kadah, Yasser M. International Conference on Computer Engineering and Systems (ICCES 07) NOV 27-29, 2007 ELECTR NETWORK Ain Shams Univ, Fac Engn, Comp Engn & Syst Dept Kadah, Yasser/B-2746-2011 Kadah, Yasser/0000-0002-2166-3191
PY  - 2007
SN  - 978-1-4244-1365-2
SP  - 70-75
ST  - Extraction of protein interaction information from unstructured text using a link grammar parser
T2  - 2007 International Conference on Computer Engineering & Systems: Icces '07
TI  - Extraction of protein interaction information from unstructured text using a link grammar parser
UR  - <Go to ISI>://WOS:000253484700014
ID  - 202
ER  - 


TY  - JOUR
AB  - PURPOSE: The General Practice Research Database (GPRD) is a database of longitudinal patient records from general practices in the United Kingdom. It is an important data source for pharmacoepidemiology studies, but until now it has been tedious to calculate the daily dose and duration of exposure to drugs prescribed. This is because general practitioners routinely record dosage instructions as free text rather than in a structured way. The objective was to develop and assess the validity of an automated algorithm to derive the daily dose from text dosage instructions. METHODS: A computer program was developed to derive numerical information from unstructured text dosage instructions. It was tested on dosage texts from a random sample of one million prescription entries. A random sample of 1,000 of these converted texts were manually checked for their accuracy. RESULTS: Out of the sample of one million prescription entries, 74.5% had text containing the daily dose, 14.5% had text but did not include a quantitative daily dose statement and 11.0% had no text entered. Of the 1000 texts which were checked manually, 767 stated the daily dose. The program interpreted 758 (98.8%) of these correctly, produced errors in four cases and failed to extract the dose from five texts. CONCLUSIONS: An automated algorithm has been developed which can accurately extract the daily dose from almost 99% of general practitioners' text dosage instructions. It increases the utility of GPRD and other prescription data sources by enabling researchers to estimate the duration of drug exposure more efficiently.
AU  - Shah, A. D.
AU  - Martinez, C.
DA  - 2006
IS  - 3
KW  - eppi-reviewer4
N1  - 21443504
6767
Shah, Anoop D Martinez, Carlos Research Support, Non-U.S. Gov't Validation Studies England Pharmacoepidemiol Drug Saf. 2006 Mar;15(3):161-6.
PY  - 2006
SN  - 1053-8569 (Print) 1053-8569 (Linking)
SP  - 161-6
ST  - An algorithm to derive a numerical daily dose from unstructured text dosage instructions
T2  - Pharmacoepidemiol Drug Saf
TI  - An algorithm to derive a numerical daily dose from unstructured text dosage instructions
UR  - http://onlinelibrary.wiley.com/store/10.1002/pds.1151/asset/1151_ftp.pdf?v=1&t=ir53yiq1&s=dcfe4f7c4138d176cf6bc539165fd3ccd9514e6b
VL  - 15
ID  - 150
ER  - 


TY  - JOUR
AB  - Clinical guidelines are a major tool in improving the quality of medical care. However, most guidelines are in free text, are not machine-comprehensible and are not easily accessible to clinicians at the point of care. We have designed and implemented a web-based, modular, distributed architecture, the Digital Electronic Guideline Library (DeGeL), which facilitates gradual conversion of clinical guidelines from text to a formal representation in the chosen target guideline ontology. The architecture supports guideline classification, semantic markup, context-sensitive search, browsing, run-time application and retrospective quality assessment. The DeGeL hybrid meta-ontology includes elements common to all guideline ontologies, such as semantic classification and domain knowledge; it also includes four content-representation formats: free text, semi-structured text, semi-formal representation and a formal representation. These formats support increasingly sophisticated computational tasks. Guidelines can thus be in a hybrid representation in which guidelines, and even parts of the same guideline, might exist at different formalisation levels. We have also developed and rigorously evaluated a methodology and an associated web-based tool, Uruz, for gradually structuring and semi-formalising free-text clinical guidelines. Finally, we have designed, implemented and evaluated a new approach, the hybrid runtime application model, for supporting runtime application of clinical guidelines that are not necessarily in a machine-comprehensible format; in particular, when the guideline is in a semi-formal representation and the patient's data are either in an electronic medical record or in a paper format. The tool implementing this new approach, the Spock module, is customised at this point to the Asbru guideline specification language and exploits the hybrid structure of guidelines in DeGeL. The Spock module also exploits our temporal-abstraction mediator to the patient record, IDAN, and our interactive intelligent-visualisation tool, KNAVE-II.
AU  - Shahar, Y.
DA  - 2006
KW  - eppi-reviewer4
N1  - 21445817
6680
Shahar, Y Italy Neurol Sci. 2006 Jun;27 Suppl 3:S250-3.
PY  - 2006
SN  - 1590-1874 (Print) 1590-1874 (Linking)
SP  - S250-3
ST  - Hybrid specification, storage, retrieval and runtime application of clinical guidelines
T2  - Neurol Sci
TI  - Hybrid specification, storage, retrieval and runtime application of clinical guidelines
UR  - http://download.springer.com/static/pdf/459/art%253A10.1007%252Fs10072-006-0629-4.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10072-006-0629-4&token2=exp=1469638256~acl=%2Fstatic%2Fpdf%2F459%2Fart%25253A10.1007%25252Fs10072-006-0629-4.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs10072-006-0629-4*~hmac=8bb26a1dedcf45ee683614fc867e84a5728dcbbbbce9c4dfc33cab0be653c74e
VL  - 27 Suppl 3
ID  - 77
ER  - 


TY  - JOUR
AB  - OBJECTIVE: As healthcare practice transitions from paper-based to computer-based records, there is increasing need to determine an effective electronic format for clinical narratives. Our research focuses on utilizing a cognitive science methodology to guide the conversion of medical texts to a more structured, user-customized presentation in the electronic medical record (EMR). DESIGN: We studied the use of discharge summaries by psychiatrists with varying expertise-experts, intermediates, and novices. Experts were given two hypothetical emergency care scenarios with narrative discharge summaries and asked to verbalize their clinical assessment. Based on the results, the narratives were presented in a more structured form. Intermediate and novice subjects received a narrative and a structured discharge summary, and were asked to verbalize their assessments of each. MEASUREMENTS: A qualitative comparison of the interview transcripts of all subjects was done by analysis of recall and inference made with respect to level of expertise. RESULTS: For intermediate and novice subjects, recall was greater with the structured form than with the narrative. Novices were also able to make more inferences (not always accurate) from the structured form than with the narrative. Errors occurred in assessments using the narrative form but not the structured form. CONCLUSIONS: Our cognitive methods to study discharge summary use enabled us to extract a conceptual representation of clinical narratives from end-users. This method allowed us to identify clinically relevant information that can be used to structure medical text for the EMR and potentially improve recall and reduce errors.
AU  - Sharda, P.
AU  - Das, A. K.
AU  - Cohen, T. A.
AU  - Patel, V.
DA  - 2006
IS  - 5
KW  - eppi-reviewer4
N1  - 21444481
6769
Sharda, Pallav Das, Amar K Cohen, Trevor A Patel, Vimla Evaluation Studies Research Support, Non-U.S. Gov't Ireland Int J Med Inform. 2006 May;75(5):346-68. Epub 2005 Aug 24.
PY  - 2006
SN  - 1386-5056 (Print) 1386-5056 (Linking)
SP  - 346-68
ST  - Customizing clinical narratives for the electronic medical record interface using cognitive methods
T2  - Int J Med Inform
TI  - Customizing clinical narratives for the electronic medical record interface using cognitive methods
UR  - http://ac.els-cdn.com/S1386505605001425/1-s2.0-S1386505605001425-main.pdf?_tid=823cf304-5417-11e6-8164-00000aab0f6c&acdnat=1469637242_be40288fbf4350d7e362566ee47fb506
VL  - 75
ID  - 108
ER  - 


TY  - JOUR
AB  - Objective: To create software platform to merge public and private datasets from completed Phase II/III ALS clinical trials. Data mining of such Pooled Resource Open-access ALS Clinical Trials (PRO-ACT) Database by research community may help in disease biomarkers discovery, provide insights into design and interpretation of clinical trials and bring us closer to finding a cure and treatment for ALS. Background Large datasets are critical for identifying statistically significant and biologically relevant observations, particularly for diseases resulting from intricate interplay of genetic and environmental factors. PRO-ACT platform provides unique approach for integration, collaboration and analyses of validated datasets of completed industry and academic clinical trials. Design/Methods: Common Data Structure (CDS) approach has been introduced, where Data Dictionaries from multiple trials are analyzed and mapped to CDS and its elements. Resulting Transformation Matrix with a set of rules for mapping fields and field-encoded values between imported clinical trial data and CDS guides data import. As CDS may change, platform's built-in flexibility allows for re-assigning individual fields to different Common Data Elements (CDEs) or sharing individual fields between multiple data elements without dataset re-importation. This approach leads to interoperability, scalability, and potential registration of ALS CDEs with regulatory authorities. Results: Initial dataset will contain approximately 5000 subject records from four major biopharma companies' clinical trials and 1000-plus subject records from academic studies, thus forming largest ever existing ALS clinical record database while making it accessible to research community for analysis. Conclusions: PRO-ACT platform is result of collaboration by industry, academic, and foundation partners. PRO-ACT database is constantly updated to include newly-available datasets. Academic institutions and biopharma companies are encouraged to contribute and analyze data for new insights into ALS. Such approach, including technical and legal methods and recommendations, shall be introduced and promoted for any neurological disease and any disease-specific research community.
AU  - Sherman, A.
AU  - Leitner, M.
AU  - Sinani, E.
AU  - Katsovskiy, I.
AU  - Cudkowicz, M.
DA  - 2012
IS  - 1
KW  - eppi-reviewer4
N1  - 21447373
7667
PY  - 2012
SN  - 0028-3878
ST  - Platform for clinical trials data integration and sharing as a unique industry, academic, and foundation collaboration resource in ALS research
T2  - Neurology
TI  - Platform for clinical trials data integration and sharing as a unique industry, academic, and foundation collaboration resource in ALS research
UR  - http://www.embase.com/search/results?subaction=viewrecord&from=export&id=L70725789 http://dx.doi.org/10.1212/WNL.78.1 http://sfxhosted.exlibrisgroup.com/fda?sid=EMBASE&issn=00283878&id=doi:10.1212%2FWNL.78.1&atitle=Platform+for+clinical+trials+data+integration+and+sharing+as+a+unique+industry%2C+academic%2C+and+foundation+collaboration+resource+in+ALS+research&stitle=Neurology&title=Neurology&volume=78&issue=1&spage=&epage=&aulast=Sherman&aufirst=Alexander&auinit=A.&aufull=Sherman+A.&coden=&is
VL  - 78
ID  - 203
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Prescription drugs can be associated with adverse effects (AEs) that are unrecognized despite evidence in the medical literature, as shown by rofecoxib's late recall in 2004. We assessed whether applying information mining to PubMed could reveal major drug-AE associations if articles testing whether drugs cause AEs are over-represented in the literature. DESIGN: MEDLINE citations published between 1949 and September 2009 were retrieved if they mentioned one of 38 drugs and one of 55 AEs. A statistical document classifier (using MeSH index terms) was constructed to remove irrelevant articles unlikely to test whether a drug caused an AE. The remaining relevant articles were analyzed using a disproportionality analysis that identified drug-AE associations (signals of disproportionate reporting) using step-up procedures developed to control the familywise type I error rate. MEASUREMENTS: Sensitivity and positive predictive value (PPV) for empirical drug-AE associations as judged against drug-AE associations subject to FDA warnings. RESULTS: In testing, the statistical document classifier identified relevant articles with 81% sensitivity and 87% PPV. Using data filtered by the statistical document classifier, base-case models showed 64.9% sensitivity and 42.4% PPV for detecting FDA warnings. Base-case models discovered 54% of all detected FDA warnings using literature published before warnings. For example, the rofecoxib-heart disease association was evident using literature published before 2002. Analyses incorporating literature mentioning AEs common to the drug class of interest yielded 71.4% sensitivity and 40.7% PPV. CONCLUSIONS: Results from large-scale literature retrieval and analysis (literature mining) compared favorably with and could complement current drug safety methods.
AU  - Shetty, K. D.
AU  - Dalal, S. R.
DA  - 2011
DO  - 10.1136/amiajnl-2011-000096. Epub 2011 May 5.
IS  - 5
KW  - eppi-reviewer4
N1  - 21448684
5492
PY  - 2011
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 668-74
ST  - Using information mining of the medical literature to improve drug safety
T2  - J Am Med Inform Assoc
TI  - Using information mining of the medical literature to improve drug safety
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168306/pdf/amiajnl-2011-000096.pdf
VL  - 18
ID  - 14
ER  - 


TY  - JOUR
AB  - The vast quantities of information on toxicogenomics such as genome sequence, genotype, gene expression, phenotype, disease information, etc. are reflected in scientific literature. However, these various and heterogeneous data has to be reconstructed by proper data model to enhance our understanding. This study suggests a semantic modeling to organize heterogeneous data types and introduces techniques and concepts such as ontologies, semantic objects, typed relationships, contexts, graphs, and information layers. These concepts were used to represent complex biochemical networks relationship. The semantic modeling tool is used as an example to demonstrate how a domain such as risk assessment is represented and how this representation is utilized for research. In this work, we show focusing on mRNA centric semantic model as a representative. From experimental data, text-mining results and public databases we generate mRNA-centric semantic modeling and demonstrate its use by mining specific molecular networks together. Application of semantic modeling: 1. Common DEGs which are differentially expressed mRNA by VOCs are identified. 2. Diseases and biological processes associated with common DEGs are identified. 3. Finally subjects associated with common disease, biological process and DEGs are identified.
AU  - Shin, G. H.
AU  - Kang, Y. K.
AU  - Lee, S. H.
AU  - Kim, S. J.
AU  - Hwang, S. Y.
AU  - Nam, S. W.
AU  - Ryu, J. C.
AU  - Kang, B. C.
DA  - 2012
IS  - 1
KW  - eppi-reviewer4
N1  - 21446846
8811
Shin, Ga-Hee Kang, Yeon-Kyung Lee, Seung-Hun Kim, Seung Jun Hwang, Seung Yong Nam, Suk-Woo Ryu, Jae-Chun Kang, Byeong-Chul
PY  - 2012
SN  - 1738-642X
SP  - 35-41
ST  - mRNA-centric semantic modeling for finding molecular signature of trace chemical in human blood
T2  - Molecular & Cellular Toxicology
TI  - mRNA-centric semantic modeling for finding molecular signature of trace chemical in human blood
UR  - <Go to ISI>://WOS:000302592000005
VL  - 8
ID  - 137
ER  - 


TY  - JOUR
AB  - The growing use of computer systems in medical institutions has been generating a tremendous quantity of data. While these data have a critical role in assisting physicians in the clinical practice, the information that can be extracted goes far beyond this utilization. This article proposes a platform capable of assembling multiple data sources within a medical imaging laboratory, through a network of intelligent sensors. The proposed integration framework follows a SOA hybrid architecture based on an information sensor network, capable of collecting information from several sources in medical imaging laboratories. Currently, the system supports three types of sensors: DICOM repository meta-data, network workflows and examination reports. Each sensor is responsible for converting unstructured information from data sources into a common format that will then be semantically indexed in the framework engine. The platform was deployed in the Cardiology department of a central hospital, allowing identification of processes' characteristics and users' behaviours that were unknown before the utilization of this solution.
AU  - Silva, L. A.
AU  - Campos, S.
AU  - Costa, C.
AU  - Oliveira, J. L.
DA  - 2014
DO  - 10.1007/s10916-014-0063-8. Epub 2014 Jun 24.
IS  - 8
KW  - eppi-reviewer4
N1  - 21447999
4406
PY  - 2014
SN  - 1573-689X (Electronic) 0148-5598 (Linking)
SP  - 63
ST  - Sensor-based architecture for medical imaging workflow analysis
T2  - J Med Syst
TI  - Sensor-based architecture for medical imaging workflow analysis
UR  - http://download.springer.com/static/pdf/603/art%253A10.1007%252Fs10916-014-0063-8.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10916-014-0063-8&token2=exp=1469638269~acl=%2Fstatic%2Fpdf%2F603%2Fart%25253A10.1007%25252Fs10916-014-0063-8.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs10916-014-0063-8*~hmac=62fe4f722b651b7dd5cff8770193ef26c84e7759a23950cd025c1591fd6eee06
VL  - 38
ID  - 41
ER  - 


TY  - CHAP
A2  - Nagaveni, N.
A2  - Renuga, R.
A2  - Kunthavai, A.
A2  - Sangeetha, M.
A2  - Ramraj, T.
AB  - Breast cancer is the most common of all cancers and is the leading cause of cancer deaths in women worldwide. It is the second most common cause of cancer death in women. It is common disease in India and accounted that 25% to 31% is the breast cancer in women in Indian cities. Breast cancer is a disease in which malignant (cancer) cells form in the tissues of the breast. The damaged cells can invade surrounding tissue, but in early detection and treatment, it is possible to cure the disease and return to normal life. This article explores knowledge discovery in health care domain. In particular, it discusses about data mining and its application in people who affected severely in breast cancer using Fuzzy Matrix Theory model, it identified the risk factor of which age group of women's associated with the breast cancer impactions. (C) 2015 The Authors. Published by Elsevier B.V.
AU  - Sithic, H. L.
AU  - UmaRani, R.
KW  - eppi-reviewer4
N1  - 21445502
8051
Sithic, H. Lookman UmaRani, R. ICGHIA 2014 International conference on Graph Algorithms, High Performance Implementations and its Applications (ICGHIA) DEC 17-19, 2014 Coimbatore Inst Technol Campus, Coimbatore, INDIA Bergen Univ Coll
PY  - 2015
SN  - 1877-0509
SP  - 282-291
ST  - Fuzzy Matrix Theory as A Knowledge Discovery in Health Care Domain
T2  - Graph Algorithms, High Performance Implementations and Its Applications
TI  - Fuzzy Matrix Theory as A Knowledge Discovery in Health Care Domain
UR  - <Go to ISI>://WOS:000359869000031
VL  - 47
ID  - 49
ER  - 


TY  - JOUR
AB  - Adverse reactions to medications to which the patient was known to be intolerant are common. Electronic decision support can prevent them but only if history of adverse reactions to medications is recorded in structured format. We have conducted a retrospective study of 31,531 patients with adverse reactions to statins documented in the notes, as identified with natural language processing. The software identified statin adverse reactions with sensitivity of 86.5% and precision of 91.9%. Only 9020 of these patients had an adverse reaction to a statin recorded in structured format. In multivariable analysis the strongest predictor of structured documentation was utilization of EMR functionality that integrated the medication list with the structured medication adverse reaction repository (odds ratio 48.6, p < 0.0001). Integration of information flow between EMR modules can help improve documentation and potentially prevent adverse drug events.
AU  - Skentzos, S.
AU  - Shubina, M.
AU  - Plutzky, J.
AU  - Turchin, A.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21448230
5298
Skentzos, Stephen Shubina, Maria Plutzky, Jorge Turchin, Alexander 1RC1LM010460/LM/NLM NIH HHS/United States Research Support, N.I.H., Extramural Validation Studies United States AMIA Annu Symp Proc. 2011;2011:1270-9. Epub 2011 Oct 22.
PY  - 2011
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1270-9
ST  - Structured vs. unstructured: factors affecting adverse drug reaction documentation in an EMR repository
T2  - AMIA Annu Symp Proc
TI  - Structured vs. unstructured: factors affecting adverse drug reaction documentation in an EMR repository
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3243255/pdf/1270_amia_2011_proc.pdf
VL  - 2011
ID  - 50
ER  - 


TY  - JOUR
AB  - Information extraction is an important text-mining task that aims at extracting prespecified types of information from large text collections and making them available in structured representations such as databases. In the biomedical domain, information extraction can be applied to help biologists make the most use of their digital-literature archives. Currently, there are large amounts of biomedical literature that contain rich information about biomedical substances. Extracting such knowledge requires a good named entity recognition technique. In this article, we combine conditional random fields (CRFs), a state-of-the-art sequence-labeling algorithm, with two semisupervised learning techniques, bootstrapping and feature sampling, to recognize disease names from biomedical literature. Two data-processing strategies for each technique also were analyzed: one sequentially processing unlabeled data partitions and another one processing unlabeled data partitions in a round-robin fashion. The experimental results showed the advantage of semisupervised learning techniques given limited labeled training data. Specifically, CRFs with bootstrapping implemented in sequential fashion outperformed strictly supervised CRFs for disease name recognition.
AU  - Suakkaphong, N.
AU  - Zhang, Z.
AU  - Chen, H. C.
DA  - 2011
IS  - 4
KW  - eppi-reviewer4
N1  - 21444818
9538
Suakkaphong, Nichalin Zhang, Zhu Chen, Hsinchun
PY  - 2011
SN  - 1532-2882
SP  - 727-737
ST  - Disease Named Entity Recognition Using Semisupervised Learning and Conditional Random Fields
T2  - Journal of the American Society for Information Science and Technology
TI  - Disease Named Entity Recognition Using Semisupervised Learning and Conditional Random Fields
UR  - <Go to ISI>://WOS:000288525000009
VL  - 62
ID  - 177
ER  - 


TY  - JOUR
AB  - Lung cancer is one of the most common forms of cancer resulting in over a million deaths per year worldwide. In this paper, the usage of support vector machine (SVM) classification for lung cancer is investigated, presenting a systematic quantitative evaluation against Boosting, Decision trees, k-nearest neighbor, LASSO regressions, neural networks and random forests. A large database of 5984 regions of interest (ROIs) and 488 input features (including textural features, patient characteristics, and morphological features) were used to train the classifiers and evaluate for their performance. The evaluation for classifiers' performance was based on a tenfold cross validation framework, receiver operating characteristic curve (ROC), and Matthews correlation coefficient. Area under curve (AUC) of SVM, Boosting, Decision trees, k-nearest neighbor, LASSO, neural networks, random forests were 0.94, 0.86, 0.73, 0.72, 0.91, 0.92, and 0.85, respectively. It was proved that SVM classification offered significantly increased classification performance compared to the reference methods. This scheme may be used as an auxiliary tool to differentiate between benign and malignant SPNs of CT images in future.
AU  - Sun, T.
AU  - Wang, J.
AU  - Li, X.
AU  - Lv, P.
AU  - Liu, F.
AU  - Luo, Y.
AU  - Gao, Q.
AU  - Zhu, H.
AU  - Guo, X.
DA  - 2013
DO  - 10.1016/j.cmpb.2013.04.016. Epub 2013 May 31.
IS  - 2
KW  - eppi-reviewer4
N1  - 21444198
4803
PY  - 2013
SN  - 1872-7565 (Electronic) 0169-2607 (Linking)
SP  - 519-24
ST  - Comparative evaluation of support vector machines for computer aided diagnosis of lung cancer in CT based on a multi-dimensional data set
T2  - Comput Methods Programs Biomed
TI  - Comparative evaluation of support vector machines for computer aided diagnosis of lung cancer in CT based on a multi-dimensional data set
UR  - http://ac.els-cdn.com/S0169260713001387/1-s2.0-S0169260713001387-main.pdf?_tid=8871c0ec-5417-11e6-bcc4-00000aab0f26&acdnat=1469637253_9a3152544ef00995b93c2673a08d1e5c
VL  - 111
ID  - 120
ER  - 


TY  - JOUR
AB  - Capture and representation of scientific knowledge in a structured format are essential to improve the understanding of biological mechanisms involved in complex diseases. Biological knowledge and knowledge about standardized terminologies are difficult to capture from literature in a usable form. A semi-automated knowledge extraction workflow is presented that was developed to allow users to extract causal and correlative relationships from scientific literature and to transcribe them into the computable and human readable Biological Expression Language (BEL). The workflow combines state-of-the-art linguistic tools for recognition of various entities and extraction of knowledge from literature sources. Unlike most other approaches, the workflow outputs the results to a curation interface for manual curation and converts them into BEL documents that can be compiled to form biological networks. We developed a new semi-automated knowledge extraction workflow that was designed to capture and organize scientific knowledge and reduce the required curation skills and effort for this task. The workflow was used to build a network that represents the cellular and molecular mechanisms implicated in atherosclerotic plaque destabilization in an apolipoprotein-E-deficient (ApoE(-/-)) mouse model. The network was generated using knowledge extracted from the primary literature. The resultant atherosclerotic plaque destabilization network contains 304 nodes and 743 edges supported by 33 PubMed referenced articles. A comparison between the semi-automated and conventional curation processes showed similar results, but significantly reduced curation effort for the semi-automated process. Creating structured knowledge from unstructured text is an important step for the mechanistic interpretation and reusability of knowledge. Our new semi-automated knowledge extraction workflow reduced the curation skills and effort required to capture and organize scientific knowledge. The atherosclerotic plaque destabilization network that was generated is a causal network model for vascular disease demonstrating the usefulness of the workflow for knowledge extraction and construction of mechanistically meaningful biological networks.
AU  - Szostak, J.
AU  - Ansari, S.
AU  - Madan, S.
AU  - Fluck, J.
AU  - Talikka, M.
AU  - Iskandar, A.
AU  - De, Leon
AU  - Hofmann-Apitius, M.
AU  - Peitsch, M. C.
AU  - Hoeng, J.
DA  - 2015
DO  - 10.1093/database/bav057.
KW  - eppi-reviewer4
N1  - 21444373
3953
PY  - 2015
SN  - 1758-0463 (Electronic) 1758-0463 (Linking)
SP  - bav057
ST  - Construction of biological networks from unstructured information based on a semi-automated curation workflow
T2  - Database (Oxford)
TI  - Construction of biological networks from unstructured information based on a semi-automated curation workflow
UR  - http://database.oxfordjournals.org/content/2015/bav057.full.pdf
VL  - 2015
ID  - 121
ER  - 


TY  - JOUR
AB  - This paper presents experiments in automatic Information Extraction of medication events, diagnoses, and laboratory tests form hospital patient records, in order to increase the completeness of the description of the episode of care. Each patient record in our hospital information system contains structured data and text descriptions, including full discharge letters. From these letters, we extract automatically information about the medication just before and in the time of hospitalization, especially for the drugs prescribed to the patient, but not delivered by the hospital pharmacy; we also extract values of lab tests not performed and not registered in our laboratory as well as all non-encoded diagnoses described only in the free text of discharge letters. Thus we increase the availability of suitable and accurate information about the hospital stay and the outpatient segment of care before the hospitalization. Information Extraction also helps to understand the clinical and organizational decisions concerning the patient without increasing the complexity of the structured health record.
AU  - Tchraktchiev, D.
AU  - Angelova, G.
AU  - Boytcheva, S.
AU  - Angelov, Z.
AU  - Zacharieva, S.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21444276
5462
Tchraktchiev, Dimitar Angelova, Galia Boytcheva, Svetla Angelov, Zhivko Zacharieva, Sabina Research Support, Non-U.S. Gov't Netherlands Stud Health Technol Inform. 2011;166:260-9.
PY  - 2011
SN  - 0926-9630 (Print) 0926-9630 (Linking)
SP  - 260-9
ST  - Completion of structured patient descriptions by semantic mining
T2  - Stud Health Technol Inform
TI  - Completion of structured patient descriptions by semantic mining
VL  - 166
ID  - 122
ER  - 


TY  - CHAP
A2  - Adlassnig, K. P.
A2  - Blobel, B.
A2  - Mantas, J.
A2  - Masic, I.
AB  - This paper proposes a PDA-based system, SERVANDO, for the home-care management of patients with COPD. In addition to making the supervision of such patients easier, the system permits the collection and structuring of large quantities of information on the evolution of COPD, allowing the application of data-mining techniques, which could open up new lines of research on this disease.
AU  - Teijeiro, T.
AU  - Tarasco, M.
AU  - Felix, P.
AU  - Presedo, J. M.
KW  - eppi-reviewer4
N1  - 21445776
9197
Teijeiro, Tomas Tarasco, Miguel Felix, Paulo Maria Presedo, Jesus 22nd International Congress of the European-Federation-for-Medical-Informatics on Medical Informatics Europe (MIE) AUG 30-SEP 02, 2009 Sarajevo, BOSNIA & HERCEG European Federat Med Informat, Soc Med Informat Bosnia & Herzegovina Felix, Paulo/K-9352-2014
PY  - 2009
SN  - 0926-9630 978-1-60750-456-6
SP  - 439-439
ST  - A Home-Care PDA-Based Program for the Management of COPD Patients
T2  - Medical Informatics in a United and Healthy Europe
TI  - A Home-Care PDA-Based Program for the Management of COPD Patients
UR  - <Go to ISI>://WOS:000326889500096
VL  - 150
ID  - 15
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To propose a method for standardised data representation and demonstrate a technology that makes it possible to translate data from device dependent formats to this standard representation format. METHODS AND RESULTS: Outcome statistics vary between emergency medical systems organising resuscitation services. Such differences indicate a potential for improvement by identifying factors affecting outcome, but data subject to analysis have to be comparable. Modern technology for communicating information makes it possible to structure, store and transfer data flexibly. Ontologies describe entities in the world and how they relate. Letting different computer systems refer to the same ontology results in a common understanding on data content. Information on therapy such as shock delivery, chest compressions and ventilation should be defined and described in a standardised ontology to enable comparison and combining data from diverse sources. By adding rules and logic data can be merged and combined in new ways to produce new information. An example ontology is designed to demonstrate the feasibility and value of such a standardised structure. CONCLUSIONS: The proposed technology makes possible capturing and storing of data from different devices in a structured and standardised format. Data can easily be transformed to this standardised format, compared and combined independent of the original structure.
AU  - Thorsen, K. A.
AU  - Eftestol, T.
AU  - Tossebro, E.
AU  - Rong, C.
AU  - Steen, P. A.
DA  - 2009
DO  - 10.1016/j.resuscitation.2008.12.018. Epub 2009 Feb 26.
IS  - 5
KW  - eppi-reviewer4
N1  - 21448697
6087
PY  - 2009
SN  - 0300-9572 (Print) 0300-9572 (Linking)
SP  - 511-6
ST  - Using ontologies to integrate and share resuscitation data from diverse medical devices
T2  - Resuscitation
TI  - Using ontologies to integrate and share resuscitation data from diverse medical devices
UR  - http://ac.els-cdn.com/S0300957209000513/1-s2.0-S0300957209000513-main.pdf?_tid=8c941d5a-5417-11e6-b186-00000aacb360&acdnat=1469637260_e05b90af640f7e409024d828a78dab45
VL  - 80
ID  - 78
ER  - 


TY  - JOUR
AB  - A growing number of international initiatives (e.g. EU-ADR, Sentinel, OMOP, PROTECT and VAESCO) are based on the combined use of multiple healthcare databases for the conduct of active surveillance studies in the area of drug and vaccine safety. The motivation behind combining multiple healthcare databases is the earlier detection and validation, and hence earlier management, of potential safety issues. Overall, the combination of multiple healthcare databases increases statistical sample size and heterogeneity of exposure for postmarketing drug and vaccine safety surveillance, despite posing several technical challenges. Healthcare databases generally differ by underlying healthcare systems, type of information collected, drug/vaccine and medical event coding systems and language. Therefore, harmonization of medical data extraction through homogeneous coding algorithms across highly different databases is necessary. Although no standard procedure is currently available to achieve this, several approaches have been developed in recent projects. Another main challenge involves choosing the work models for data management and analyses whilst respecting country-specific regulations in terms of data privacy and anonymization. Dedicated software (e.g. Jerboa) has been produced to deal with privacy issues by sharing only anonymized and aggregated data using a common data model. Finally, storage and safe access to the data from different databases requires the development of a proper remote research environment. The aim of this review is to provide a summary of the potential, disadvantages, methodological issues and possible solutions concerning the conduct of postmarketing multidatabase drug and vaccine safety studies, as demonstrated by several international initiatives.
AU  - Trifiro, G.
AU  - Coloma, P. M.
AU  - Rijnbeek, P. R.
AU  - Romio, S.
AU  - Mosseveld, B.
AU  - Weibel, D.
AU  - Bonhoeffer, J.
AU  - Schuemie, M.
AU  - van der, Lei
AU  - Sturkenboom, M.
DA  - 2014
DO  - 10.1111/joim.12159.
IS  - 6
KW  - eppi-reviewer4
N1  - 21444146
4499
PY  - 2014
SN  - 1365-2796 (Electronic) 0954-6820 (Linking)
SP  - 551-61
ST  - Combining multiple healthcare databases for postmarketing drug and vaccine safety surveillance: why and how?
T2  - J Intern Med
TI  - Combining multiple healthcare databases for postmarketing drug and vaccine safety surveillance: why and how?
UR  - http://onlinelibrary.wiley.com/store/10.1111/joim.12159/asset/joim12159.pdf?v=1&t=ir53z2g0&s=76f476d5dd550d4e28e72b255007587aae5495b4
VL  - 275
ID  - 32
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To determine whether specific design interventions (changes in the user interface (UI)) of an electronic health record (EHR) medication module are associated with an increase or decrease in the incidence of contradictions between the structured and narrative components of electronic prescriptions (internal prescription discrepancies). MATERIALS AND METHODS: We performed a retrospective analysis of 960,000 randomly selected electronic prescriptions generated in a single EHR between 01/2004 and 12/2011. Internal prescription discrepancies were identified using a validated natural language processing tool with recall of 76% and precision of 84%. A multivariable autoregressive integrated moving average (ARIMA) model was used to evaluate the effect of five UI changes in the EHR medication module on incidence of internal prescription discrepancies. RESULTS: Over the study period 175,725 (18.4%) prescriptions were found to have internal discrepancies. The highest rate of prescription discrepancies was observed in March 2006 (22.5%) and the lowest in March 2009 (15.0%). Addition of "as directed" option to the <Frequency> dropdown decreased prescription discrepancies by 195 / month (p = 0.0004). An non-interruptive alert that reminded providers to ensure that structured and narrative components did not contradict each other decreased prescription discrepancies by 145 / month (p = 0.03). Addition of a "Renew / Sign" button to the Medication module (a negative control) did not have an effect in prescription discrepancies. CONCLUSIONS: Several UI changes in the electronic medication module were effective in reducing the incidence of internal prescription discrepancies. Further research is needed to identify interventions that can completely eliminate this type of prescription error and their effects on patient outcomes.
AU  - Turchin, A.
AU  - Sawarkar, A.
AU  - Dementieva, Y. A.
AU  - Breydo, E.
AU  - Ramelson, H.
DA  - 2014
DO  - 10.4338/ACI-2014-03-RA-0023. eCollection 2014.
IS  - 3
KW  - eppi-reviewer4
N1  - 21444914
4254
PY  - 2014
SN  - 1869-0327 (Electronic)
SP  - 708-20
ST  - Effect of EHR user interface changes on internal prescription discrepancies
T2  - Appl Clin Inform
TI  - Effect of EHR user interface changes on internal prescription discrepancies
UR  - https://aci.schattauer.de/contents/archive/issue/1884/manuscript/22256/download.html
VL  - 5
ID  - 138
ER  - 


TY  - BOOK
A2  - Yang, M. Q.
A2  - Zhu, M. M.
A2  - Zhang, Y.
A2  - Arabnia, H. R.
A2  - Deng, Y.
A2  - Bourbakis, N.
AB  - We present an information retrieval model for combining evidence from concept-based semantics, term statistics, and context for improving search precision of genomics literature by accurately identifying concise, variable length passages of text to answer a user query. The system combines a dimensional data model for indexing scientific literature at multiple levels of document structure and context with a rule-based query processing algorithm. The query processing algorithm uses an iterative information extraction technique to identify query concepts, and a retrieval function for systematically combining concepts with term statistics at multiple levels of context. We define context by variable length passages of text and different levels of document lexical structure including terms, sentences, paragraphs, and entire documents. Our results demonstrate improved search results in the presence of varying levels of semantic evidence, and higher performance using retrieval functions that combine document as well as sentence and passage level information versus using document, sentence or passage level information alone. Initial results are promising. When ranking documents based on the most relevant extracted passages, the results exceed the state-of-the-art by 13.89% as assessed by the TREC 2005 Genomics track collection of 4.5 million MEDLINE citations.
AU  - Urbain, J.
AU  - Goharian, N.
AU  - Frieder, O.
AU  - Yang, J. Y.
DA  - 2007
KW  - eppi-reviewer4
N1  - 21444151
10094
Urbain, Jay Goharian, Nazli Frieder, Ophir 7th IEEE International Conference on Bioinformatics and Bioengineering OCT 14-17, 2007 Boston, MA IEEE, IEEE Comp Soc, IEEE Engn Med Biol, NSF, Int Soc Intelligent Biol Med, Syst, Man, Cybermet Soc
PY  - 2007
SN  - 978-1-4244-1509-0
SP  - 1313-1317
ST  - Combining semantics, context, and statistical evidence in genomics literature search
T2  - Proceedings of the 7th Ieee International Symposium on Bioinformatics and Bioengineering, Vols I and Ii
TI  - Combining semantics, context, and statistical evidence in genomics literature search
UR  - <Go to ISI>://WOS:000252958200210
ID  - 151
ER  - 


TY  - JOUR
AB  - OBJECTIVE: To synthesise recent research on the use of machine learning approaches to mining textual injury surveillance data. DESIGN: Systematic review. DATA SOURCES: The electronic databases which were searched included PubMed, Cinahl, Medline, Google Scholar, and Proquest. The bibliography of all relevant articles was examined and associated articles were identified using a snowballing technique. SELECTION CRITERIA: For inclusion, articles were required to meet the following criteria: (a) used a health-related database, (b) focused on injury-related cases, AND used machine learning approaches to analyse textual data. METHODS: The papers identified through the search were screened resulting in 16 papers selected for review. Articles were reviewed to describe the databases and methodology used, the strength and limitations of different techniques, and quality assurance approaches used. Due to heterogeneity between studies meta-analysis was not performed. RESULTS: Occupational injuries were the focus of half of the machine learning studies and the most common methods described were Bayesian probability or Bayesian network based methods to either predict injury categories or extract common injury scenarios. Models were evaluated through either comparison with gold standard data or content expert evaluation or statistical measures of quality. Machine learning was found to provide high precision and accuracy when predicting a small number of categories, was valuable for visualisation of injury patterns and prediction of future outcomes. However, difficulties related to generalizability, source data quality, complexity of models and integration of content and technical knowledge were discussed. CONCLUSIONS: The use of narrative text for injury surveillance has grown in popularity, complexity and quality over recent years. With advances in data mining techniques, increased capacity for analysis of large databases, and involvement of computer scientists in the injury prevention field, along with more comprehensive use and description of quality assurance methods in text mining approaches, it is likely that we will see a continued growth and advancement in knowledge of text mining in the injury field.
AU  - Vallmuur, K.
DA  - 2015
DO  - 10.1016/j.aap.2015.03.018. Epub 2015 Mar 19.
KW  - eppi-reviewer4
N1  - 21446435
4072
PY  - 2015
SN  - 1879-2057 (Electronic) 0001-4575 (Linking)
SP  - 41-9
ST  - Machine learning approaches to analysing textual injury surveillance data: a systematic review
T2  - Accid Anal Prev
TI  - Machine learning approaches to analysing textual injury surveillance data: a systematic review
UR  - http://ac.els-cdn.com/S0001457515000925/1-s2.0-S0001457515000925-main.pdf?_tid=94d55f56-5417-11e6-ab3e-00000aacb362&acdnat=1469637274_920c4e36758dd93be6a372ab229c0f3f
VL  - 79
ID  - 33
ER  - 


TY  - JOUR
AB  - MOTIVATION: In the field of biomolecular text mining, black box behavior of machine learning systems currently limits understanding of the true nature of the predictions. However, feature selection (FS) is capable of identifying the most relevant features in any supervised learning setting, providing insight into the specific properties of the classification algorithm. This allows us to build more accurate classifiers while at the same time bridging the gap between the black box behavior and the end-user who has to interpret the results. RESULTS: We show that our FS methodology successfully discards a large fraction of machine-generated features, improving classification performance of state-of-the-art text mining algorithms. Furthermore, we illustrate how FS can be applied to gain understanding in the predictions of a framework for biomolecular event extraction from text. We include numerous examples of highly discriminative features that model either biological reality or common linguistic constructs. Finally, we discuss a number of insights from our FS analyses that will provide the opportunity to considerably improve upon current text mining tools. AVAILABILITY: The FS algorithms and classifiers are available in Java-ML (http://java-ml.sf.net). The datasets are publicly available from the BioNLP'09 Shared Task web site (http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/).
AU  - Van, Landeghem
AU  - Abeel, T.
AU  - Saeys, Y.
AU  - Van de, Peer
DA  - 2010
DO  - 10.1093/bioinformatics/btq381.
IS  - 18
KW  - eppi-reviewer4
N1  - 21444814
5688
PY  - 2010
SN  - 1367-4811 (Electronic) 1367-4803 (Linking)
SP  - i554-60
ST  - Discriminative and informative features for biomolecular text mining with ensemble feature selection
T2  - Bioinformatics
TI  - Discriminative and informative features for biomolecular text mining with ensemble feature selection
VL  - 26
ID  - 173
ER  - 


TY  - JOUR
AB  - Corpora with specific entities and relationships annotated are essential to train and evaluate text-mining systems that are developed to extract specific structured information from a large corpus. In this paper we describe an approach where a named-entity recognition system produces a first annotation and annotators revise this annotation using a web-based interface. The agreement figures achieved show that the inter-annotator agreement is much better than the agreement with the system provided annotations. The corpus has been annotated for drugs, disorders, genes and their inter-relationships. For each of the drug-disorder, drug-target, and target-disorder relations three experts have annotated a set of 100 abstracts. These annotated relationships will be used to train and evaluate text-mining software to capture these relationships in texts.
AU  - van, Mulligen
AU  - E, M.
AU  - Fourrier-Reglat, A.
AU  - Gurwitz, D.
AU  - Molokhia, M.
AU  - Nieto, A.
AU  - Trifiro, G.
AU  - Kors, J. A.
AU  - Furlong, L. I.
DA  - 2012
DO  - 10.1016/j.jbi.2012.04.004. Epub 2012 Apr 25.
IS  - 5
KW  - eppi-reviewer4
N1  - 21445119
5184
PY  - 2012
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 879-84
ST  - The EU-ADR corpus: annotated drugs, diseases, targets, and their relationships
T2  - J Biomed Inform
TI  - The EU-ADR corpus: annotated drugs, diseases, targets, and their relationships
VL  - 45
ID  - 72
ER  - 


TY  - JOUR
AB  - OBJECTIVE: This study evaluated a computerized method for extracting numeric clinical measurements related to diabetes care from free text in electronic patient records (EPR) of general practitioners. DESIGN AND MEASUREMENTS: Accuracy of this number-oriented approach was compared to manual chart abstraction. Audits measured performance in clinical practice for two commonly used electronic record systems. RESULTS: Numeric measurements embedded within free text of the EPRs constituted 80% of relevant measurements. For 11 of 13 clinical measurements, the study extraction method was 94%-100% sensitive with a positive predictive value (PPV) of 85%-100%. Post-processing increased sensitivity several points and improved PPV to 100%. Application in clinical practice involved processing times averaging 7.8 minutes per 100 patients to extract all relevant data. CONCLUSION: The study method converted numeric clinical information to structured data with high accuracy, and enabled research and quality of care assessments for practices lacking structured data entry.
AU  - Voorham, J.
AU  - Denig, P.
DA  - 2007
IS  - 3
KW  - eppi-reviewer4
N1  - 21444320
6546
Voorham, Jaco Denig, Petra Evaluation Studies Research Support, Non-U.S. Gov't United States J Am Med Inform Assoc. 2007 May-Jun;14(3):349-54. Epub 2007 Feb 28.
PY  - 2007
SN  - 1067-5027 (Print) 1067-5027 (Linking)
SP  - 349-54
ST  - Computerized extraction of information on the quality of diabetes care from free text in electronic patient records of general practitioners
T2  - J Am Med Inform Assoc
TI  - Computerized extraction of information on the quality of diabetes care from free text in electronic patient records of general practitioners
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2244890/pdf/349.S1067502707000540.main.pdf
VL  - 14
ID  - 139
ER  - 


TY  - JOUR
AB  - Purpose: To describe how computer-assisted presentation of case data can lead experts to infer machine-implementable rules for case definition in electronic health records. As an illustration the technique has been applied to obtain a definition of acute liver dysfunction (ALD) in persons with inflammatory bowel disease (IBD). Methods: The technique consists of repeatedly sampling new batches of case candidates from an enriched pool of persons meeting presumed minimal inclusion criteria, classifying the candidates by a machine-implementable candidate rule and by a human expert, and then updating the rule so that it captures new distinctions introduced by the expert. Iteration continues until an update results in an acceptably small number of changes to form a final case definition. Results: The technique was applied to structured data and terms derived by natural language processing from text records in 29,336 adults with IBD. Over three rounds the technique led to rules with increasing predictive value, as the experts identified exceptions, and increasing sensitivity, as the experts identified missing inclusion criteria. In the final rule inclusion and exclusion terms were often keyed to an ALD onset date. When compared against clinical review in an independent test round, the derived final case definition had a sensitivity of 92% and a positive predictive value of 79%. Conclusion: An iterative technique of machine-supported expert review can yield a case definition that accommodates available data, incorporates pre-existing medical knowledge, is transparent and is open to continuous improvement. The expert updates to rules may be informative in themselves. In this limited setting, the final case definition for ALD performed better than previous, published attempts using expert definitions. (C) 2015 Elsevier Ireland Ltd. All rights reserved.
AU  - Walker, A. M.
AU  - Zhou, X. F.
AU  - Ananthakrishnan, A. N.
AU  - Weiss, L. S.
AU  - Shen, R. J.
AU  - Sobel, R. E.
AU  - Bate, A.
AU  - Reynolds, R. F.
DA  - 2016
KW  - eppi-reviewer4
N1  - 21444318
7754
Walker, Alexander M. Zhou, Xiaofeng Ananthakrishnan, Ashwin N. Weiss, Lisa S. Shen, Rongjun Sobel, Rachel E. Bate, Andrew Reynolds, Robert F. Walker, Alexander/0000-0001-9738-5127; Sobel, Rachel/0000-0002-4492-9562
PY  - 2016
SN  - 1386-5056
SP  - 62-70
ST  - Computer-assisted expert case definition in electronic health records
T2  - International Journal of Medical Informatics
TI  - Computer-assisted expert case definition in electronic health records
UR  - <Go to ISI>://WOS:000367357000009
http://ac.els-cdn.com/S1386505615300484/1-s2.0-S1386505615300484-main.pdf?_tid=9ba136fc-5417-11e6-82e8-00000aab0f01&acdnat=1469637285_7e81f7ec31fbb64f11b80072c19e500a
VL  - 86
ID  - 167
ER  - 


TY  - JOUR
AB  - In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers. (c) 2012 Published by Elsevier B.V.
AU  - Wang, S. J.
AU  - Summers, R. M.
DA  - 2012
IS  - 5
KW  - eppi-reviewer4
N1  - 21446431
8776
Wang, Shijun Summers, Ronald M.
PY  - 2012
SN  - 1361-8415
SP  - 933-951
ST  - Machine learning and radiology
T2  - Medical Image Analysis
TI  - Machine learning and radiology
UR  - <Go to ISI>://WOS:000306390300001
http://ac.els-cdn.com/S1361841512000333/1-s2.0-S1361841512000333-main.pdf?_tid=9f22ee2e-5417-11e6-bed6-00000aacb360&acdnat=1469637291_1a1bf831970a08211143c6e0d47a1ca3
VL  - 16
ID  - 73
ER  - 


TY  - JOUR
AB  - BACKGROUND: The number of corpora, collections of structured texts, has been increasing, as a result of the growing interest in the application of natural language processing methods to biological texts. Many named entity recognition (NER) systems have been developed based on these corpora. However, in the biomedical community, there is yet no general consensus regarding named entity annotation; thus, the resources are largely incompatible, and it is difficult to compare the performance of systems developed on resources that were divergently annotated. On the other hand, from a practical application perspective, it is desirable to utilize as many existing annotated resources as possible, because annotation is costly. Thus, it becomes a task of interest to integrate the heterogeneous annotations in these resources. RESULTS: We explore the potential sources of incompatibility among gene and protein annotations that were made for three common corpora: GENIA, GENETAG and AIMed. To show the inconsistency in the corpora annotations, we first tackle the incompatibility problem caused by corpus integration, and we quantitatively measure the effect of this incompatibility on protein mention recognition. We find that the F-score performance declines tremendously when training with integrated data, instead of training with pure data; in some cases, the performance drops nearly 12%. This degradation may be caused by the newly added heterogeneous annotations, and cannot be fixed without an understanding of the heterogeneities that exist among the corpora. Motivated by the result of this preliminary experiment, we further qualitatively analyze a number of possible sources for these differences, and investigate the factors that would explain the inconsistencies, by performing a series of well-designed experiments. Our analyses indicate that incompatibilities in the gene/protein annotations exist mainly in the following four areas: the boundary annotation conventions, the scope of the entities of interest, the distribution of annotated entities, and the ratio of overlap between annotated entities. We further suggest that almost all of the incompatibilities can be prevented by properly considering the four aspects aforementioned. CONCLUSION: Our analysis covers the key similarities and dissimilarities that exist among the diverse gene/protein corpora. This paper serves to improve our understanding of the differences in the three studied corpora, which can then lead to a better understanding of the performance of protein recognizers that are based on the corpora.
AU  - Wang, Y.
AU  - Kim, J. D.
AU  - Saetre, R.
AU  - Pyysalo, S.
AU  - Tsujii, J.
DA  - 2009
DO  - 10.1186/1471-2105-10-403.
KW  - eppi-reviewer4
N1  - 21446204
5898
PY  - 2009
SN  - 1471-2105 (Electronic) 1471-2105 (Linking)
SP  - 403
ST  - Investigating heterogeneous protein annotations toward cross-corpora utilization
T2  - BMC Bioinformatics
TI  - Investigating heterogeneous protein annotations toward cross-corpora utilization
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2804683/pdf/1471-2105-10-403.pdf
VL  - 10
ID  - 88
ER  - 


TY  - JOUR
AB  - Automatic diagnosis is one of the most important parts in the expert system of traditional Chinese medicine (TCM), and in recent years, it has been studied widely. Most of the previous researches are based on well-structured datasets which are manually collected, structured and normalized by TCM experts. However, the obtained results of the former work could not be directly and effectively applied to clinical practice, because the raw free-text clinical records differ a lot from the well-structured datasets. They are unstructured and are denoted by TCM doctors without the support of authoritative editorial board in their routine diagnostic work. Therefore, in this paper, a novel framework of automatic diagnosis of TCM utilizing raw free-text clinical records for clinical practice is proposed and investigated for the first time. A series of appropriate methods are attempted to tackle several challenges in the framework, and the Naive Bayes classifier and the Support Vector Machine classifier are employed for TCM automatic diagnosis. The framework is analyzed carefully. Its feasibility is validated through evaluating the performance of each module of the framework and its effectiveness is demonstrated based on the precision, recall and F-Measure of automatic diagnosis results.
AU  - Wang, Y.
AU  - Yu, Z.
AU  - Jiang, Y.
AU  - Liu, Y.
AU  - Chen, L.
AU  - Liu, Y.
DA  - 2012
DO  - 10.1016/j.jbi.2011.10.010. Epub 2011 Nov 10.
IS  - 2
KW  - eppi-reviewer4
N1  - 21445446
5337
PY  - 2012
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 210-23
ST  - A framework and its empirical study of automatic diagnosis of traditional Chinese medicine utilizing raw free-text clinical records
T2  - J Biomed Inform
TI  - A framework and its empirical study of automatic diagnosis of traditional Chinese medicine utilizing raw free-text clinical records
UR  - http://ac.els-cdn.com/S1532046411001766/1-s2.0-S1532046411001766-main.pdf?_tid=a2fc897e-5417-11e6-bcc4-00000aab0f26&acdnat=1469637297_b55ad32c09fc306ec5b3fab28e84051f
VL  - 45
ID  - 129
ER  - 


TY  - JOUR
AB  - BACKGROUND: Studies in individuals or small kindreds have implicated rare variants in 25 different genes in lone and familial atrial fibrillation (AF) using linkage and segregation analysis, functional characterization, and rarity in public databases. Here, we used a cohort of 20 204 patients of European or African ancestry with electronic medical records and exome chip data to compare the frequency of AF among carriers and noncarriers of these rare variants. METHODS AND RESULTS: The exome chip included 19 of 115 rare variants, in 9 genes, previously associated with lone or familial AF. Using validated algorithms querying a combination of clinical notes, structured billing codes, ECG reports, and procedure codes, we identified 1056 AF cases (>18 years) and 19 148 non-AF controls (>50 years) with available genotype data on the Illumina HumanExome BeadChip v.1.0 in the Vanderbilt electronic medical record-linked DNA repository, BioVU. Known correlations between AF and common variants at 4q25 were replicated. None of the 19 variants previously associated with AF were over-represented among AF cases (P>0.1 for all), and the frequency of variant carriers among non-AF controls was >0.1% for 14 of 19. Repeat analyses using non-AF controls aged >60 (n=14 904), >70 (n=9670), and >80 (n=4729) years did not influence these findings. CONCLUSIONS: Rare variants previously implicated in lone or familial forms of AF present on the exome chip are detected at low frequencies in a general population but are not associated with AF. These findings emphasize the need for caution when ascribing variants as pathogenic or causative.
AU  - Weeke, P.
AU  - Denny, J. C.
AU  - Basterache, L.
AU  - Shaffer, C.
AU  - Bowton, E.
AU  - Ingram, C.
AU  - Darbar, D.
AU  - Roden, D. M.
DA  - 2015
DO  - 10.1161/CIRCGENETICS.114.000718. Epub 2014 Nov 19.
IS  - 1
KW  - eppi-reviewer4
N1  - 21445222
4216
PY  - 2015
SN  - 1942-3268 (Electronic) 1942-3268 (Linking)
SP  - 58-63
ST  - Examining rare and low-frequency genetic variants previously associated with lone or familial forms of atrial fibrillation in an electronic medical record system: a cautionary note
T2  - Circ Cardiovasc Genet
TI  - Examining rare and low-frequency genetic variants previously associated with lone or familial forms of atrial fibrillation in an electronic medical record system: a cautionary note
UR  - http://circgenetics.ahajournals.org/content/8/1/58.full.pdf
VL  - 8
ID  - 3
ER  - 


TY  - CHAP
A2  - Morvan, F.
A2  - Tjoa, A. M.
A2  - Wagner, R. R.
AB  - The wide spread of electronic data collection in medical environments leads to an exponential growth of clinical data extracted from heterogeneous patient samples. Collecting, managing, integrating and analyzing these data are essential activities in order to shed light on diseases and on related therapies. The major issues in clinical data analysis are the incompleteness (missing values), the different adopted measure scales, the integration of the disparate collection procedures. Therefore, the main challenges are in managing clinical data, in discovering patients interactions, and in integrating the different data sources. The final goal is to extract relevant information from huge amounts of clinical data. Therefore, the analysis of clinical data requires new effective and efficient methods to extract compact and relevant information: the interdisciplinary field of data mining, which guides the automated knowledge discovery process, is a natural way to approach the complex task of clinical data analysis. Data mining deals with structured and unstructured data, that are, respectively, data for which we can give a model or not. For example, in clinical contexts it is important to highlight those trials (variables) that are frequent in a particular disease diagnosis. The objective of this work is to study and apply methods to manage and retrieve relevant information in clinical data sets. A practical analysis from real patient data collected from several dementia clinical departments in Italy is reported as example of clinical data mining. The particular field of logic classification, where a data model is computed in form of propositional logic formulas, is investigated for clinical data mining and compared to other techniques, showing that it is a successful approach to compute a compact data model for clinical knowledge discovery.
AU  - Weitschek, E.
AU  - Felici, G.
AU  - Bertolazzi, P.
KW  - eppi-reviewer4
N1  - 21444066
8606
Weitschek, Emanuel Felici, Giovanni Bertolazzi, Paola DEXA 2013 24th International Workshop on Database and Expert Systems Applications (DEXA) AUG 26-30, 2013 Univ Econ, Prague, CZECH REPUBLIC Weitschek, Emanuel/0000-0002-8045-2925
PY  - 2013
SN  - 1529-4188 978-0-7695-5070-1
SP  - 90-94
ST  - Clinical data mining: problems, pitfalls and solutions
T2  - 2013 24th International Workshop on Database and Expert Systems Applications
TI  - Clinical data mining: problems, pitfalls and solutions
UR  - <Go to ISI>://WOS:000335791800017
ID  - 51
ER  - 


TY  - JOUR
AB  - BACKGROUND: Pathology reports typically require manual review to abstract research data. We developed a natural language processing (NLP) system to automatically interpret free-text breast pathology reports with limited assistance from manual abstraction. METHODS: We used an iterative approach of machine learning algorithms and constructed groups of related findings to identify breast-related procedures and results from free-text pathology reports. We evaluated the NLP system using an all-or-nothing approach to determine which reports could be processed entirely using NLP and which reports needed manual review beyond NLP. We divided 3234 reports for development (2910, 90%), and evaluation (324, 10%) purposes using manually reviewed pathology data as our gold standard. RESULTS: NLP correctly coded 12.7% of the evaluation set, flagged 49.1% of reports for manual review, incorrectly coded 30.8%, and correctly omitted 7.4% from the evaluation set due to irrelevancy (i.e. not breast-related). Common procedures and results were identified correctly (e.g. invasive ductal with 95.5% precision and 94.0% sensitivity), but entire reports were flagged for manual review because of rare findings and substantial variation in pathology report text. CONCLUSIONS: The NLP system we developed did not perform sufficiently for abstracting entire breast pathology reports. The all-or-nothing approach resulted in too broad of a scope of work and limited our flexibility to identify breast pathology procedures and results. Our NLP system was also limited by the lack of the gold standard data on rare findings and wide variation in pathology text. Focusing on individual, common elements and improving pathology text report standardization may improve performance.
AU  - Wieneke, A. E.
AU  - Bowles, E. J.
AU  - Cronkite, D.
AU  - Wernli, K. J.
AU  - Gao, H.
AU  - Carrell, D.
AU  - Buist, D. S.
DA  - 2015
DO  - 10.4103/2153-3539.159215. eCollection 2015.
KW  - eppi-reviewer4
N1  - 21448739
3967
PY  - 2015
SN  - 2229-5089 (Print)
SP  - 38
ST  - Validation of natural language processing to extract breast cancer pathology procedures and results
T2  - J Pathol Inform
TI  - Validation of natural language processing to extract breast cancer pathology procedures and results
UR  - http://www.jpathinformatics.org/article.asp?issn=2153-3539;year=2015;volume=6;issue=1;spage=38;epage=38;aulast=Wieneke
VL  - 6
ID  - 152
ER  - 


TY  - JOUR
AB  - In this study,, we propose the web forum analysis model that can support corporate foresight activities. The medical industry can utilize the rich, objective decision-making data contained within web forums through which participants who have common interests disseminate and receive information and form self-contained communities. We collect and analyze the contents of the web forum using Web, text, and data mining techniques. We identify the major needs of Alzheimer disease patients and their families. We also show how to track the time-series patterns of major topics providing insight to the medical industry. Furthermore, we study the dynamic mechanisms of major needs using the epidemic model and describe how users in a web forum collectively participate in topic discussions. Using the proposed model, the medical industry can predict the future market by estimating how long a topic will persist and how strongly a topic attracts attention. (C) 2014 Elsevier Inc. All rights reserved.
AU  - Woo, J.
AU  - Lee, M. J.
AU  - Ku, Y.
AU  - Chen, H. C.
DA  - 2015
KW  - eppi-reviewer4
N1  - 21446794
7878
Woo, Jiyoung Lee, Min Jung Ku, Yungchang Chen, Hsinchun
PY  - 2015
SN  - 0040-1625
SP  - 77-90
ST  - Modeling the dynamics of medical information through web forums in medical industry
T2  - Technological Forecasting and Social Change
TI  - Modeling the dynamics of medical information through web forums in medical industry
UR  - <Go to ISI>://WOS:000357905400007
VL  - 97
ID  - 63
ER  - 


TY  - JOUR
AB  - BACKGROUND: High smoking prevalence is a major public health concern for people with mental disorders. Improved monitoring could be facilitated through electronic health record (EHR) databases. We evaluated whether EHR information held in structured fields might be usefully supplemented by open-text information. The prevalence and correlates of EHR-derived current smoking in people with severe mental illness were also investigated. METHODS: All cases had been referred to a secondary mental health service between 2008-2011 and received a diagnosis of schizophreniform or bipolar disorder. The study focused on those aged over 15 years who had received active care from the mental health service for at least a year (N=1,555). The 'CRIS-IE-Smoking' application used General Architecture for Text Engineering (GATE) natural language processing software to extract smoking status information from open-text fields. A combination of CRIS-IE-Smoking with data from structured fields was evaluated for coverage and the prevalence and demographic correlates of current smoking were analysed. RESULTS: Proportions of patients with recorded smoking status increased from 11.6% to 64.0% through supplementing structured fields with CRIS-IE-Smoking data. The prevalence of current smoking was 59.6% in these 995 cases for whom this information was available. After adjustment, younger age (below 65 years), male sex, and non-cohabiting status were associated with current smoking status. CONCLUSIONS: A natural language processing application substantially improved routine EHR data on smoking status above structured fields alone and could thus be helpful in improving monitoring of this lifestyle behaviour. However, limited information on smoking status remained a challenge.
AU  - Wu, C. Y.
AU  - Chang, C. K.
AU  - Robson, D.
AU  - Jackson, R.
AU  - Chen, S. J.
AU  - Hayes, R. D.
AU  - Stewart, R.
DA  - 2013
DO  - 10.1371/journal.pone.0074262. eCollection 2013.
IS  - 9
KW  - eppi-reviewer4
N1  - 21445173
4696
PY  - 2013
SN  - 1932-6203 (Electronic) 1932-6203 (Linking)
SP  - e74262
ST  - Evaluation of smoking status identification using electronic health records and open-text information in a large mental health case register
T2  - PLoS One
TI  - Evaluation of smoking status identification using electronic health records and open-text information in a large mental health case register
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3772070/pdf/pone.0074262.pdf
VL  - 8
ID  - 89
ER  - 


TY  - JOUR
AB  - BACKGROUND: One challenge in reusing clinical data stored in electronic medical records is that these data are heterogenous. Clinical Natural Language Processing (NLP) plays an important role in transforming information in clinical text to a standard representation that is comparable and interoperable. Information may be processed and shared when a type system specifies the allowable data structures. Therefore, we aim to define a common type system for clinical NLP that enables interoperability between structured and unstructured data generated in different clinical settings. RESULTS: We describe a common type system for clinical NLP that has an end target of deep semantics based on Clinical Element Models (CEMs), thus interoperating with structured data and accommodating diverse NLP approaches. The type system has been implemented in UIMA (Unstructured Information Management Architecture) and is fully functional in a popular open-source clinical NLP system, cTAKES (clinical Text Analysis and Knowledge Extraction System) versions 2.0 and later. CONCLUSIONS: We have created a type system that targets deep semantics, thereby allowing for NLP systems to encapsulate knowledge from text and share it alongside heterogenous clinical data sources. Rather than surface semantics that are typically the end product of NLP algorithms, CEM-based semantics explicitly build in deep clinical semantics as the point of interoperability with more structured data types.
AU  - Wu, S. T.
AU  - Kaggal, V. C.
AU  - Dligach, D.
AU  - Masanz, J. J.
AU  - Chen, P.
AU  - Becker, L.
AU  - Chapman, W. W.
AU  - Savova, G. K.
AU  - Liu, H.
AU  - Chute, C. G.
DA  - 2013
DO  - 10.1186/2041-1480-4-1.
IS  - 1
KW  - eppi-reviewer4
N1  - 21444172
4943
PY  - 2013
SN  - 2041-1480 (Electronic)
SP  - 1
ST  - A common type system for clinical natural language processing
T2  - J Biomed Semantics
TI  - A common type system for clinical natural language processing
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575354/pdf/2041-1480-4-1.pdf
VL  - 4
ID  - 74
ER  - 


TY  - JOUR
AB  - OBJECTIVE: This study aims to propose a data-driven framework that takes unstructured free text narratives in Chinese Electronic Medical Records (EMRs) as input and converts them into structured time-event-description triples, where the description is either an elaboration or an outcome of the medical event. MATERIALS AND METHODS: Our framework uses a hybrid approach. It consists of constructing cross-domain core medical lexica, an unsupervised, iterative algorithm to accrue more accurate terms into the lexica, rules to address Chinese writing conventions and temporal descriptors, and a Support Vector Machine (SVM) algorithm that innovatively utilizes Normalized Google Distance (NGD) to estimate the correlation between medical events and their descriptions. RESULTS: The effectiveness of the framework was demonstrated with a dataset of 24,817 de-identified Chinese EMRs. The cross-domain medical lexica were capable of recognizing terms with an F1-score of 0.896. 98.5% of recorded medical events were linked to temporal descriptors. The NGD SVM description-event matching achieved an F1-score of 0.874. The end-to-end time-event-description extraction of our framework achieved an F1-score of 0.846. DISCUSSION: In terms of named entity recognition, the proposed framework outperforms state-of-the-art supervised learning algorithms (F1-score: 0.896 vs. 0.886). In event-description association, the NGD SVM is superior to SVM using only local context and semantic features (F1-score: 0.874 vs. 0.838). CONCLUSIONS: The framework is data-driven, weakly supervised, and robust against the variations and noises that tend to occur in a large corpus. It addresses Chinese medical writing conventions and variations in writing styles through patterns used for discovering new terms and rules for updating the lexica.
AU  - Xu, D.
AU  - Zhang, M.
AU  - Zhao, T.
AU  - Ge, C.
AU  - Gao, W.
AU  - Wei, J.
AU  - Zhu, K. Q.
DA  - 2015
DO  - 10.1371/journal.pone.0136270. eCollection 2015.
IS  - 8
KW  - eppi-reviewer4
N1  - 21444568
3913
PY  - 2015
SN  - 1932-6203 (Electronic) 1932-6203 (Linking)
SP  - e0136270
ST  - Data-Driven Information Extraction from Chinese Electronic Medical Records
T2  - PLoS One
TI  - Data-Driven Information Extraction from Chinese Electronic Medical Records
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4546596/pdf/pone.0136270.pdf
VL  - 10
ID  - 34
ER  - 


TY  - JOUR
AB  - Identification of a cohort of patients with specific diseases is an important step for clinical research that is based on electronic health records (EHRs). Informatics approaches combining structured EHR data, such as billing records, with narrative text data have demonstrated utility for such tasks. This paper describes an algorithm combining machine learning and natural language processing to detect patients with colorectal cancer (CRC) from entire EHRs at Vanderbilt University Hospital. We developed a general case detection method that consists of two steps: 1) extraction of positive CRC concepts from all clinical notes (document-level concept identification); and 2) determination of CRC cases using aggregated information from both clinical narratives and structured billing data (patient-level case determination). For each step, we compared performance of rule-based and machine-learning-based approaches. Using a manually reviewed data set containing 300 possible CRC patients (150 for training and 150 for testing), we showed that our method achieved F-measures of 0.996 for document level concept identification, and 0.93 for patient level case detection.
AU  - Xu, H.
AU  - Fu, Z.
AU  - Shah, A.
AU  - Chen, Y.
AU  - Peterson, N. B.
AU  - Chen, Q.
AU  - Mani, S.
AU  - Levy, M. A.
AU  - Dai, Q.
AU  - Denny, J. C.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21445290
5297
Xu, Hua Fu, Zhenming Shah, Anushi Chen, Yukun Peterson, Neeraja B Chen, Qingxia Mani, Subramani Levy, Mia A Dai, Qi Denny, Josh C 1UL1RR024975-01/RR/NCRR NIH HHS/United States LM008635/LM/NLM NIH HHS/United States LM010016/LM/NLM NIH HHS/United States R01CA141307/CA/NCI NIH HHS/United States Comparative Study Research Support, N.I.H., Extramural United States AMIA Annu Symp Proc. 2011;2011:1564-72. Epub 2011 Oct 22.
PY  - 2011
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1564-72
ST  - Extracting and integrating data from entire electronic health records for detecting colorectal cancer cases
T2  - AMIA Annu Symp Proc
TI  - Extracting and integrating data from entire electronic health records for detecting colorectal cancer cases
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3243156/pdf/1564_amia_2011_proc.pdf
VL  - 2011
ID  - 130
ER  - 


TY  - JOUR
AB  - OBJECTIVE: A system that translates narrative text in the medical domain into structured representation is in great demand. The system performs three sub-tasks: concept extraction, assertion classification, and relation identification. DESIGN: The overall system consists of five steps: (1) pre-processing sentences, (2) marking noun phrases (NPs) and adjective phrases (APs), (3) extracting concepts that use a dosage-unit dictionary to dynamically switch two models based on Conditional Random Fields (CRF), (4) classifying assertions based on voting of five classifiers, and (5) identifying relations using normalized sentences with a set of effective discriminating features. MEASUREMENTS: Macro-averaged and micro-averaged precision, recall and F-measure were used to evaluate results. RESULTS: The performance is competitive with the state-of-the-art systems with micro-averaged F-measure of 0.8489 for concept extraction, 0.9392 for assertion classification and 0.7326 for relation identification. CONCLUSIONS: The system exploits an array of common features and achieves state-of-the-art performance. Prudent feature engineering sets the foundation of our systems. In concept extraction, we demonstrated that switching models, one of which is especially designed for telegraphic sentences, improved extraction of the treatment concept significantly. In assertion classification, a set of features derived from a rule-based classifier were proven to be effective for the classes such as conditional and possible. These classes would suffer from data scarcity in conventional machine-learning methods. In relation identification, we use two-staged architecture, the second of which applies pairwise classifiers to possible candidate classes. This architecture significantly improves performance.
AU  - Xu, Y.
AU  - Hong, K.
AU  - Tsujii, J.
AU  - Chang, E. I.
DA  - 2012
DO  - 10.1136/amiajnl-2011-000776. Epub 2012 May 14.
IS  - 5
KW  - eppi-reviewer4
N1  - 21445354
5173
PY  - 2012
SN  - 1527-974X (Electronic) 1067-5027 (Linking)
SP  - 824-32
ST  - Feature engineering combined with machine learning and rule-based methods for structured information extraction from narrative clinical discharge summaries
T2  - J Am Med Inform Assoc
TI  - Feature engineering combined with machine learning and rule-based methods for structured information extraction from narrative clinical discharge summaries
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3422834/pdf/amiajnl-2011-000776.pdf
VL  - 19
ID  - 75
ER  - 


TY  - JOUR
AB  - BACKGROUND: The authors have previously demonstrated highly reliable automated classification of free-text computed tomography (CT) imaging reports using a hybrid system that pairs linguistic (natural language processing) and statistical (machine learning) techniques. Previously performed for identifying the outcome of orbital fracture in unprocessed radiology reports from a clinical data repository, the performance has not been replicated for more complex outcomes. OBJECTIVES: To validate automated outcome classification performance of a hybrid natural language processing (NLP) and machine learning system for brain CT imaging reports. The hypothesis was that our system has performance characteristics for identifying pediatric traumatic brain injury (TBI). METHODS: This was a secondary analysis of a subset of 2,121 CT reports from the Pediatric Emergency Care Applied Research Network (PECARN) TBI study. For that project, radiologists dictated CT reports as free text, which were then deidentified and scanned as PDF documents. Trained data abstractors manually coded each report for TBI outcome. Text was extracted from the PDF files using optical character recognition. The data set was randomly split evenly for training and testing. Training patient reports were used as input to the Medical Language Extraction and Encoding (MedLEE) NLP tool to create structured output containing standardized medical terms and modifiers for negation, certainty, and temporal status. A random subset stratified by site was analyzed using descriptive quantitative content analysis to confirm identification of TBI findings based on the National Institute of Neurological Disorders and Stroke (NINDS) Common Data Elements project. Findings were coded for presence or absence, weighted by frequency of mentions, and past/future/indication modifiers were filtered. After combining with the manual reference standard, a decision tree classifier was created using data mining tools WEKA 3.7.5 and Salford Predictive Miner 7.0. Performance of the decision tree classifier was evaluated on the test patient reports. RESULTS: The prevalence of TBI in the sampled population was 159 of 2,217 (7.2%). The automated classification for pediatric TBI is comparable to our prior results, with the notable exception of lower positive predictive value. Manual review of misclassified reports, 95.5% of which were false-positives, revealed that a sizable number of false-positive errors were due to differing outcome definitions between NINDS TBI findings and PECARN clinical important TBI findings and report ambiguity not meeting definition criteria. CONCLUSIONS: A hybrid NLP and machine learning automated classification system continues to show promise in coding free-text electronic clinical data. For complex outcomes, it can reliably identify negative reports, but manual review of positive reports may be required. As such, it can still streamline data collection for clinical research and performance improvement.
AU  - Yadav, K.
AU  - Sarioglu, E.
AU  - Choi, H. A.
AU  - Cartwright, W. B. th
AU  - Hinds, P. S.
AU  - Chamberlain, J. M.
DA  - 2016
DO  - 10.1111/acem.12859. Epub 2016 Jan 14.
IS  - 2
KW  - eppi-reviewer4
N1  - 21443740
3846
PY  - 2016
SN  - 1553-2712 (Electronic) 1069-6563 (Linking)
SP  - 171-8
ST  - Automated Outcome Classification of Computed Tomography Imaging Reports for Pediatric Traumatic Brain Injury
T2  - Acad Emerg Med
TI  - Automated Outcome Classification of Computed Tomography Imaging Reports for Pediatric Traumatic Brain Injury
VL  - 23
ID  - 109
ER  - 


TY  - JOUR
AB  - BACKGROUND: Reliably abstracting outcomes from free-text electronic health records remains a challenge. While automated classification of free text has been a popular medical informatics topic, performance validation using real-world clinical data has been limited. The two main approaches are linguistic (natural language processing [NLP]) and statistical (machine learning). The authors have developed a hybrid system for abstracting computed tomography (CT) reports for specified outcomes. OBJECTIVES: The objective was to measure performance of a hybrid NLP and machine learning system for automated outcome classification of emergency department (ED) CT imaging reports. The hypothesis was that such a system is comparable to medical personnel doing the data abstraction. METHODS: A secondary analysis was performed on a prior diagnostic imaging study on 3,710 blunt facial trauma victims. Staff radiologists dictated CT reports as free text, which were then deidentified. A trained data abstractor manually coded the reference standard outcome of acute orbital fracture, with a random subset double-coded for reliability. The data set was randomly split evenly into training and testing sets. Training patient reports were used as input to the Medical Language Extraction and Encoding (MedLEE) NLP tool to create structured output containing standardized medical terms and modifiers for certainty and temporal status. Findings were filtered for low certainty and past/future modifiers and then combined with the manual reference standard to generate decision tree classifiers using data mining tools Waikato Environment for Knowledge Analysis (WEKA) 3.7.5 and Salford Predictive Miner 6.6. Performance of decision tree classifiers was evaluated on the testing set with or without NLP processing. RESULTS: The performance of machine learning alone was comparable to prior NLP studies (sensitivity = 0.92, specificity = 0.93, precision = 0.95, recall = 0.93, f-score = 0.94), and the combined use of NLP and machine learning showed further improvement (sensitivity = 0.93, specificity = 0.97, precision = 0.97, recall = 0.96, f-score = 0.97). This performance is similar to, or better than, that of medical personnel in previous studies. CONCLUSIONS: A hybrid NLP and machine learning automated classification system shows promise in coding free-text electronic clinical data.
AU  - Yadav, K.
AU  - Sarioglu, E.
AU  - Smith, M.
AU  - Choi, H. A.
DA  - 2013
DO  - 10.1111/acem.12174.
IS  - 8
KW  - eppi-reviewer4
N1  - 21443741
4711
PY  - 2013
SN  - 1553-2712 (Electronic) 1069-6563 (Linking)
SP  - 848-54
ST  - Automated outcome classification of emergency department computed tomography imaging reports
T2  - Acad Emerg Med
TI  - Automated outcome classification of emergency department computed tomography imaging reports
UR  - http://onlinelibrary.wiley.com/store/10.1111/acem.12174/asset/acem12174.pdf?v=1&t=ir540h0g&s=3af7d6179bd450056b7048743eba3a3e1ca64c35
VL  - 20
ID  - 131
ER  - 


TY  - JOUR
AB  - Transcription factors (TFs) play a crucial role in gene regulation, and providing structured and curated information about them is important for genome biology. Manual curation of TF related data is time-consuming and always lags behind the actual knowledge available in the biomedical literature. Here we present a machine-learning text mining approach for identification and tagging of protein mentions that play a TF role in a given context to support the curation process. More precisely, the method explicitly identifies those protein mentions in text that refer to their potential TF functions. The prediction features are engineered from the results of shallow parsing and domain-specific processing (recognition of relevant appearing in phrases) and a phrase-based Conditional Random Fields (CRF) model is used to capture the content and context information of candidate entities. The proposed approach for the identification of TF mentions has been tested on a set of evidence sentences from the TRANSFAC and FlyTF databases. It achieved an F-measure of around 51.5% with a precision of 62.5% using 5-fold cross-validation evaluation. The experimental results suggest that the phrase-based CRF model benefits from the flexibility to use correlated domain-specific features that describe the dependencies between TFs and other entities. To the best of our knowledge, this work is one of the first attempts to apply text-mining techniques to the task of assigning semantic roles to protein mentions.
AU  - Yang, H.
AU  - Keane, J.
AU  - Bergman, C. M.
AU  - Nenadic, G.
DA  - 2009
DO  - 10.1016/j.jbi.2009.04.001. Epub 2009 Apr 11.
IS  - 5
KW  - eppi-reviewer4
N1  - 21443693
6057
PY  - 2009
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 887-94
ST  - Assigning roles to protein mentions: the case of transcription factors
T2  - J Biomed Inform
TI  - Assigning roles to protein mentions: the case of transcription factors
UR  - http://ac.els-cdn.com/S1532046409000525/1-s2.0-S1532046409000525-main.pdf?_tid=b6dc9a92-5417-11e6-a1e4-00000aacb361&acdnat=1469637331_69754ae17874c2348a4e41a3e0724a3e
VL  - 42
ID  - 93
ER  - 


TY  - CHAP
A2  - Huang, Z.
A2  - Liu, C.
A2  - He, J.
A2  - Huang, G.
AB  - Most digital information resources for readers of the medical library exist in the form of unstructured free text (journal papers). Therefore it has become the new direction of data mining research to extract keywords in the collection of medical literature and turn them into structured knowledge that is easily accessible and analyzable. MetaMap, a mapping tool from free text to the UMLS Metathesaurus developed by the U.S. National Library of Medicine, maps keywords to the normative UMLS thesaurus, and provides a rating for the mapping degree of every word. The present study extracts keywords from the English language literature of insulin-like growth factors 1 research, assigns weights to the keywords using the BM25F model, screens out groups of important keywords, carries out a cluster analysis of these keywords.
AU  - Yin, S. M.
AU  - Li, C. Y.
AU  - Zhou, Y. G.
AU  - Huang, J.
KW  - eppi-reviewer4
N1  - 21444656
8380
Yin, Shumei Li, Chunying Zhou, Yigang Huang, Jun 14th Conference on Web Information Systems Engineering (WISE) OCT 13-15, 2013 Nanjing, PEOPLES R CHINA Zhou, Yigang/M-8006-2014 Zhou, Yigang/0000-0003-4236-4884
PY  - 2014
SN  - 0302-9743 978-3-642-54370-8; 978-3-642-54369-2
SP  - 359-372
ST  - Detecting Hotspots in Insulin-Like Growth Factors 1 Research through MetaMap and Data Mining Technologies
T2  - Web Information Systems Engineering - Wise 2013 Workshops
TI  - Detecting Hotspots in Insulin-Like Growth Factors 1 Research through MetaMap and Data Mining Technologies
UR  - <Go to ISI>://WOS:000337296200031
VL  - 8182
ID  - 174
ER  - 


TY  - JOUR
AB  - PURPOSE: A large share of the information in electronic medical records (EMRs) consists of free-text compositions. From a computational point-of-view, the continuing prevalence of free-text entry is a major hindrance when the goal is to increase automation in EMRs. However, the efforts in developing standards for the structured representation of medical information have not proven to be a panacea. The information space of clinical medicine is very diverse and constantly evolving, making it challenging to develop standards for the domain. This paper reports a study aiming to increase automation in the EMR through the computational understanding of specific class of medical text in English, namely emergency department chief complaints. METHODS: We apply domain-specific analytical modeling for the computational understanding of chief complaints. We evaluate the performance of this approach in the automatic classification of chief complaints, e.g., for use in automatic syndromic surveillance. RESULTS: The evaluation in a multi-hospital setting showed that the presented algorithm was accurate in terms of classification correctness. Also, use of approximate matching in the algorithm to cope with typographic variance did not affect classification correctness while increasing classification completeness.
AU  - Yli-Hietanen, J.
AU  - Niiranen, S.
AU  - Aswell, M.
AU  - Nathanson, L.
DA  - 2009
DO  - 10.1016/j.ijmedinf.2009.02.002. Epub 2009 Mar 23.
IS  - 12
KW  - eppi-reviewer4
N1  - 21444869
6072
PY  - 2009
SN  - 1872-8243 (Electronic) 1386-5056 (Linking)
SP  - e27-30
ST  - Domain-specific analytical language modeling--the chief complaint as a case study
T2  - Int J Med Inform
TI  - Domain-specific analytical language modeling--the chief complaint as a case study
UR  - http://ac.els-cdn.com/S1386505609000197/1-s2.0-S1386505609000197-main.pdf?_tid=bc2f8464-5417-11e6-82e8-00000aab0f01&acdnat=1469637340_3a02d36df42ffdf55bb4248ba691e93b
VL  - 78
ID  - 35
ER  - 


TY  - JOUR
AB  - Sudden Unexpected Death in Epilepsy (SUDEP) is the leading mode of epilepsy-related death and is most common in patients with intractable, frequent, and continuing seizures. A statistically significant cohort of patients for SUDEP study requires meticulous, prospective follow up of a large population that is at an elevated risk, best represented by the Epilepsy Monitoring Unit (EMU) patient population. Multiple EMUs need to collaborate, share data for building a larger cohort of potential SUDEP patient using a state-of-the-art informatics infrastructure. To address the challenges of data integration and data access from multiple EMUs, we developed the Multi-Modality Epilepsy Data Capture and Integration System (MEDCIS) that combines retrospective clinical free text processing using NLP, prospective structured data capture using an ontology-driven interface, interfaces for cohort search and signal visualization, all in a single integrated environment. A dedicated Epilepsy and Seizure Ontology (EpSO) has been used to streamline the user interfaces, enhance its usability, and enable mappings across distributed databases so that federated queries can be executed. MEDCIS contained 936 patient data sets from the EMUs of University Hospitals Case Medical Center (UH CMC) in Cleveland and Northwestern Memorial Hospital (NMH) in Chicago. Patients from UH CMC and NMH were stored in different databases and then federated through MEDCIS using EpSO and our mapping module. More than 77GB of multi-modal signal data were processed using the Cloudwave pipeline and made available for rendering through the web-interface. About 74% of the 40 open clinical questions of interest were answerable accurately using the EpSO-driven VISual AGregagator and Explorer (VISAGE) interface. Questions not directly answerable were either due to their inherent computational complexity, the unavailability of primary information, or the scope of concept that has been formulated in the existing EpSO terminology system.
AU  - Zhang, G. Q.
AU  - Cui, L.
AU  - Lhatoo, S.
AU  - Schuele, S. U.
AU  - Sahoo, S. S.
DA  - 2014
KW  - eppi-reviewer4
N1  - 21446530
4597
Zhang, Guo-Qiang Cui, Licong Lhatoo, Samden Schuele, Stephan U Sahoo, Satya S 1-P20-NS076965-01/NS/NINDS NIH HHS/United States UL1 TR000439/TR/NCATS NIH HHS/United States UL1TR000439/TR/NCATS NIH HHS/United States Research Support, N.I.H., Extramural Research Support, Non-U.S. Gov't United States AMIA Annu Symp Proc. 2014 Nov 14;2014:1248-57. eCollection 2014.
PY  - 2014
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1248-57
ST  - MEDCIS: Multi-Modality Epilepsy Data Capture and Integration System
T2  - AMIA Annu Symp Proc
TI  - MEDCIS: Multi-Modality Epilepsy Data Capture and Integration System
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4420009/pdf/1986714.pdf
VL  - 2014
ID  - 204
ER  - 


TY  - JOUR
AB  - PURPOSE: The purpose of this study was to investigate whether aspirin use can be captured from the clinical notes in a nonvalvular atrial fibrillation population. METHODS: A total of 29,507 patients with newly diagnosed nonvalvular atrial fibrillation were identified from January 1, 2006, through December 31, 2011, and were followed up through December 31, 2012. More than 3 million clinical notes were retrieved from electronic medical records. A training data set of 2949 notes was created to develop a computer-based method to automatically extract aspirin use status and dosage information using natural language processing (NLP). A gold standard data set of 5339 notes was created using a blinded manual review. NLP results were validated against the gold standard data set. The aspirin data from the structured medication databases were also compared with the results from NLP. Positive and negative predictive values, along with sensitivity and specificity, were calculated. FINDINGS: NLP achieved 95.5% sensitivity and 98.9% specificity when compared with the gold standard data set. The positive predictive value was 93.0%, and the negative predictive value was 99.3%. NLP identified aspirin use for 83.8% of the study population, and 70% of the low dose aspirin use was identified only by the NLP method. IMPLICATIONS: We developed and validated an NLP method specifically designed to identify low dose aspirin use status from the clinical notes with high accuracy. This method can be a valuable tool to supplement existing structured medication data.
AU  - Zheng, C.
AU  - Rashid, N.
AU  - Koblick, R.
AU  - An, J.
DA  - 2015
DO  - 10.1016/j.clinthera.2015.07.002. Epub 2015 Jul 29.
IS  - 9
KW  - eppi-reviewer4
N1  - 21446538
3939
PY  - 2015
SN  - 1879-114X (Electronic) 0149-2918 (Linking)
SP  - 2048-2058 e2
ST  - Medication Extraction from Electronic Clinical Notes in an Integrated Health System: A Study on Aspirin Use in Patients with Nonvalvular Atrial Fibrillation
T2  - Clin Ther
TI  - Medication Extraction from Electronic Clinical Notes in an Integrated Health System: A Study on Aspirin Use in Patients with Nonvalvular Atrial Fibrillation
UR  - http://ac.els-cdn.com/S0149291815009303/1-s2.0-S0149291815009303-main.pdf?_tid=c05d2d5c-5417-11e6-bd44-00000aacb362&acdnat=1469637347_1f54032e9c27a896577df491f4dbe9f1
VL  - 37
ID  - 4
ER  - 


TY  - JOUR
AB  - To facilitate the process of extracting information from narrative medical reports and transforming extracted data into standardized structured forms, we present an interactive, incrementally learning based information extraction system - ASLForm. ASLForm provides users a convenient interface that can be used as a simple data extraction and data entry system. It is unique, however, in its ability to transparently analyze and quickly learn, from users' interactions with a small number of reports, the desired values for the data fields. Additional user feedback (through acceptance decision or edits on the generated values) can incrementally refine the decision model in real-time, which further reduces users' interaction effort thereafter. The system eventually achieves high accuracy on data extraction with minimal effort from users. ASLForm requires no special configuration or training sets, and is not constrained to specific domains, thus it is easy to use and highly portable. Our experiments demonstrate the effectiveness of the system.
AU  - Zheng, S.
AU  - Wang, F.
AU  - Lu, J. J.
DA  - 2013
KW  - eppi-reviewer4
N1  - 21443669
4526
Zheng, Shuai Wang, Fusheng Lu, James J United States AMIA Annu Symp Proc. 2013 Nov 16;2013:1590-9. eCollection 2013.
PY  - 2013
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1590-9
ST  - ASLForm: an adaptive self learning medical form generating system
T2  - AMIA Annu Symp Proc
TI  - ASLForm: an adaptive self learning medical form generating system
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900168/pdf/amia_2013_symposium_1590.pdf
VL  - 2013
ID  - 178
ER  - 


TY  - JOUR
AB  - Temporal information is crucial in electronic medical records and biomedical information systems. Processing temporal information in medical narrative data is a very challenging area. It lies at the intersection of temporal representation and reasoning (TRR) in artificial intelligence and medical natural language processing (MLP). Some fundamental concepts and important issues in relation to TRR have previously been discussed, mainly in the context of processing structured data in biomedical informatics; however, it is important that these concepts be re-examined in the context of processing narrative data using MLP. Theoretical and methodological TRR studies in biomedical informatics can be classified into three main categories: category 1 applies theories and models from temporal reasoning in AI; category 2 defines frameworks that meet needs from clinical applications; category 3 resolves issues such as temporal granularity and uncertainty. Currently, most MLP systems are not designed with a formal representation of time, and their ability to reason about temporal relations among medical events is limited. Previous work in processing time with clinical narrative data includes processing time in clinical reports, modeling textual temporal expressions in clinical databases, processing time in clinical guidelines, and building time standards for data exchange and integration. In addition to common problems in MLP, there are challenges specific to TRR in medical text, which occur at each level of linguistic structure and analysis. Despite advances in temporal reasoning in biomedical informatics, processing time in medical text deserves more attention. Besides the need for more research in temporal granularity, fuzzy time, temporal contradiction, intermittent events and uncertainty, broad areas for future research include enhancing functions of current MLP systems on processing temporal information, incorporating medical knowledge into temporal reasoning systems, resolving coreference, integrating narrative data with structured data and evaluating these systems.
AU  - Zhou, L.
AU  - Hripcsak, G.
DA  - 2007
IS  - 2
KW  - eppi-reviewer4
N1  - 21448367
6548
Zhou, Li Hripcsak, George R01 LM006910/LM/NLM NIH HHS/United States Review United States J Biomed Inform. 2007 Apr;40(2):183-202. Epub 2007 Jan 11.
PY  - 2007
SN  - 1532-0480 (Electronic) 1532-0464 (Linking)
SP  - 183-202
ST  - Temporal reasoning with medical data--a review with emphasis on medical natural language processing
T2  - J Biomed Inform
TI  - Temporal reasoning with medical data--a review with emphasis on medical natural language processing
UR  - http://ac.els-cdn.com/S1532046407000032/1-s2.0-S1532046407000032-main.pdf?_tid=c37921da-5417-11e6-8e16-00000aacb361&acdnat=1469637352_0e0711a61f4a606644316d27509be348
VL  - 40
ID  - 36
ER  - 


TY  - JOUR
AB  - BACKGROUND: The ability to manage and leverage family history information in the electronic health record (EHR) is crucial to delivering high-quality clinical care. OBJECTIVES: We aimed to evaluate existing standards in representing relative information, examine this information documented in EHRs, and develop a natural language processing (NLP) application to extract relative information from free-text clinical documents. METHODS: We reviewed a random sample of 100 admission notes and 100 discharge summaries of 198 patients, and also reviewed the structured entries for these patients in an EHR system's family history module. We investigated the two standards used by Stage 2 of Meaningful Use (SNOMED CT and HL7 Family History Standard) and identified coverage gaps of each standard in coding relative information. Finally, we evaluated the performance of the MTERMS NLP system in identifying relative information from free-text documents. RESULTS: The structure and content of SNOMED CT and HL7 for representing relative information are different in several ways. Both terminologies have high coverage to represent local relative concepts built in an ambulatory EHR system, but gaps in key concept coverage were detected; coverage rates for relative information in free-text clinical documents were 95.2% and 98.6%, respectively. Compared to structured entries, richer family history information was only available in free-text documents. Using a comprehensive lexicon that included concepts and terms of relative information from different sources, we expanded the MTERMS NLP system to extract and encode relative information in clinical documents and achieved a corresponding precision of 100% and recall of 97.4%. CONCLUSIONS: Comprehensive assessment and user guidance are critical to adopting standards into EHR systems in a meaningful way. A significant portion of patients' family history information is only documented in free-text clinical documents and NLP can be used to extract this information.
AU  - Zhou, L.
AU  - Lu, Y.
AU  - Vitale, C. J.
AU  - Mar, P. L.
AU  - Chang, F.
AU  - Dhopeshwarkar, N.
AU  - Rocha, R. A.
DA  - 2014
DO  - 10.4338/ACI-2013-10-RA-0080. eCollection 2014.
IS  - 2
KW  - eppi-reviewer4
N1  - 21447781
4379
PY  - 2014
SN  - 1869-0327 (Electronic)
SP  - 349-67
ST  - Representation of information about family relatives as structured data in electronic health records
T2  - Appl Clin Inform
TI  - Representation of information about family relatives as structured data in electronic health records
UR  - https://aci.schattauer.de/contents/archive/issue/1853/manuscript/21027/download.html
VL  - 5
ID  - 110
ER  - 


TY  - JOUR
AB  - Clinical information is often coded using different terminologies, and therefore is not interoperable. Our goal is to develop a general natural language processing (NLP) system, called Medical Text Extraction, Reasoning and Mapping System (MTERMS), which encodes clinical text using different terminologies and simultaneously establishes dynamic mappings between them. MTERMS applies a modular, pipeline approach flowing from a preprocessor, semantic tagger, terminology mapper, context analyzer, and parser to structure inputted clinical notes. Evaluators manually reviewed 30 free-text and 10 structured outpatient clinical notes compared to MTERMS output. MTERMS achieved an overall F-measure of 90.6 and 94.0 for free-text and structured notes respectively for medication and temporal information. The local medication terminology had 83.0% coverage compared to RxNorm's 98.0% coverage for free-text notes. 61.6% of mappings between the terminologies are exact match. Capture of duration was significantly improved (91.7% vs. 52.5%) from systems in the third i2b2 challenge.
AU  - Zhou, L.
AU  - Plasek, J. M.
AU  - Mahoney, L. M.
AU  - Karipineni, N.
AU  - Chang, F.
AU  - Yan, X.
AU  - Chang, F.
AU  - Dimaggio, D.
AU  - Goldman, D. S.
AU  - Rocha, R. A.
DA  - 2011
KW  - eppi-reviewer4
N1  - 21448690
5296
Zhou, Li Plasek, Joseph M Mahoney, Lisa M Karipineni, Neelima Chang, Frank Yan, Xuemin Chang, Fenny Dimaggio, Dana Goldman, Debora S Rocha, Roberto A 1R03HS018288-01/HS/AHRQ HHS/United States R03 HS018288/HS/AHRQ HHS/United States Research Support, Non-U.S. Gov't Research Support, U.S. Gov't, P.H.S. United States AMIA Annu Symp Proc. 2011;2011:1639-48. Epub 2011 Oct 22.
PY  - 2011
SN  - 1942-597X (Electronic) 1559-4076 (Linking)
SP  - 1639-48
ST  - Using Medical Text Extraction, Reasoning and Mapping System (MTERMS) to process medication information in outpatient clinical notes
T2  - AMIA Annu Symp Proc
TI  - Using Medical Text Extraction, Reasoning and Mapping System (MTERMS) to process medication information in outpatient clinical notes
UR  - http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3243163/pdf/1639_amia_2011_proc.pdf
VL  - 2011
ID  - 213
ER  - 


TY  - JOUR
AB  - OBJECTIVE: Traditional Chinese medicine (TCM) is a scientific discipline, which develops the related theories from the long-term clinical practices. The large-scale clinical data are the core empirical knowledge source for TCM research. This paper introduces a clinical data warehouse (CDW) system, which incorporates the structured electronic medical record (SEMR) data for medical knowledge discovery and TCM clinical decision support (CDS). MATERIALS AND METHODS: We have developed the clinical reference information model (RIM) and physical data model to manage the various information entities and their relationships in TCM clinical data. An extraction-transformation-loading (ETL) tool is implemented to integrate and normalize the clinical data from different operational data sources. The CDW includes online analytical processing (OLAP) and complex network analysis (CNA) components to explore the various clinical relationships. Furthermore, the data mining and CNA methods are used to discover the valuable clinical knowledge from the data. RESULTS: The CDW has integrated 20,000 TCM inpatient data and 20,000 outpatient data, which contains manifestations (e.g. symptoms, physical examinations and laboratory test results), diagnoses and prescriptions as the main information components. We propose a practical solution to accomplish the large-scale clinical data integration and preprocessing tasks. Meanwhile, we have developed over 400 OLAP reports to enable the multidimensional analysis of clinical data and the case-based CDS. We have successfully conducted several interesting data mining applications. Particularly, we use various classification methods, namely support vector machine, decision tree and Bayesian network, to discover the knowledge of syndrome differentiation. Furthermore, we have applied association rule and CNA to extract the useful acupuncture point and herb combination patterns from the clinical prescriptions. CONCLUSION: A CDW system consisting of TCM clinical RIM, ETL, OLAP and data mining as the core components has been developed to facilitate the tasks of TCM knowledge discovery and CDS. We have conducted several OLAP and data mining tasks to explore the empirical knowledge from the TCM clinical data. The CDW platform would be a promising infrastructure to make full use of the TCM clinical data for scientific hypothesis generation, and promote the development of TCM from individualized empirical knowledge to large-scale evidence-based medicine.
AU  - Zhou, X.
AU  - Chen, S.
AU  - Liu, B.
AU  - Zhang, R.
AU  - Wang, Y.
AU  - Li, P.
AU  - Guo, Y.
AU  - Zhang, H.
AU  - Gao, Z.
AU  - Yan, X.
DA  - 2010
DO  - 10.1016/j.artmed.2009.07.012. Epub 2010 Feb 1.
IS  - 2-3
KW  - eppi-reviewer4
N1  - 21444738
5847
PY  - 2010
SN  - 1873-2860 (Electronic) 0933-3657 (Linking)
SP  - 139-52
ST  - Development of traditional Chinese medicine clinical data warehouse for medical knowledge discovery and decision support
T2  - Artif Intell Med
TI  - Development of traditional Chinese medicine clinical data warehouse for medical knowledge discovery and decision support
UR  - http://ac.els-cdn.com/S0933365709001055/1-s2.0-S0933365709001055-main.pdf?_tid=c7d19d84-5417-11e6-aef6-00000aacb362&acdnat=1469637359_a6ebdb6100a5dfa3f6b65378d039712c
VL  - 48
ID  - 52
ER  - 


